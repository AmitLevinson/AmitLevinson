<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>R on Amit Levinson</title>
    <link>/categories/r/</link>
    <description>Recent content in R on Amit Levinson</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>&amp;copy; Amit Levinson {year}</copyright>
    <lastBuildDate>Thu, 09 Jul 2020 00:00:00 +0000</lastBuildDate>
    
        <atom:link href="/categories/r/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Defining uncertainty in the Israeli lottery</title>
      <link>/post/uncertainty-in-the-israeli-lottery/</link>
      <pubDate>Thu, 09 Jul 2020 00:00:00 +0000</pubDate>
      
      <guid>/post/uncertainty-in-the-israeli-lottery/</guid>
      <description>


&lt;blockquote&gt;
&lt;p&gt;‚ÄúI have a great idea to get rich. All we need is a lot of money.‚Äù &lt;/br&gt; ‚Äï A meme on the internet&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;A while ago I was reading about Bernoulli trials and decided that I wanted to explore them further. I was wondering what would be an interesting case study for such a topic, and then it hit me (üí°): Why not explore lottery probabilities? Little did I know that this topic would lead me down the geometric distribution road and help me better understand the uncertainty in lotteries.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;hunger.jpg&#34; width=&#34;85%&#34; height=&#34;85%&#34; /&gt;&lt;/p&gt;
&lt;div id=&#34;the-rules&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;The rules&lt;/h3&gt;
&lt;p&gt;We‚Äôll be exploring the probability of winning the Israeli lottery held by the &lt;a href=&#34;https://www.pais.co.il/&#34;&gt;Pais organization&lt;/a&gt;, a well-known lottery enterprise in Israel. The ticket we‚Äôll be discussing costs is applicable to the main lottery called ‚ÄòLoto‚Äô and costs 5.8 New Israeli Shekels (NIS), approximately ~1.6 dollars.&lt;/p&gt;
&lt;p&gt;Pais holds their lotteries twice a week with the regular prize of 5 million NIS, equivalent to $1,420,000. If there‚Äôs no winner for a given week, the prize accumulates to the following lottery. For the sake of the post I‚Äôll only discuss winning a first place prize (be the only winner). In addition I won‚Äôt be discussing the effect of prize increase on when to participate.&lt;/p&gt;
&lt;p&gt;When filling out a lottery form you choose 6 numbers in the range of 1‚Äì37 and a ‚Äòstrong‚Äô number in the range of 1‚Äì7&lt;a href=&#34;#fn1&#34; class=&#34;footnote-ref&#34; id=&#34;fnref1&#34;&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt;. In order to win first place you have to &lt;del&gt;get&lt;/del&gt; guess correctly both the 6 number set and the strong number. Luckily, the order of the 6 numbers doesn‚Äôt matter therefore if you wrote &lt;span class=&#34;math inline&#34;&gt;\(6,12...n\)&lt;/span&gt; or &lt;span class=&#34;math inline&#34;&gt;\(12,6,...n\)&lt;/span&gt; you‚Äôre good on both (Also known as combinations, more on that in a minute).&lt;/p&gt;
&lt;p&gt;Another ‚Äòluck‚Äô in our favor is that in each lottery ticket we have the option to fill out two sets of numbers, therefore doubling our odds of winning. I‚Äôm assuming we‚Äôre on the same page and you won‚Äôt use both of your attempts to guess the same sets of numbers, so that somewhat increases our odds of winning. Speaking of odds, let‚Äôs have a look at them.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;the-odds&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;The odds&lt;/h3&gt;
&lt;p&gt;To understand the lottery probabilities, we need to calculate &lt;strong&gt;the probability of guessing a combination of 6 numbers out of 37 options along with one strong number out of 7 possible numbers.&lt;/strong&gt; In order to do this, we can turn to combinations:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;In mathematics, a combination is a selection of items from a collection, such that (unlike permutations) the order of selection does not matter. &lt;/br&gt; ‚Äï &lt;a href=&#34;https://en.wikipedia.org/wiki/Combination&#34;&gt;Wikipedia&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;That‚Äôs exactly what we need. We want to calculate the probability of randomly guessing six numbers regardless of their order; If the order was important we‚Äôd want to look at permutations. In addition, each number is drawn without replacement, and therefore there can‚Äôt be repetition of the same number.&lt;/p&gt;
&lt;p&gt;The formula to calculate combinations is as follows: &lt;span class=&#34;math inline&#34;&gt;\(C(n,k) = \frac{n!}{k!(n-k)!}\)&lt;/span&gt; Where &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; is the number of options to choose from and &lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt; is the number of choices we make.&lt;/p&gt;
&lt;p&gt;Inputting our numbers we get &lt;span class=&#34;math inline&#34;&gt;\(C(37,6) = \frac{37!}{6!(37-6)!}\)&lt;/span&gt;, and now we just calculate away. However, don‚Äôt forget we also need to guess another number out of 7 possible numbers (the strong one), so we‚Äôll multiply our outcome by &lt;span class=&#34;math inline&#34;&gt;\(\frac1 7\)&lt;/span&gt;, yielding a probability of &lt;span class=&#34;math inline&#34;&gt;\(p = \frac{1}{16273488}\)&lt;/span&gt;. ‚ÄòLuckily‚Äô we choose two sets of numbers in a given ticket, so we multiply the probability by 2.&lt;/p&gt;
&lt;p&gt;Therefore, the probability of winning the lottery is &lt;strong&gt;&lt;span class=&#34;math inline&#34;&gt;\(p = \frac{1}{8136744}\)&lt;/span&gt;.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Wow! that‚Äôs a very low probability. How low? Let‚Äôs try and visualize it.&lt;/p&gt;
&lt;p&gt;Sometimes when we receive a probability it‚Äôs hard to grasp the odds and numbers thrown at us. Therefore, I‚Äôll try to visualize it for us. Imagine there‚Äôs a pool filled with 8,136,744 balls. One of those balls is red and choosing that exact red ball blindly will win you the lottery:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(ggplot2)
library(dplyr)
library(scattermore)
library(extrafont)
library(ggtext)

set.seed(123)

df_viz &amp;lt;- data.frame(x = rnorm(8136743, mean = 1000, sd = 1000),y = rnorm(8136743))
point_highlight &amp;lt;- data.frame(x = -1811.674, y = -2.268505588)

lot_p &amp;lt;- ggplot()+
  geom_scattermost(df_viz, pointsize = 0.1, pixels = c(2000,2000))+
  geom_point(data = point_highlight, aes(x = x, y = y), size = 0.3, color = &amp;quot;red&amp;quot;)+
  annotate(geom = &amp;quot;curve&amp;quot;, x = -2750, xend = -1860, y = -3.10, yend = -2.29,
    curvature = -.2, color = &amp;quot;grey25&amp;quot;, size = 0.75, arrow = arrow(length = unit(1.5, &amp;quot;mm&amp;quot;)))+
  annotate(&amp;quot;text&amp;quot; ,x = -2750, y = -3.30, label = &amp;quot;Winner&amp;quot;, family = &amp;quot;Roboto Condensed&amp;quot;, size = 3)+
  labs(title = &amp;quot;Winning the Israeli lottery&amp;quot;, subtitle = &amp;quot;To win, imagine trying to randomly choose a &amp;lt;b&amp;gt;&amp;lt;span style=&amp;#39;color:red&amp;#39;&amp;gt;specific ball&amp;lt;/span&amp;gt;&amp;lt;/b&amp;gt; out of 8,136,744 balls&amp;quot;)+
  theme_void()+
  theme(text = element_text(family = &amp;quot;Roboto Condensed&amp;quot;),
        plot.title.position = &amp;quot;plot&amp;quot;,
        plot.title = element_text(size = 16, face= &amp;quot;bold&amp;quot;),
        plot.subtitle = element_markdown(family = &amp;quot;Roboto Condensed&amp;quot;,size = 12),
        plot.margin = margin(4,2,2,4, unit = &amp;quot;mm&amp;quot;))

lot_p&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/uncertainty-in-the-israeli-lottery/index_files/figure-html/unnamed-chunk-2-1.png&#34; width=&#34;672&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Not easy is it?&lt;/p&gt;
&lt;p&gt;Now that we know the probability of winning at each attempt, let‚Äôs see how it manifests across multiple attempts.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;multiple-attempts---geometric-distribution&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Multiple attempts - Geometric distribution&lt;/h3&gt;
&lt;p&gt;A Bernoulli trial is a random experiment with exactly two outcomes - such as success\failure, heads\tails - in which the probability for each outcome is the same every time &lt;a href=&#34;https://en.wikipedia.org/wiki/Bernoulli_trial&#34;&gt;(Wikipedia)&lt;/a&gt;. This sets the ground for discussing an outcome of a lottery in which you either win or lose.&lt;/p&gt;
&lt;p&gt;But we want to learn more about the &lt;em&gt;distribution of attempts&lt;/em&gt;, and this brings us to the geometric distribution. A &lt;a href=&#34;https://en.wikipedia.org/wiki/Geometric_distribution&#34;&gt;Geometric distribution&lt;/a&gt; enables us to calculate the probability distribution of a number of failures before the first success&lt;a href=&#34;#fn2&#34; class=&#34;footnote-ref&#34; id=&#34;fnref2&#34;&gt;&lt;sup&gt;2&lt;/sup&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Before we begin, we must meet several conditions to use the geometric distribution:&lt;/p&gt;
&lt;p&gt;‚úîÔ∏è Each trial is independent from one another - succeeding in one trial doesn‚Äôt affect the next trial. We know this is true since winning in one lottery won‚Äôt affect your chances of winning the next round.&lt;/p&gt;
&lt;p&gt;‚úîÔ∏è Every trial has an outcome of a success or failure. This assumption is true in our case where each lottery you participate in you either win or lose.&lt;/p&gt;
&lt;p&gt;‚úîÔ∏è The probability of success &lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt; is the same every trial - This is also true given the lottery probabilities are consistent across each game.&lt;/p&gt;
&lt;p&gt;Now that we got the technicalities out of the way we can start exploring some of the uncertainty surrounding the lottery.&lt;/p&gt;
&lt;div id=&#34;winning-at-a-given-trial&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Winning &lt;strong&gt;at&lt;/strong&gt; a given trial&lt;/h4&gt;
&lt;p&gt;We can denote the probability of winning as &lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt;, which in the case of a lottery game is equal to &lt;span class=&#34;math inline&#34;&gt;\(p = \frac{1}{8136744}\)&lt;/span&gt;. What if we wanted to know the probability of winning the lottery on the third try? That means we need two failures and then a success. If the probability of success - guessing the correct numbers - is &lt;span class=&#34;math inline&#34;&gt;\(p = \frac{1}{8136744}\)&lt;/span&gt;, so the probability of a failure is &lt;span class=&#34;math inline&#34;&gt;\(q = 1 - p\)&lt;/span&gt;, in this case &lt;span class=&#34;math inline&#34;&gt;\(q = \frac{8136743}{8136744}\)&lt;/span&gt;. In order to win the lottery on the third try, this means getting two failures and then a success, resulting in a total of &lt;span class=&#34;math inline&#34;&gt;\(k = 3\)&lt;/span&gt; attempts. Thus, the probability of winning on the third attempt is as follows:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(p(3) = (\frac{8136743}{8136744})\cdot(\frac{8136743}{8136744})\cdot(\frac{1}{8136744})\)&lt;/span&gt;, equaling &lt;span class=&#34;math inline&#34;&gt;\(p = 0.0000001228993\)&lt;/span&gt;. In other words there‚Äôs a ~0.0000123% chance we‚Äôll win the lottery &lt;em&gt;exactly&lt;/em&gt; on the third try.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Generalizing, the probability distribution of the number of Bernoulli trials needed to get one success on the &lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt; th trial is: &lt;span class=&#34;math inline&#34;&gt;\(P(X = k) = (1 - p)^{(k-1)} \cdot p\)&lt;/span&gt;.&lt;/strong&gt; We can break this up according to our previous example:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(P\)&lt;/span&gt; stands for the probability of getting our value &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; on the &lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt; attempt. Meaning, we want to win the lottery only on the third attempt.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;So our first two attempts should be a failure, thus a probability of &lt;span class=&#34;math inline&#34;&gt;\(q = 1 - \frac{1}{8136744}\)&lt;/span&gt; multiplied by two (two rounds of failures), written as &lt;span class=&#34;math inline&#34;&gt;\((\frac{8136743}{8136744})^{3 - 1}\)&lt;/span&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Lastly, &lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt; stands for the probability of succeeding, &lt;span class=&#34;math inline&#34;&gt;\(\frac{1}{8136744}\)&lt;/span&gt; occurring exactly on the &lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt; attempt.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The probability we just discussed is also known as the probability mass function (PMF) of the geometric distribution. PMF is a function that gives the probability that a random discrete variable is exactly equal to some value. In our above example, the probability that we‚Äôll win exactly on the third try.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;winning-by-a-given-trial&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Winning &lt;strong&gt;by&lt;/strong&gt; a given trial&lt;/h4&gt;
&lt;p&gt;We don‚Äôt necessarily want to win the lottery on on a specific &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; attempt, but explore the probabilities of winning &lt;em&gt;by&lt;/em&gt; the &lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt;th attempt. Reframing our previous question we can ask &lt;strong&gt;‚Äúwhat is the probability of winning the lottery on at least one of the first 3 attempts?‚Äù&lt;/strong&gt;, bringing us to the Cumulative distribution function (CDF). In a cumulative distribution we calculate the probability that &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; will take a value less than or equal to &lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt; (in our case representing the number of attempts).&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;How does this question change our calculation?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Let‚Äôs assume we‚Äôre still talking about 3 attempts. Our new framed question means we want to win the lottery either on the first attempt, the second or the third. In other words, we want to add the probability of success when &lt;span class=&#34;math inline&#34;&gt;\(P(X = 1)\)&lt;/span&gt; + &lt;span class=&#34;math inline&#34;&gt;\(P(X = 2)\)&lt;/span&gt; + &lt;span class=&#34;math inline&#34;&gt;\(P(X = 3)\)&lt;/span&gt;. Given that our probability of failure is &lt;span class=&#34;math inline&#34;&gt;\(q = 1 - p\)&lt;/span&gt;, we can write the argument as follows: &lt;span class=&#34;math inline&#34;&gt;\(P(X \leq 3) = q^0\cdot p + q^1 \cdot p + q^2 \cdot p\)&lt;/span&gt;, inputting our values of &lt;span class=&#34;math inline&#34;&gt;\({(\frac{8136743}{8136744}})^0 \cdot p \cdot({\frac{8136743}{8136744}})^1\cdot p, ...\)&lt;/span&gt;, resulting in the probability of winning in one of the first three attempts &lt;span class=&#34;math inline&#34;&gt;\(P(X \leq 3) = 0.000000368\)&lt;/span&gt;, also written as a 0.0000368% chance.&lt;/p&gt;
&lt;p&gt;But if we want to look at the first 50 attempts? we‚Äôll have to sum each individual PMF?&lt;/p&gt;
&lt;p&gt;Here‚Äôs exactly the use of the geometric CDF written as &lt;span class=&#34;math inline&#34;&gt;\(P(X &amp;lt;= x) = 1 - q^x\)&lt;/span&gt;. We raise the probability of losing to the power of attempts to win by and deduct it from 1, resulting in the probability of winning by a given trial.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;winning-on-the-first-x-trials&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Winning on the first X trials&lt;/h3&gt;
&lt;p&gt;We just looked at the probability of winning on the first 3 trials, and now that we learned about the CDF we can calculate the probability of winning on the first &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt; trials, for e.g.¬†on the first 100, 1000 and so on. In addition, another important factor we can take into account exploring the cumulative distribution is the money spent reaching each attempt.&lt;/p&gt;
&lt;p&gt;We‚Äôll start by declaring our values. We know the probability for winning the lottery &lt;em&gt;with each ticket we have&lt;/em&gt; is &lt;span class=&#34;math inline&#34;&gt;\(p = \frac{1}{8,136,744}\)&lt;/span&gt; (remember, we get to choose two sets of numbers in each ticket), so let‚Äôs declare that:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;p &amp;lt;- 1/8136744&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Next we know the probability for not winning is &lt;span class=&#34;math inline&#34;&gt;\(q = 1 - p\)&lt;/span&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;q &amp;lt;- 1 - p&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now we can create a data frame to account for some 250,000 attempts. We don‚Äôt need each attempt so we‚Äôll simulate data for the first 50,000 and then have points spread out in a 500 interval jump all the way to the 100,000,000 attempt.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;df_prob &amp;lt;- tibble(trial = c(1:50000, seq(50000, 1e8, 500)))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Once we have that we can calculate both the probability of winning up to a specific attempt and the cumulative amount of money spent reaching there (according to a price ticket of NIS 5.8):&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;df_prob &amp;lt;- df_prob %&amp;gt;% 
  mutate(cdf = 1 - (q)^trial,
         money_spent = trial * 5.8)

head(df_prob)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 6 x 3
##   trial         cdf money_spent
##   &amp;lt;dbl&amp;gt;       &amp;lt;dbl&amp;gt;       &amp;lt;dbl&amp;gt;
## 1     1 0.000000123         5.8
## 2     2 0.000000246        11.6
## 3     3 0.000000369        17.4
## 4     4 0.000000492        23.2
## 5     5 0.000000614        29  
## 6     6 0.000000737        34.8&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Looks good!&lt;/p&gt;
&lt;p&gt;We see our top 6 observations with 3 columns we just defined (from left to right): the lottery raffle (trial), the probability of winning at a given trial until that point (cdf) and the money spent by that trial. Our probability of winning at &lt;em&gt;any&lt;/em&gt; trial is constant (&lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt;), so it‚Äôll be redundant to add that in here.&lt;/p&gt;
&lt;p&gt;Now let‚Äôs look at specific points along our data frame and see how much money is spent reaching there. More specifically, let‚Äôs look at the details of some attempts such as 1; 10; 100; 1000; 2500, 5000, &lt;span class=&#34;math inline&#34;&gt;\(...\)&lt;/span&gt;, 1,000,000, 10,000,000; 50,000,000:&lt;/p&gt;
&lt;style&gt;html {
  font-family: -apple-system, BlinkMacSystemFont, &#39;Segoe UI&#39;, Roboto, Oxygen, Ubuntu, Cantarell, &#39;Helvetica Neue&#39;, &#39;Fira Sans&#39;, &#39;Droid Sans&#39;, Arial, sans-serif;
}

#hvuocxzswv .gt_table {
  display: table;
  border-collapse: collapse;
  margin-left: auto;
  margin-right: auto;
  color: #333333;
  font-size: 16px;
  background-color: #FFFFFF;
  width: auto;
  border-top-style: solid;
  border-top-width: 2px;
  border-top-color: #A8A8A8;
  border-right-style: none;
  border-right-width: 2px;
  border-right-color: #D3D3D3;
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #A8A8A8;
  border-left-style: none;
  border-left-width: 2px;
  border-left-color: #D3D3D3;
}

#hvuocxzswv .gt_heading {
  background-color: #FFFFFF;
  text-align: left;
  border-bottom-color: #FFFFFF;
  border-left-style: none;
  border-left-width: 1px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 1px;
  border-right-color: #D3D3D3;
}

#hvuocxzswv .gt_title {
  color: #333333;
  font-size: 125%;
  font-weight: bold;
  padding-top: 4px;
  padding-bottom: 4px;
  border-bottom-color: #FFFFFF;
  border-bottom-width: 0;
}

#hvuocxzswv .gt_subtitle {
  color: #333333;
  font-size: 85%;
  font-weight: initial;
  padding-top: 0;
  padding-bottom: 4px;
  border-top-color: #FFFFFF;
  border-top-width: 0;
}

#hvuocxzswv .gt_bottom_border {
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
}

#hvuocxzswv .gt_col_headings {
  border-top-style: solid;
  border-top-width: 2px;
  border-top-color: #D3D3D3;
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
  border-left-style: none;
  border-left-width: 1px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 1px;
  border-right-color: #D3D3D3;
}

#hvuocxzswv .gt_col_heading {
  color: #333333;
  background-color: #FFFFFF;
  font-size: 100%;
  font-weight: normal;
  text-transform: inherit;
  border-left-style: none;
  border-left-width: 1px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 1px;
  border-right-color: #D3D3D3;
  vertical-align: bottom;
  padding-top: 5px;
  padding-bottom: 6px;
  padding-left: 5px;
  padding-right: 5px;
  overflow-x: hidden;
}

#hvuocxzswv .gt_column_spanner_outer {
  color: #333333;
  background-color: #FFFFFF;
  font-size: 100%;
  font-weight: normal;
  text-transform: inherit;
  padding-top: 0;
  padding-bottom: 0;
  padding-left: 4px;
  padding-right: 4px;
}

#hvuocxzswv .gt_column_spanner_outer:first-child {
  padding-left: 0;
}

#hvuocxzswv .gt_column_spanner_outer:last-child {
  padding-right: 0;
}

#hvuocxzswv .gt_column_spanner {
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
  vertical-align: bottom;
  padding-top: 5px;
  padding-bottom: 6px;
  overflow-x: hidden;
  display: inline-block;
  width: 100%;
}

#hvuocxzswv .gt_group_heading {
  padding: 8px;
  color: #333333;
  background-color: #FFFFFF;
  font-size: 100%;
  font-weight: initial;
  text-transform: inherit;
  border-top-style: solid;
  border-top-width: 2px;
  border-top-color: #D3D3D3;
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
  border-left-style: none;
  border-left-width: 1px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 1px;
  border-right-color: #D3D3D3;
  vertical-align: middle;
}

#hvuocxzswv .gt_empty_group_heading {
  padding: 0.5px;
  color: #333333;
  background-color: #FFFFFF;
  font-size: 100%;
  font-weight: initial;
  border-top-style: solid;
  border-top-width: 2px;
  border-top-color: #D3D3D3;
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
  vertical-align: middle;
}

#hvuocxzswv .gt_striped {
  background-color: rgba(128, 128, 128, 0.05);
}

#hvuocxzswv .gt_from_md &gt; :first-child {
  margin-top: 0;
}

#hvuocxzswv .gt_from_md &gt; :last-child {
  margin-bottom: 0;
}

#hvuocxzswv .gt_row {
  padding-top: 8px;
  padding-bottom: 8px;
  padding-left: 5px;
  padding-right: 5px;
  margin: 10px;
  border-top-style: solid;
  border-top-width: 1px;
  border-top-color: #D3D3D3;
  border-left-style: none;
  border-left-width: 1px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 1px;
  border-right-color: #D3D3D3;
  vertical-align: middle;
  overflow-x: hidden;
}

#hvuocxzswv .gt_stub {
  color: #333333;
  background-color: #FFFFFF;
  font-size: 100%;
  font-weight: initial;
  text-transform: inherit;
  border-right-style: solid;
  border-right-width: 2px;
  border-right-color: #D3D3D3;
  padding-left: 12px;
}

#hvuocxzswv .gt_summary_row {
  color: #333333;
  background-color: #FFFFFF;
  text-transform: inherit;
  padding-top: 8px;
  padding-bottom: 8px;
  padding-left: 5px;
  padding-right: 5px;
}

#hvuocxzswv .gt_first_summary_row {
  padding-top: 8px;
  padding-bottom: 8px;
  padding-left: 5px;
  padding-right: 5px;
  border-top-style: solid;
  border-top-width: 2px;
  border-top-color: #D3D3D3;
}

#hvuocxzswv .gt_grand_summary_row {
  color: #333333;
  background-color: #FFFFFF;
  text-transform: inherit;
  padding-top: 8px;
  padding-bottom: 8px;
  padding-left: 5px;
  padding-right: 5px;
}

#hvuocxzswv .gt_first_grand_summary_row {
  padding-top: 8px;
  padding-bottom: 8px;
  padding-left: 5px;
  padding-right: 5px;
  border-top-style: double;
  border-top-width: 6px;
  border-top-color: #D3D3D3;
}

#hvuocxzswv .gt_table_body {
  border-top-style: solid;
  border-top-width: 2px;
  border-top-color: #D3D3D3;
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
}

#hvuocxzswv .gt_footnotes {
  color: #333333;
  background-color: #FFFFFF;
  border-bottom-style: none;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
  border-left-style: none;
  border-left-width: 2px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 2px;
  border-right-color: #D3D3D3;
}

#hvuocxzswv .gt_footnote {
  margin: 0px;
  font-size: 90%;
  padding: 4px;
}

#hvuocxzswv .gt_sourcenotes {
  color: #333333;
  background-color: #FFFFFF;
  border-bottom-style: none;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
  border-left-style: none;
  border-left-width: 2px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 2px;
  border-right-color: #D3D3D3;
}

#hvuocxzswv .gt_sourcenote {
  font-size: 90%;
  padding: 4px;
}

#hvuocxzswv .gt_left {
  text-align: left;
}

#hvuocxzswv .gt_center {
  text-align: center;
}

#hvuocxzswv .gt_right {
  text-align: right;
  font-variant-numeric: tabular-nums;
}

#hvuocxzswv .gt_font_normal {
  font-weight: normal;
}

#hvuocxzswv .gt_font_bold {
  font-weight: bold;
}

#hvuocxzswv .gt_font_italic {
  font-style: italic;
}

#hvuocxzswv .gt_super {
  font-size: 65%;
}

#hvuocxzswv .gt_footnote_marks {
  font-style: italic;
  font-size: 65%;
}
&lt;/style&gt;
&lt;div id=&#34;hvuocxzswv&#34; style=&#34;overflow-x:auto;overflow-y:auto;width:auto;height:auto;&#34;&gt;&lt;table class=&#34;gt_table&#34;&gt;
  &lt;thead class=&#34;gt_header&#34;&gt;
    &lt;tr&gt;
      &lt;th colspan=&#34;3&#34; class=&#34;gt_heading gt_title gt_font_normal&#34; style&gt;&lt;b&gt;&lt;span style=&#39;font-family:Roboto Condensed&#39;&gt;Lottery probabilities with the geometric distribution&lt;/span&gt;&lt;/b&gt;&lt;/th&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th colspan=&#34;3&#34; class=&#34;gt_heading gt_subtitle gt_font_normal gt_bottom_border&#34; style&gt;&lt;span style=&#39;font-family:Roboto Condensed&#39;&gt;Lottery probabilities winning by a given attempt, the money spent reaching there and your chances of winning by then&lt;/span&gt;&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;thead class=&#34;gt_col_headings&#34;&gt;
    &lt;tr&gt;
      &lt;th class=&#34;gt_col_heading gt_columns_bottom_border gt_left&#34; rowspan=&#34;1&#34; colspan=&#34;1&#34;&gt;Attempt&lt;/th&gt;
      &lt;th class=&#34;gt_col_heading gt_columns_bottom_border gt_left&#34; rowspan=&#34;1&#34; colspan=&#34;1&#34;&gt;Money spent&lt;sup class=&#34;gt_footnote_marks&#34;&gt;1&lt;/sup&gt;&lt;/th&gt;
      &lt;th class=&#34;gt_col_heading gt_columns_bottom_border gt_left&#34; rowspan=&#34;1&#34; colspan=&#34;1&#34;&gt;% winning by then&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody class=&#34;gt_table_body&#34;&gt;
    &lt;tr&gt;
      &lt;td class=&#34;gt_row gt_left&#34;&gt;1&lt;/td&gt;
      &lt;td class=&#34;gt_row gt_left&#34;&gt;$2&lt;/td&gt;
      &lt;td class=&#34;gt_row gt_left&#34;&gt;0.00001%&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td class=&#34;gt_row gt_left&#34;&gt;10&lt;/td&gt;
      &lt;td class=&#34;gt_row gt_left&#34;&gt;$17&lt;/td&gt;
      &lt;td class=&#34;gt_row gt_left&#34;&gt;0.00012%&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td class=&#34;gt_row gt_left&#34;&gt;100&lt;/td&gt;
      &lt;td class=&#34;gt_row gt_left&#34;&gt;$166&lt;/td&gt;
      &lt;td class=&#34;gt_row gt_left&#34;&gt;0.00123%&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td class=&#34;gt_row gt_left&#34;&gt;500&lt;/td&gt;
      &lt;td class=&#34;gt_row gt_left&#34;&gt;$829&lt;/td&gt;
      &lt;td class=&#34;gt_row gt_left&#34;&gt;0.00614%&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td class=&#34;gt_row gt_left&#34;&gt;1,000&lt;/td&gt;
      &lt;td class=&#34;gt_row gt_left&#34;&gt;$1,657&lt;/td&gt;
      &lt;td class=&#34;gt_row gt_left&#34;&gt;0.01229%&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td class=&#34;gt_row gt_left&#34;&gt;2,500&lt;/td&gt;
      &lt;td class=&#34;gt_row gt_left&#34;&gt;$4,143&lt;/td&gt;
      &lt;td class=&#34;gt_row gt_left&#34;&gt;0.03072%&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td class=&#34;gt_row gt_left&#34;&gt;5,000&lt;/td&gt;
      &lt;td class=&#34;gt_row gt_left&#34;&gt;$8,286&lt;/td&gt;
      &lt;td class=&#34;gt_row gt_left&#34;&gt;0.06143%&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td class=&#34;gt_row gt_left&#34;&gt;10,000&lt;/td&gt;
      &lt;td class=&#34;gt_row gt_left&#34;&gt;$16,571&lt;/td&gt;
      &lt;td class=&#34;gt_row gt_left&#34;&gt;0.12282%&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td class=&#34;gt_row gt_left&#34;&gt;25,000&lt;/td&gt;
      &lt;td class=&#34;gt_row gt_left&#34;&gt;$41,429&lt;/td&gt;
      &lt;td class=&#34;gt_row gt_left&#34;&gt;0.30678%&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td class=&#34;gt_row gt_left&#34;&gt;100,000&lt;/td&gt;
      &lt;td class=&#34;gt_row gt_left&#34;&gt;$165,714&lt;/td&gt;
      &lt;td class=&#34;gt_row gt_left&#34;&gt;1.22147%&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td class=&#34;gt_row gt_left&#34;&gt;250,000&lt;/td&gt;
      &lt;td class=&#34;gt_row gt_left&#34;&gt;$414,286&lt;/td&gt;
      &lt;td class=&#34;gt_row gt_left&#34;&gt;3.02576%&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td class=&#34;gt_row gt_left&#34;&gt;500,000&lt;/td&gt;
      &lt;td class=&#34;gt_row gt_left&#34;&gt;$828,571&lt;/td&gt;
      &lt;td class=&#34;gt_row gt_left&#34;&gt;5.95997%&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td class=&#34;gt_row gt_left&#34;&gt;1,000,000&lt;/td&gt;
      &lt;td class=&#34;gt_row gt_left&#34;&gt;$1,657,143&lt;/td&gt;
      &lt;td class=&#34;gt_row gt_left&#34;&gt;11.56473%&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td class=&#34;gt_row gt_left&#34;&gt;10,000,000&lt;/td&gt;
      &lt;td class=&#34;gt_row gt_left&#34;&gt;$16,571,429&lt;/td&gt;
      &lt;td class=&#34;gt_row gt_left&#34;&gt;70.74129%&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td class=&#34;gt_row gt_left&#34;&gt;50,000,000&lt;/td&gt;
      &lt;td class=&#34;gt_row gt_left&#34;&gt;$82,857,143&lt;/td&gt;
      &lt;td class=&#34;gt_row gt_left&#34;&gt;99.78557%&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
  
  &lt;tfoot&gt;
    &lt;tr class=&#34;gt_footnotes&#34;&gt;
      &lt;td colspan=&#34;3&#34;&gt;
        &lt;p class=&#34;gt_footnote&#34;&gt;
          &lt;sup class=&#34;gt_footnote_marks&#34;&gt;
            &lt;em&gt;1&lt;/em&gt;
          &lt;/sup&gt;
           
          Money spent corresponds to the cumulative number of attempts played
          &lt;br /&gt;
        &lt;/p&gt;
      &lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tfoot&gt;
&lt;/table&gt;&lt;/div&gt;
&lt;p&gt;In the above table I printed specific observations along the lottery‚Äôs cumulative geometric distribution. In our left column we have the trial number, next the approximate amount of money spent up to that trial and lastly the percent of winning by that given trial. Notice that I converted the New Israeli Shekels to dollars ($1 dollar = ~ NIS 3.5).&lt;/p&gt;
&lt;p&gt;If we played 100 consecutive games with the same number, we would spend 166 dollars by that point and have only a 0.00123% chance of winning. We only pass the 1% (!) chance of winning after buying more than 100,000 tickets, spending a total of $165,714 dollars.&lt;br /&gt;
&lt;strong&gt;To pass the 10% chance of winning you‚Äôd have to play 1,000,000 games and spend ~1,600,000 dollars! Reminder: the default prize is only some $1,412,000!&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;dumber.jpg&#34; width=&#34;246&#34; height=&#34;80%&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;average-number-of-attempts&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Average number of attempts&lt;/h3&gt;
&lt;p&gt;An interesting feature of the geometric distribution is that we can calculate the expected number of attempts and variance of the distribution. The expected number of attempts in the discussed geometric cumulative distribution is &lt;span class=&#34;math inline&#34;&gt;\(E(X) = \frac{1}{p}\)&lt;/span&gt; with a variance of &lt;span class=&#34;math inline&#34;&gt;\(var(X) = \frac{1-p}{p^2}\)&lt;/span&gt;. &lt;span class=&#34;math inline&#34;&gt;\(E(X)\)&lt;/span&gt; is basically the expected value for the number of independent trials needed to get the first success (Think of it as an average, only theoretically). So in our lottery example the expected number of attempts to reach a success is &lt;span class=&#34;math inline&#34;&gt;\(E(X) = \frac{1}{\frac{1}{8136744}}\)&lt;/span&gt;, resulting in 8,136,744 attempts.&lt;/p&gt;
&lt;p&gt;R has built in functions for working with the geometric distribution such as &lt;code&gt;pgeom&lt;/code&gt;, &lt;code&gt;rgeom&lt;/code&gt;, &lt;code&gt;qgeom&lt;/code&gt; and &lt;code&gt;dgeom&lt;/code&gt; which you can explore more &lt;a href=&#34;https://www.statology.org/dgeom-pgeom-qgeom-rgeom-r/&#34;&gt;here&lt;/a&gt;. For the purpose of exploring the mean we can use the &lt;code&gt;rgeom&lt;/code&gt; function which generates a value representing the number of failures before a success occurred. For example, let‚Äôs see how many failures we‚Äôre required to reach one success:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;rgeom(n = 1,p = p)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 30687199&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In the above example &lt;code&gt;rgeom&lt;/code&gt; takes the number of rounds (n = 1) and the probability of winning (p = p). The outputted value indicates the number of failures before our success.&lt;/p&gt;
&lt;p&gt;Using this we can calculate the average number of attempts from 2,000,000 games:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mean(rgeom(2e6, p))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 8129065&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Pretty close to our expected value!&lt;/p&gt;
&lt;p&gt;So what does the &lt;span class=&#34;math inline&#34;&gt;\(E(X)\)&lt;/span&gt; mean in terms of the lottery? &lt;strong&gt;You‚Äôd have to play approximately 8,136,744 games to win the lottery, spending a total of NIS 47,193,115 (~$13,483,747) to win approximately NIS 5M (approximately 1.42M dollars)!&lt;/strong&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;conclusion&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Conclusion&lt;/h3&gt;
&lt;p&gt;In this post we were able to uncover and better understand some of the uncertainty that covers a lottery game. Using the geometric distribution we explored the probability of winning the lottery at a specific event, and winning it in the form of a cumulative distribution - Chances of winning up to a given trial.&lt;/p&gt;
&lt;p&gt;Unfortunately, the numbers aren‚Äôt in our favor. You‚Äôd find yourself spending a great deal of money before actually winning the lottery. I‚Äôm definitely not going to tell you what to do with your money, but I hope this blog post helped you understand a little better the chances of (not) winning the lottery. But hey, apparently a &lt;a href=&#34;https://www.businessinsider.com/powerball-lottery-playing-same-numbers-odds-of-winning-2018-11&#34;&gt;New Yorker won the lottery after participating each week for 25 years&lt;/a&gt; so you never know.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;further-reading-exploring&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Further reading \ exploring&lt;/h3&gt;
&lt;p&gt;Two resources I found extremely valuable in learning more about the geometric distribution:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;The &lt;a href=&#34;https://en.wikipedia.org/wiki/Geometric_distribution&#34;&gt;Geometric distribution Wikipedia‚Äôs page&lt;/a&gt;. I‚Äôm constantly amazed at the vast amount and well articulated statistical pages they have.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Continuing on that, I found the resource that the Wikipedia page relies on extremely helpful: ‚ÄúA modern introduction to probability and statistics : understanding why and how‚Äù.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;If you‚Äôre more of a video kind of person, I highly recommend a video by &lt;a href=&#34;https://www.youtube.com/channel/UCEWpbFLzoYGPfuWUMFPSaoA&#34;&gt;The Organic Chemistry Tutor&lt;/a&gt; about the &lt;a href=&#34;https://www.youtube.com/watch?v=d5iAWPnrH6w&amp;amp;t=1s&#34;&gt;Geometric distribution&lt;/a&gt;. I think he does a superb job in explaining different various statistical analysis and always enjoys his videos.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;notes&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Notes&lt;/h3&gt;
&lt;/div&gt;
&lt;div class=&#34;footnotes&#34;&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li id=&#34;fn1&#34;&gt;&lt;p&gt;An amusing anecdote is that the Pais organization offers information about ‚ÄòHot‚Äô numbers and the &lt;a href=&#34;https://www.pais.co.il/lotto/statistics.aspx&#34;&gt;frequency of appearance for each number&lt;/a&gt;. Considering that the lottery is random I wouldn‚Äôt rely on such a pattern‚Ä¶&lt;a href=&#34;#fnref1&#34; class=&#34;footnote-back&#34;&gt;‚Ü©Ô∏é&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn2&#34;&gt;&lt;p&gt;In this blog post I only explore one aspect of the geometric PMF by looking at number of failures before the first success in a set of &lt;span class=&#34;math inline&#34;&gt;\(k \in \{0 , 1, 2, ...\}\)&lt;/span&gt; attempts. To read more about the PMF I recommend starting with the Wikipedia page of the &lt;a href=&#34;https://en.wikipedia.org/wiki/Geometric_distribution&#34;&gt;Geometric distribution&lt;/a&gt;.&lt;a href=&#34;#fnref2&#34; class=&#34;footnote-back&#34;&gt;‚Ü©Ô∏é&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
      
            <category>probability</category>
      
      
            <category>R</category>
      
    </item>
    
    <item>
      <title>Learning Tfidf with Political Theorists</title>
      <link>/post/learning-tfidf-with-political-theorists/</link>
      <pubDate>Sun, 31 May 2020 00:00:00 +0000</pubDate>
      
      <guid>/post/learning-tfidf-with-political-theorists/</guid>
      <description>
&lt;script src=&#34;/rmarkdown-libs/kePrint/kePrint.js&#34;&gt;&lt;/script&gt;


&lt;style&gt;
p.caption {
  font-size: 0.9em;
}
&lt;/style&gt;
&lt;p&gt;Thanks to &lt;a href=&#34;https://almogsi.com/&#34;&gt;Almog Simchon&lt;/a&gt; for insightful comments on a first draft of this post.&lt;/p&gt;
&lt;div id=&#34;introduction&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Introduction&lt;/h3&gt;
&lt;p&gt;Learning &lt;code&gt;R&lt;/code&gt; for the past nine months or so has enabled me to explore new topics that are of interest to me, one of them being text analysis. In this post I‚Äôll explain what is Term-Frequency Inverse Document Frequency (tf-idf) and how it can help us explore important words for a document within a corpus of documents&lt;a href=&#34;#fn1&#34; class=&#34;footnote-ref&#34; id=&#34;fnref1&#34;&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt;. The analysis helps in finding words that are common in a given document but are rare across all other documents.&lt;/p&gt;
&lt;p&gt;Following the explanation we‚Äôll implement the method on four great philosophers‚Äô books: ‚ÄòRepublic‚Äô (Plato), ‚ÄòThe Prince‚Äô (Machiavelli), ‚ÄòLeviathan‚Äô (Hobbes) and lastly, one of my favorite books - ‚ÄòOn Liberty‚Äô (Mill) üòç. Lastly, we‚Äôll see how tf-idf compares to a Bag of Words analysis (word count) and how using both can benefit your exploring of text.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;The post is aimed for anyone exploring text-analysis&lt;/strong&gt; and wants to learn about tf-idf. &lt;strong&gt;I will be using &lt;code&gt;R&lt;/code&gt; to analyze our data but won‚Äôt be explaining the different functions&lt;/strong&gt;, as this post focuses on the tf-idf analysis. If you wish to see the code, feel free to download or explore the .Rmd source code on my &lt;a href=&#34;https://github.com/AmitLevinson/amitlevinson.com/blob/master/content/post/learning-tfidf-with-political-theorists/index.Rmd&#34;&gt;github repository&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;term-frequency&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Term frequency&lt;/h3&gt;
&lt;p&gt;tf-idf gauges a word‚Äôs value according to two parameters: The first parameter is the &lt;strong&gt;term-frequency of a word: How common is a word in a given document&lt;/strong&gt; (Bag of Words analysis); one method to calculate term frequency of a word is just to count the total number of times each words appears. Another method - which we‚Äôll use in the tf-idf - is, after summing the total number of times a word appears, we‚Äôll divide it by the total number of words in that document, &lt;strong&gt;describing term frequency as such:&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[tf = \frac{\textrm{Number of times a word appears in a document}}{\textrm{Total number of words in that document}}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Also written as &lt;span class=&#34;math inline&#34;&gt;\(tf(t,d)\)&lt;/span&gt; where &lt;span class=&#34;math inline&#34;&gt;\(t\)&lt;/span&gt; is the number of times a term appears out of all words in document &lt;span class=&#34;math inline&#34;&gt;\(d\)&lt;/span&gt;. Using the above method we‚Äôll have the &lt;strong&gt;proportion&lt;/strong&gt; of each word in our document, a value ranging from 0 to 1, where common words will have higher values.&lt;/p&gt;
&lt;p&gt;While this gives us a value gauging how common a word is in a document, what happens when we have many words across many documents? How do we find &lt;ins&gt;unique&lt;/ins&gt; words for each document? This brings us to &lt;em&gt;idf&lt;/em&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;inverse-document-frequency&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Inverse document frequency&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Inverse document frequency accounts for the occurrence of a word across all documents, thereby giving a higher value to words appearing in less documents.&lt;/strong&gt; In this case, for each term we will calculate the log ratio&lt;a href=&#34;#fn2&#34; class=&#34;footnote-ref&#34; id=&#34;fnref2&#34;&gt;&lt;sup&gt;2&lt;/sup&gt;&lt;/a&gt; of all documents divided by the number of documents that word appears in. This gives us the following:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ idf = \log {\frac{\textrm{N documents in corpus}}{\textrm{n documents containing the term}}}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Also written as &lt;span class=&#34;math inline&#34;&gt;\(idf = \log{\frac{N}{n(t)}}\)&lt;/span&gt; Where &lt;span class=&#34;math inline&#34;&gt;\(N\)&lt;/span&gt; is the total number of documents in our corpus and &lt;span class=&#34;math inline&#34;&gt;\(n(t)\)&lt;/span&gt; is the number of documents the word appears within our corpus of documents.&lt;/p&gt;
&lt;p&gt;To those unfamiliar, a logarithmic transformation helps in reducing wide-ranged numbers to smaller scopes. In this case, if we have 7 documents, and our term appears in all 7 documents, we‚Äôll have following idf value: &lt;span class=&#34;math inline&#34;&gt;\(log_e(\frac{7}{7}) = 0\)&lt;/span&gt;. What if we have a term that appears in only 1 document out of all 7 documents? We‚Äôll have the following: &lt;span class=&#34;math inline&#34;&gt;\(log_e(\frac{7}{1}) = 1.945\)&lt;/span&gt;. Even if a word appears in only 1 document out of 100, a logarithmic transformation will reduce its high value to mitigate bias when we multiply it with its &lt;span class=&#34;math inline&#34;&gt;\(tf\)&lt;/span&gt; value.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;So what do we understand from the idf?&lt;/strong&gt; Since our numerate always remains the same (N documents in corpus), the &lt;em&gt;idf&lt;/em&gt; of a word is contingent upon how common it is &lt;em&gt;across&lt;/em&gt; documents. Words that appear in a small number of documents will have a higher &lt;em&gt;idf&lt;/em&gt;, while words that are common across documents will have a lower &lt;em&gt;idf&lt;/em&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;term-frequency-inverse-document-frequency-tfidf&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Term-Frequency Inverse Document Frequency (tfidf)&lt;/h3&gt;
&lt;p&gt;Once we have the term frequency and inverse document frequency for each word we can calculate the tf-idf by multiplying the two: &lt;span class=&#34;math inline&#34;&gt;\(tf(t,d) \cdot idf(t,D)\)&lt;/span&gt; where &lt;span class=&#34;math inline&#34;&gt;\(D\)&lt;/span&gt; is our corpus of documents.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;To summarize our explanation:&lt;/strong&gt; The two paramteres used to calculate the tf-idf provide each word with a value for its importance to that document in that corpus of text. Ideally We take &lt;strong&gt;words that are &lt;u&gt;common within a document&lt;/u&gt; and that are &lt;u&gt;rare across documents&lt;/u&gt;&lt;/strong&gt;. I write ideally because as we‚Äôll see soon, we might have words that are extremely common in one document but are filtered out because they‚Äôre evident in all documents (can happen in a small corpus of documents). This also highlights the question as to what is &lt;em&gt;important&lt;/em&gt;; I define important as contributing to understanding a document in comparison to all other documents.&lt;/p&gt;
&lt;div class=&#34;figure&#34; style=&#34;text-align: center&#34;&gt;&lt;span id=&#34;fig:unnamed-chunk-2&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;/post/learning-tfidf-with-political-theorists/index_files/figure-html/unnamed-chunk-2-1.png&#34; alt=&#34;Using tf-idf we can calculate how common a word is within a document and how rare is it across documents&#34; width=&#34;85%&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 1: Using tf-idf we can calculate how common a word is within a document and how rare is it across documents
&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;Now that we have some background as to how tf-idf works, let‚Äôs dive in to our case study.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;tf-idf-on-political-theorists.&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;TF-IDF on political theorists.&lt;/h3&gt;
&lt;p&gt;I‚Äôm a big fan of political theory. I have a small collection at home and always like to read and learn more about it. Except for Mill, we read Plato, Machiavelli and Hobbes in our BA first semester course in political theory. While some of the theorists overlap to some degree, over-all they discuss different topics. tf-idf will help us distinguish important words specific to each book, in a comparison across all books.&lt;/p&gt;
&lt;p&gt;Before we conduct our tf-idf we‚Äôd like to explore our text a bit. The following exploratory analysis is inspired from Julia Silge‚Äôs blog post &lt;a href=&#34;https://juliasilge.com/blog/term-frequency-tf-idf/&#34;&gt;‚ÄòTerm Frequency and tf-idf Using Tidy Data Principles‚Äô&lt;/a&gt;, a fantastic read.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;data-collection-analysis&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Data collection &amp;amp; Analysis&lt;/h3&gt;
&lt;p&gt;The package we‚Äôll use to gather the data is the &lt;code&gt;{gutenbergr}&lt;/code&gt; package. It enables us to access the &lt;a href=&#34;https://www.gutenberg.org/&#34;&gt;Project Gutenberg&lt;/a&gt; free books, a library of over 60,000 free books. As many other amazing things in &lt;code&gt;R&lt;/code&gt; someone, in this case David Robinson, created a package for it. All we need to do is download them to our computer.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;Mill &amp;lt;- gutenberg_download(34901)
Hobbes &amp;lt;- gutenberg_download(3207)
Machiavelli &amp;lt;- gutenberg_download(1232)
Plato &amp;lt;- gutenberg_download(150)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Several of the books contain sections at the beginning or at the end that aren‚Äôt relevant for our analysis. For example long introductions from contemporary scholars; another whole different book at the end, etc. These can confound our analysis and therefore we‚Äôll exclude them. In order to conduct our analysis we also need all the books we collected in one object.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Once we are able to clean the books, this is what our text looks like:&lt;/strong&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;remove_text &amp;lt;- function(book, low_id, top_id = max(rowid), author = deparse(substitute(book))){
  book %&amp;gt;%
  mutate(author = as.factor(author)) %&amp;gt;% 
  rowid_to_column() %&amp;gt;% 
  filter(rowid &amp;gt;= {{low_id}}, rowid &amp;lt;= {{top_id}}) %&amp;gt;% 
  select(author, text, -c(rowid, gutenberg_id))}

books &amp;lt;- rbind(
  remove_text(Mill, 454),
  remove_text(Hobbes, 360, 22317),
  remove_text(Machiavelli, 464, 3790),
  remove_text(Plato, 606))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 45,490 x 2
##    author text                                                                  
##    &amp;lt;fct&amp;gt;  &amp;lt;chr&amp;gt;                                                                 
##  1 Mill   &amp;quot;&amp;quot;                                                                    
##  2 Mill   &amp;quot;&amp;quot;                                                                    
##  3 Mill   &amp;quot;CHAPTER I.&amp;quot;                                                          
##  4 Mill   &amp;quot;&amp;quot;                                                                    
##  5 Mill   &amp;quot;INTRODUCTORY.&amp;quot;                                                       
##  6 Mill   &amp;quot;&amp;quot;                                                                    
##  7 Mill   &amp;quot;&amp;quot;                                                                    
##  8 Mill   &amp;quot;The subject of this Essay is not the so-called Liberty of the Will, ~
##  9 Mill   &amp;quot;unfortunately opposed to the misnamed doctrine of Philosophical&amp;quot;     
## 10 Mill   &amp;quot;Necessity; but Civil, or Social Liberty: the nature and limits of th~
## # ... with 45,480 more rows&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Each row is some text with chapters separated by headings and a column referencing who is the author. Our data frame consists of ~45,000 rows with the filtered text from our four books. Tf-idf can also be done on any n-grams we choose (number of consequent words). We could calculate the tf-idf for each bigram of words (two-words), trigram, etc. I find a unigram an appropriate approach both for tf-idf and especially now when we want to learn more about it. &lt;strong&gt;We just saw that our text is in the form of sentences, so let‚Äôs break it into single words.&lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 12 x 4
## # Groups:   author [4]
##    author      word      n sum_words
##    &amp;lt;fct&amp;gt;       &amp;lt;chr&amp;gt; &amp;lt;int&amp;gt;     &amp;lt;int&amp;gt;
##  1 Hobbes      the   14536    207849
##  2 Hobbes      of    10523    207849
##  3 Hobbes      and    7113    207849
##  4 Plato       the    7054    118639
##  5 Plato       and    5746    118639
##  6 Plato       of     4640    118639
##  7 Mill        the    3019     48006
##  8 Mill        of     2461     48006
##  9 Machiavelli the    2006     34821
## 10 Mill        to     1765     48006
## 11 Machiavelli to     1468     34821
## 12 Machiavelli and    1333     34821&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We see that stop-words dominant the frequency of occurrences. That makes sense as they are commonly used, but they‚Äôre not usually helpful for learning about a text, specifically here. &lt;strong&gt;We‚Äôll start by exploring how the word frequencies occur within a text:&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/post/learning-tfidf-with-political-theorists/index_files/figure-html/unnamed-chunk-8-1.png&#34; width=&#34;672&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The plot above shows the frequency of terms across documents. We see some words that appear frequently (higher proportion = right side of the x-axis) and many words that are rarer (low proportion). Actually, I had to limit the x-axis or otherwise it would distort the plot with words that are extremely common.&lt;/p&gt;
&lt;p&gt;To help find useful words with the highest tf-idf from each book, we‚Äôll remove stop words before we extract the words with a high tf-idf value:&lt;/p&gt;
&lt;table class=&#34;table&#34; style=&#34;width: auto !important; margin-left: auto; margin-right: auto;&#34;&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:center;&#34;&gt;
Author
&lt;/th&gt;
&lt;th style=&#34;text-align:center;&#34;&gt;
Word
&lt;/th&gt;
&lt;th style=&#34;text-align:center;&#34;&gt;
n
&lt;/th&gt;
&lt;th style=&#34;text-align:center;&#34;&gt;
Sum words
&lt;/th&gt;
&lt;th style=&#34;text-align:center;&#34;&gt;
Term Frequency
&lt;/th&gt;
&lt;th style=&#34;text-align:center;&#34;&gt;
IDF
&lt;/th&gt;
&lt;th style=&#34;text-align:center;&#34;&gt;
TF-IDF
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
Mill
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
opinion
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
150
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
48006
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.0094132
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.0000000
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.0000000
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
Hobbes
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
god
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
1047
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
207849
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.0149024
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.0000000
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.0000000
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
Machiavelli
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
prince
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
185
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
34821
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.0172704
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.2876821
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.0049684
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
Plato
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
true
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
485
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
118639
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.0152953
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.0000000
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.0000000
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;tfoot&gt;
&lt;tr&gt;
&lt;td style=&#34;padding: 0; border: 0;&#34; colspan=&#34;100%&#34;&gt;
&lt;sup&gt;&lt;/sup&gt; Random sample of words and their corresponding tf-idf values
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tfoot&gt;
&lt;/table&gt;
&lt;p&gt;Above we have our tf-idf for a given word from each document. I removed stop-words and calculated the tf-idf for each word in each book. For Hobbes the word ‚ÄòGod‚Äô appears 1047 times, thus has a &lt;span class=&#34;math inline&#34;&gt;\(tf\)&lt;/span&gt; of &lt;span class=&#34;math inline&#34;&gt;\(\frac {1047} {207849}\)&lt;/span&gt; and an idf of 0 (since it appears in all documents), so it‚Äôll have a tf-idf of 0.&lt;/p&gt;
&lt;p&gt;With Machiavelli the word prince appears 185 times, with a &lt;span class=&#34;math inline&#34;&gt;\(tf\)&lt;/span&gt; of &lt;span class=&#34;math inline&#34;&gt;\(\frac {185} {34821}\)&lt;/span&gt;, resulting in a proportion of 0.0173. The word prince has an idf of 0.288 &lt;span class=&#34;math inline&#34;&gt;\((log_e(\frac 4 {3}))\)&lt;/span&gt;, as there are 4 documents and it appears in 3 of them, so a total tf-idf value of &lt;span class=&#34;math inline&#34;&gt;\(0.0173 \cdot 0.288\)&lt;/span&gt; = &lt;span class=&#34;math inline&#34;&gt;\(0.00497\)&lt;/span&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;tf-idf-plot&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Tf-idf plot&lt;/h3&gt;
&lt;p&gt;As we wrap up our tf-idf analysis, &lt;strong&gt;We don‚Äôt want to see all words and their tf-idf, but only words with the highest tf-idf value&lt;/strong&gt; for each author, indicating the importance of a word to a given document. We can look at these words by plotting the top 10 highest valued tf-idf words for each author:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt; ggplot(data = books_for_plot, aes(x = word, y = tf_idf, fill = author))+
  geom_col(show.legend = FALSE)+
  labs(x = NULL, y = &amp;quot;tf-idf&amp;quot;)+
  coord_flip()+
  scale_x_reordered()+
  facet_wrap(~ author, scales = &amp;quot;free_y&amp;quot;, ncol = 2)+
  labs(title = &amp;quot;&amp;lt;b&amp;gt;Term Frequency Inverse Document Frequency&amp;lt;/b&amp;gt; - Political theorists&amp;quot;,
       subtitle = &amp;quot;tf-idf for The Leviathan (Hobbes), On Liberty (Mill), The Prince (Machiavelli)\nand Republic (Plato)&amp;quot;)+
  scale_fill_manual(values = plot_colors)+
  theme_post+
  theme(plot.title = element_markdown())&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/learning-tfidf-with-political-theorists/index_files/figure-html/unnamed-chunk-11-1.png&#34; width=&#34;672&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Lovely!&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Let‚Äôs review each book and see what we can learn from our tf-idf analysis. My memory of these books is kind of rusty but I‚Äôll try my best:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Hobbes:&lt;/strong&gt; Hobbes in his book describes the &lt;em&gt;natural&lt;/em&gt; state of human beings and how they can leave it by revoking many of their right to the &lt;em&gt;sovereign&lt;/em&gt; who will facilitate order. In his book he describes the soveragin (note the ‚Äòa‚Äô) as needed to be strict, rigorous and &lt;em&gt;hath&lt;/em&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Machiavelli:&lt;/strong&gt; Machiavelli provides a leader with a guide on how to rule his country. He prefaces his book with an introduction letter to the &lt;em&gt;Duke&lt;/em&gt;, the recipient of his work. Machiavelli throughout the book conveys his message with examples of many &lt;em&gt;princes&lt;/em&gt;, &lt;em&gt;Alexander&lt;/em&gt; the great, the &lt;em&gt;Orsini&lt;/em&gt; brothers and more. Several of his examples include mentioning of Italy (where he resides), specifically &lt;em&gt;Venetians&lt;/em&gt; and &lt;em&gt;Milan&lt;/em&gt;.&lt;br /&gt;
&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Mill:&lt;/strong&gt; Mill in his book ‚ÄòOn Liberty‚Äô describes the importance of freedom and liberty for individuals. He does so by describing the relation between people and their &lt;em&gt;society&lt;/em&gt; and other relations with the &lt;em&gt;social&lt;/em&gt;. He highlights in his discussion on liberty a &lt;em&gt;person‚Äôs&lt;/em&gt; belonging; these can be &lt;em&gt;Feelings&lt;/em&gt; or basically anything &lt;em&gt;personal&lt;/em&gt;. Protecting the personal is important for the &lt;em&gt;development&lt;/em&gt; of both society and that of the individual.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Plato:&lt;/strong&gt; Plato‚Äôs book consists of 10 chapters and it is by far the longest compared to the others. The book is written in the form of a dialogue with &lt;em&gt;replies&lt;/em&gt; between Socrate and his discussants. Along Socrate‚Äôs journey to finding out what is the meaning of justice he talks to many people, among them &lt;em&gt;Glaucon&lt;/em&gt;, &lt;em&gt;Thrasymachus&lt;/em&gt; and &lt;em&gt;Adeimantus&lt;/em&gt;. In one section Socrates describes a just society with distinct &lt;em&gt;classes&lt;/em&gt; such as the &lt;em&gt;guardians&lt;/em&gt;. The classes should receive appropriate education, for e.g.¬†&lt;em&gt;gymnastics&lt;/em&gt; for the guardians.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;With the above analysis we were able to explore uniqueness of words for each book across all books. &lt;strong&gt;Some words provided us with great insights while others didn‚Äôt necessarily help us despite their uniqeness&lt;/strong&gt;, for example, the names of discussants with Socrate. Tf-idf gauges them as important (as to how I defined importance here) to distinguish between Plato‚Äôs book and the others, but I‚Äôm sure they‚Äôre not the first words that come to mind when someone talks about the Republic.&lt;/p&gt;
&lt;p&gt;The analysis also shows this methodology‚Äôs value addition is not in just applying tf-idf - or any other statistical analysis ‚Äì rather its power lies in its explanatory abilities. In other words, &lt;strong&gt;tf-idf provides us with a value indicating the importance of a word to a given document within a corpus, it is our job to take that extra step interpreting and contextualizing the output.&lt;/strong&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;comparing-to-bag-of-words-bog&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Comparing to Bag Of Words (BOG)&lt;/h3&gt;
&lt;p&gt;A common text analysis is a word count I discussed earlier, also known as Bag of Words (BoW). This is an easy to understand method that can be done easily when exploring text. However, relying only on a bag of words method to draw insights can limit its usefulness if other analytic methods are not also included. The BoW relies only on the frequency of a word, so if a word is common across all documents, it might show up in all of them and not contribute to finding &lt;em&gt;unique words&lt;/em&gt; for each document.&lt;/p&gt;
&lt;p&gt;Now that we have our books we can also explore the raw occurrence of each word to compare it to our above tf-idf analysis:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(data = bow_books, aes(x = reorder(word_with_color,n), y = n, fill = author))+
  geom_col(show.legend = FALSE)+
  labs(x = NULL, y = &amp;quot;Word Frequency&amp;quot;)+
  coord_flip()+
  scale_x_reordered()+
  facet_wrap(~ author, scales = &amp;quot;free&amp;quot;, ncol = 2)+
  labs(title = &amp;quot;&amp;lt;b&amp;gt;Term Frequency&amp;lt;/b&amp;gt; - Political theorists&amp;quot;)+
  scale_fill_manual(values = plot_colors)+
  theme_post+
  theme(axis.text.y = element_markdown(),
        plot.title = element_markdown(),
        strip.text = element_text(color = &amp;quot;grey50&amp;quot;))&lt;/code&gt;&lt;/pre&gt;
&lt;div class=&#34;figure&#34; style=&#34;text-align: center&#34;&gt;&lt;span id=&#34;fig:unnamed-chunk-13&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;/post/learning-tfidf-with-political-theorists/index_files/figure-html/unnamed-chunk-13-1.png&#34; alt=&#34;Term frequency plot with words that are common across documents in bold&#34; width=&#34;672&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 2: Term frequency plot with words that are common across documents in bold
&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;&lt;strong&gt;The above plot amplifies, in my opinion, tf-idf‚Äôs contribution in finding unique words for each document.&lt;/strong&gt; While many of the words are similar to those we found in the previous tf-idf analysis, we also draw words that are common across documents. For example, we see the frequency of ‚ÄòTime‚Äô, ‚ÄòPeople‚Äô and ‚ÄòNature‚Äô twice in different books and words such as ‚ÄòTrue‚Äô and ‚ÄòTruth‚Äô with similar meanings do so too (however this could have happened in tf-idf too).&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;However, the Bag of Words also provided new words we didn‚Äôt see earlier.&lt;/strong&gt; Here we can learn on new words like Power in Hobbes, Opinions in Mill and more. With the bag of words we get words that are common without controlling for other texts, while the tf-idf searches for words that are common within but are rare across.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;closing-remarks&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Closing remarks&lt;/h3&gt;
&lt;p&gt;In this post we learned the term frequency inverse document frequency (tf-idf) analysis and implemented it on four great political theorists. We finished by exploring tfidf in comparison to a bag of words analysis and showed the benefits of each. This also emphasizes how we define &lt;em&gt;important&lt;/em&gt;: Important to a document by itself or important to a document compared to other documents.
The definition of ‚Äòimportant‚Äô here also highlights tf-idf heuristic quantifying approach (&lt;a href=&#34;https://en.wikipedia.org/wiki/Tf%E2%80%93idf&#34;&gt;specifically the idf&lt;/a&gt;) and thus should be used with caution. If you are aware of theoretical development of it I‚Äôd be glad to read more about it.&lt;/p&gt;
&lt;p&gt;By now you should be equipped to give tf-idf a try yourself on a corpus of documents you find appropriate.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;where-to-next&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Where to next&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Further reading about text analysis - If you want to read more on text mining with R, I highly recommend the Julia Silge &amp;amp; David Robinson‚Äôs &lt;a href=&#34;https://www.tidytextmining.com/&#34;&gt;text mining with R book&lt;/a&gt;and/or exploring the &lt;a href=&#34;https://quanteda.io/&#34;&gt;&lt;code&gt;{quanteda}&lt;/code&gt;&lt;/a&gt; package.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Text datasets - As to finding text data, you can try the &lt;code&gt;{gutenbergr}&lt;/code&gt; package that gives access to thousands of books, a &lt;a href=&#34;https://github.com/rfordatascience/tidytuesday&#34;&gt;#TidyTuesday&lt;/a&gt; data set or collect tweets from Twitter using the &lt;code&gt;{rtweet}&lt;/code&gt; package.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Other posts of mine - If you‚Äôre interested in other posts of mine where I explore some text you can read my &lt;a href=&#34;https://amitlevinson.com/2020/04/20/israeli-elections-on-twitter/&#34;&gt;Israeli elections Twitter tweets analysis&lt;/a&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;That‚Äôs it for now. Feel free to contact me for any and all comments!&lt;/p&gt;
&lt;div id=&#34;notes&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Notes&lt;/h4&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;footnotes&#34;&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li id=&#34;fn1&#34;&gt;&lt;p&gt;A single document can be a book, chapter, paragraph or sentence, it all depends on your research and what you define as an ‚Äòentity‚Äô within a corpus of text.&lt;a href=&#34;#fnref1&#34; class=&#34;footnote-back&#34;&gt;‚Ü©&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn2&#34;&gt;&lt;p&gt;&lt;strong&gt;What‚Äôs log ratio?&lt;/strong&gt; In general, and for the purpose of the tf-idf, a logarithm transformation (in short &lt;span class=&#34;math inline&#34;&gt;\(log\)&lt;/span&gt;) helps in reducing wide ranged numbers to smaller scopes. Assuming we have the following &lt;span class=&#34;math inline&#34;&gt;\(\log _{2}(16) = x\)&lt;/span&gt;, we ask ourselves (and calculate) 2 in the power of what (x) will give us 16. so in this case 2^3 will give us 16, which is basically written as &lt;span class=&#34;math inline&#34;&gt;\(\log _{2}(16) = 3\)&lt;/span&gt;. In order to generalize it, &lt;span class=&#34;math inline&#34;&gt;\(\log _{b}(x) = y\)&lt;/span&gt;, means b is the base we will raise to the power of y to reach x. Therefore written oppositely as &lt;span class=&#34;math inline&#34;&gt;\(b^y = x\)&lt;/span&gt;. The common uses of log are &lt;span class=&#34;math inline&#34;&gt;\(\log_2\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(\log_{10}\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(log_e\)&lt;/span&gt;, also written as plain log.&lt;a href=&#34;#fnref2&#34; class=&#34;footnote-back&#34;&gt;‚Ü©&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
      
            <category>tidytext</category>
      
      
            <category>R</category>
      
    </item>
    
    <item>
      <title>Israeli elections on Twitter</title>
      <link>/post/israeli-elections-on-twitter/</link>
      <pubDate>Mon, 20 Apr 2020 00:00:00 +0000</pubDate>
      
      <guid>/post/israeli-elections-on-twitter/</guid>
      <description>


&lt;div id=&#34;introduction&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Introduction&lt;/h3&gt;
&lt;p&gt;Israel had its 3rd election within 12 months on March 2, 2020. This is because our Knesset - Hebrew term for house of representatives - wasn‚Äôt able to form or hold a government after each of the previous elections. As I won‚Äôt get into the politics of why they didn‚Äôt succeed in forming one (get it? politics üòâ), I do want to take the opportunity and analyze some tweets posted in the time before and after the elections.&lt;br /&gt;
When we think of a data aggregating tweets, many questions arise - who, what, when, where and more about our data. Namely, with the collected data I want to answer the following questions:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;What was the frequency of tweets associated with the word ‚Äòelections‚Äô?&lt;/li&gt;
&lt;li&gt;Who tweeted the most?&lt;/li&gt;
&lt;li&gt;What was the most common #Hashtag tweeted?&lt;/li&gt;
&lt;li&gt;Which tweet was most liked and which was retweeted the most?&lt;/li&gt;
&lt;li&gt;What were the most common words and bigrams (two words) in tweets?&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;div id=&#34;gathering-the-data&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Gathering the data &lt;i class=&#34;fab fa-twitter&#34;&gt;&lt;/i&gt;&lt;/h3&gt;
&lt;p&gt;Twitter‚Äôs API allows scraping &lt;strong&gt;6-9 days back for free&lt;/strong&gt;. Therefore, I scraped the data already on March 7, 2020 and saved it for later use.&lt;/p&gt;
&lt;p&gt;Let‚Äôs start with the packages we‚Äôll use:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(rtweet)
library(tidyverse)
library(tidytext)
library(igraph)
library(hrbrthemes)
library(ggraph)
library(extrafont)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;I could use a consistent plot theme throughout the post but I‚Äôll probably be editing each one a bit, while also some are not our regular graphs. With that said, There are some tweaks that will be consistent acorss several of the plots. Therefore, let‚Äôs create a theme function as a supplement to all other theme arguments I‚Äôll use that will save a few lines of code:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mini_theme &amp;lt;- function(family = &amp;quot;Roboto Condensed&amp;quot;, tsize = 16) {
  theme_classic() +
  theme(text = element_text(family = family),
        axis.ticks = element_blank(),
        axis.line = element_blank(),
        plot.title = element_text(size = tsize))}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Next we‚Äôll gather the tweets we need:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;elections_raw &amp;lt;- search_tweets(&amp;quot;◊ë◊ó◊ô◊®◊ï◊™&amp;quot;, n = 18000, retryonratelimit = TRUE)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To gather the tweets we can use the &lt;a href=&#34;https://rtweet.info/&#34;&gt;{rtweet}&lt;/a&gt; package which is amazing for collecting Twitter data. As I mentioned earlier, I already scraped the data a few days after the elections but left the command here to show what we did and how easy it is to do it. I searched only one term, ‚Äòelections‚Äô in Hebrew, and rtweet gathered all tweets containing that word.&lt;/p&gt;
&lt;p&gt;What did our search yield? Let‚Äôs have a look:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;dim(elections)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 16560    90&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;16,560 rows and 90 columns! As we can see, the &lt;code&gt;{rtweet}&lt;/code&gt; package brings back a lot of information!&lt;/p&gt;
&lt;div id=&#34;some-caveats&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Some Caveats:&lt;/h4&gt;
&lt;p&gt;Before we begin, I will say this post doesn‚Äôt aim to be representative of the discussions that were held during the election period. As a matter of fact, nor does it aim to be representative of the twitter discussions surrounding the elections. this is due to two main reasons:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;Twitter isn‚Äôt common in Israel at all. I‚Äôm not sure what‚Äôs the usage rate but it‚Äôs definitely not representative of the Israeli population.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;I searched for only one word - elections (in Hebrew) - which yielded some 16560 tweets. This is definitely not a large enough pool of tweets to claim for representation.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;With that said, the data gathered provides an opportunity to look at some Twitter data from the election period and motivate others to use the &lt;code&gt;{rtweet}&lt;/code&gt; package, so why not give it a go.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;tweet-frequency&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Tweet frequency&lt;/h3&gt;
&lt;p&gt;First, let‚Äôs see how the tweets distribute across the time span we searched for. we can create a quick time plot using the &lt;code&gt;ts_plot()&lt;/code&gt; argument from the &lt;code&gt;{rtweet}&lt;/code&gt; package:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;elections %&amp;gt;% 
  ts_plot(&amp;quot;2 hours&amp;quot;)+
  geom_line(size = 1, color = &amp;quot;black&amp;quot;)+
  mini_theme()+
  scale_x_datetime(date_breaks = &amp;quot;1 day&amp;quot;,date_labels = &amp;quot;%d %b&amp;quot;)+
  labs(x= NULL, y = NULL,
       title = &amp;quot;Tweet frequency throughout the Israeli elections week&amp;quot;,
       subtitle = &amp;quot;Tweets aggregated by two-hour interval. Only tweets containing the word &amp;#39;elections&amp;#39;\nin Hebrew were gathered&amp;quot;)+
  geom_text(aes(x = as.POSIXct(&amp;quot;2020-03-02 23:00:00&amp;quot;), y = 435, label = &amp;quot;10 PM:\nPolls close&amp;quot;),
            hjust = 0, size = 3, family = &amp;quot;Roboto Condensed&amp;quot;)+
  geom_vline(xintercept = as.POSIXct(&amp;quot;2020-03-02 22:00&amp;quot;),linetype = &amp;quot;dashed&amp;quot;, size = 0.5, color = &amp;quot;black&amp;quot;, alpha = 5/10)+
  theme(plot.subtitle = element_text(color = &amp;quot;gray70&amp;quot;))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/elections-twitter/index_files/figure-html/unnamed-chunk-6-1.png&#34; width=&#34;672&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Interesting - we see the number of tweets during the closing time is equivalent to that of midday on March 4th. Most of the votes were counted by the end of March 3rd, so I can‚Äôt really put my finger on what this jump represents. After all, I collected tweets containing our word so it could have been that many people tweeted that specific term in that time slot. Anyway, I wasn‚Äôt able to find anything interesting that happened on the news that day but feel free to explore and offer suggestions.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;users-with-most-tweets&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Users with most tweets&lt;/h3&gt;
&lt;p&gt;Next, let‚Äôs look at who tweeted the most:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;elections %&amp;gt;% 
  count(screen_name, sort = T) %&amp;gt;% 
  slice(1:15) %&amp;gt;% 
  mutate(screen_name = reorder(screen_name,n)) %&amp;gt;% 
  ggplot(aes(x= screen_name, y= n))+
  geom_col(fill = &amp;quot;gray70&amp;quot;)+
  coord_flip()+
  scale_y_continuous(breaks = seq(0,180, 30), labels = seq(0,180,30))+
  labs(x = &amp;quot;Screen name&amp;quot;, y = &amp;quot;Number of tweets&amp;quot;, title = &amp;quot;Top 15 users tweeting the word &amp;#39;elections&amp;#39;&amp;quot;)+
  mini_theme()+
  theme(text = element_text(family = &amp;quot;Calibri&amp;quot;),
        axis.text = element_text(size = 12),
        axis.title.y = element_blank())&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/elections-twitter/index_files/figure-html/unnamed-chunk-7-1.png&#34; width=&#34;768&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;We see that many news companies tweeted a lot using the word ‚Äòelections‚Äô: ‚Äònewisrael13‚Äô, ‚Äòkann_news‚Äô, ‚ÄòMaarivOnline‚Äô, ‚ÄòRotterNews‚Äô, ‚Äòbahazit_news‚Äô, ‚ÄòRotterNet‚Äô. I personnaly don‚Äôt recognize the rest, but on the other hand I use Twitter mostly to follow &lt;code&gt;R&lt;/code&gt; and academic related tweets, not necessarily Israeli politics.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;common-hashtags&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Common Hashtags&lt;/h3&gt;
&lt;p&gt;When using the &lt;code&gt;{rtweet}&lt;/code&gt; package to gather twitter data, one of the variables collected is the hashtags used in tweets. Although it doesn‚Äôt require too many lines of code to extract hashtags out of text, I think this is an amazing feature that shows the effort and details &lt;a href=&#34;https://mikewk.com/&#34;&gt;Michael W. Kearney&lt;/a&gt; and contributors put into the package.&lt;/p&gt;
&lt;p&gt;According to &lt;a href=&#34;https://en.wikipedia.org/wiki/Hashtag&#34;&gt;Wikipedia&lt;/a&gt;, a ‚ÄòHashtag‚Äô ‚Äúis a type of metadata tag used on social networks such as Twitter and other microblogging services.‚Äù, that basically tags the message with a specific theme. This helps to see trends and themes in a macro level.&lt;/p&gt;
&lt;p&gt;OK then, let‚Äôs see what we have:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;hashtags &amp;lt;- elections %&amp;gt;% 
  select(hashtags) %&amp;gt;% 
  unlist() %&amp;gt;% 
  as.tibble() %&amp;gt;% 
  mutate(value = tolower(value)) %&amp;gt;% 
  count(value, name = &amp;quot;Count&amp;quot;, sort = T) %&amp;gt;%
  mutate(value = reorder(value, Count),
         iscorona = ifelse(value == &amp;quot;◊ß◊ï◊®◊ï◊†◊î&amp;quot; | value == &amp;quot;coronavirus&amp;quot;, &amp;quot;y&amp;quot;, &amp;quot;n&amp;quot;)) %&amp;gt;% 
  filter(!is.na(value)) %&amp;gt;% 
  slice(1:20)

ggplot(data = hashtags, aes(x = Count, y = value, fill = iscorona))+
  geom_col(show.legend = FALSE)+
  scale_fill_manual(values = c(y = &amp;quot;#1DA1F2&amp;quot;, n = &amp;quot;gray70&amp;quot;))+
  labs(y = NULL, x = &amp;quot;Number of Tweets&amp;quot;, title = &amp;quot;Top 20 Hashtags addressing the Israeli elections&amp;quot;)+
  mini_theme()+
  theme(text = element_text(family = &amp;quot;Calibri&amp;quot;),
        axis.text = element_text(size = 12))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/elections-twitter/index_files/figure-html/unnamed-chunk-9-1.png&#34; width=&#34;672&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The tweets include pretty much what we expect - hashtags about the elections - with the two leading ones being ‚Äòelections‚Äô and ‚Äòelections2020‚Äô. We also see a peculiar hashtag ‚Äòright_following_right_people‚Äô, and others such as ‚ÄòNetanyahu‚Äô (the Prime minister at the time), ‚ÄòIsrael‚Äô and others.&lt;br /&gt;
I highlighted in blue an interesting hashtag at the time - &lt;font color=&#34;#1DA1F2&#34;&gt;&lt;strong&gt;Corona&lt;/strong&gt;&lt;/font&gt; (in hebrew) and &lt;font color=&#34;#1DA1F2&#34;&gt;&lt;strong&gt;coronavirus&lt;/strong&gt;&lt;/font&gt;. The elections were held on March 2, 2020, a little bit after the first cases reached Israel. Little did we know how it will affect us (I‚Äôm finalzing this post on April 18, 2020, and only now we‚Äôre starting to get back to routine. Slowly)&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;most-liked-and-retweeted&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Most liked and retweeted&lt;/h3&gt;
&lt;p&gt;Let‚Äôs have a look at which tweet was &lt;strong&gt;most liked&lt;/strong&gt;. Twitter doesn‚Äôt define it as ‚Äòlikes‚Äô but as ‚Äòfavorite‚Äô, or at least in the data that is collected through the &lt;code&gt;{rtweet}&lt;/code&gt; package. Since I will want to gather the most of something - both favorite and later retweeted - I‚Äôll create a function that will minimize re-writing the code.&lt;br /&gt;
&lt;br&gt;
The function takes in a variable, reorders our dataset according to the variable we declared, extracts the first row and then pulls (extracts) the status id of that tweet. Lastly, the &lt;code&gt;blogdown::shortcode&lt;/code&gt; enables to embed tweets, youtube videos and more into a blogdown post such as this, so we end the function by inserting our status id into that. For those just getting into functions notice that within the &lt;code&gt;arrange&lt;/code&gt; argument we insert our variable in two curly brackets {{}}. This is a powerful feature of &lt;code&gt;{rlang}&lt;/code&gt; when you want to manipulate a variable in a dataframe within a function. You can read more about that &lt;a href=&#34;https://www.tidyverse.org/blog/2019/06/rlang-0-4-0/&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;get_most &amp;lt;- function(var){
elections %&amp;gt;% 
  arrange(desc({{var}})) %&amp;gt;% 
  .[1,] %&amp;gt;% 
  pull(status_id) %&amp;gt;% 
  blogdown::shortcode(&amp;#39;tweet&amp;#39;,.)
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now Let‚Äôs see which tweet was &lt;strong&gt;most liked&lt;/strong&gt; during that week:&lt;/p&gt;
&lt;center&gt;
&lt;blockquote class=&#34;twitter-tweet&#34;&gt;&lt;p lang=&#34;iw&#34; dir=&#34;rtl&#34;&gt;◊ô◊ï◊™◊® ◊û◊î◊õ◊ú, ◊ê◊†◊ô ◊©◊û◊ó ◊©◊ú◊ê ◊ô◊î◊ô◊ï ◊¢◊ï◊ì ◊ë◊ó◊ô◊®◊ï◊™ ◊ë◊©◊ë◊ô◊ú ◊î◊û◊©◊§◊ó◊î ◊©◊ú◊ô ◊©◊°◊ë◊ú◊î ◊ë◊í◊ë◊ï◊®◊î ◊©◊†◊î ◊ï◊®◊ë◊¢. ◊®◊¢◊ï◊™ ◊¢◊ë◊®◊ô ◊ï◊¢◊†◊® üòç&lt;/p&gt;&amp;mdash; ◊¢◊û◊ô◊™ ◊°◊í◊ú Amit Segal (@amit_segal) &lt;a href=&#34;https://twitter.com/amit_segal/status/1234584864415997952?ref_src=twsrc%5Etfw&#34;&gt;March 2, 2020&lt;/a&gt;&lt;/blockquote&gt;
&lt;script async src=&#34;https://platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;

&lt;/center&gt;
&lt;p&gt;The tweet is by ‚ÄòAmit Segal‚Äô - an Israeli news reporter - and it says (my translation):&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;‚ÄúMore than anything, I‚Äôm glad there won‚Äôt be another elections for my family that suffered in honors a year and a quarter. Reut, Ivri and Aner üòç‚Äù&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Ha, interestingly he wrote it before the end of the elections, hopefully he‚Äôs right!&lt;/p&gt;
&lt;p&gt;Now let‚Äôs look at the &lt;strong&gt;most re-tweeted&lt;/strong&gt; tweet:&lt;/p&gt;
&lt;center&gt;
&lt;blockquote class=&#34;twitter-tweet&#34;&gt;&lt;p lang=&#34;iw&#34; dir=&#34;rtl&#34;&gt;◊ê◊ù ◊î◊î◊ß◊ú◊ò◊î ◊©◊ú ◊ô◊ï◊¢◊¶◊ï ◊©◊ú ◊í◊†◊• ◊û◊ë◊ï◊©◊ú◊™ ◊ï◊©◊ß◊®◊ô◊™ (◊õ◊ì◊ë◊®◊ô ◊í◊†◊• ◊¢◊õ◊©◊ô◊ï), ◊ê◊ñ ◊ú◊û◊î ◊í◊†◊• ◊§◊ô◊ò◊® ◊ê◊ï◊™◊ï?&lt;br&gt;&lt;br&gt;◊ô◊ï◊¢◊¶◊ï ◊©◊ú ◊í◊†◊• ◊§◊ï◊ò◊® ◊ë◊í◊ú◊ú ◊©◊ê◊û◊® ◊ê◊™ ◊î◊ê◊û◊™ ◊©◊õ◊ï◊ú◊ù ◊ô◊ï◊ì◊¢◊ô◊ù: ◊í◊†◊• ◊ú◊ê ◊ô◊õ◊ï◊ú ◊ú◊î◊ô◊ï◊™ ◊®◊ê◊© ◊û◊û◊©◊ú◊î. ◊ê◊†◊ó◊†◊ï ◊õ◊ü. ◊¢◊ï◊ì 2 ◊û◊†◊ì◊ò◊ô◊ù ◊ú◊ú◊ô◊õ◊ï◊ì ◊ï◊ê◊†◊ó◊†◊ï ◊û◊ï◊¶◊ô◊ê◊ô◊ù ◊ê◊™ ◊î◊û◊ì◊ô◊†◊î ◊û◊î◊§◊ú◊ï◊†◊ò◊®, ◊û◊ï◊†◊¢◊ô◊ù ◊¢◊ï◊ì ◊ë◊ó◊ô◊®◊ï◊™ ◊ï◊û◊ß◊ô◊û◊ô◊ù ◊û◊û◊©◊ú◊î&lt;/p&gt;&amp;mdash; Benjamin Netanyahu (@netanyahu) &lt;a href=&#34;https://twitter.com/netanyahu/status/1233342393740603394?ref_src=twsrc%5Etfw&#34;&gt;February 28, 2020&lt;/a&gt;&lt;/blockquote&gt;
&lt;script async src=&#34;https://platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;

&lt;/center&gt;
&lt;p&gt;The tweet is by Benjamin Netanyahu, at the time the prime minister of Israel, who writes:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;‚ÄúIf the recording of Gantz‚Äôs advisor is orcherstrated and fabricated (according to Gantz‚Äôs words just now), why did Gantz fire him? Gantz‚Äôs advisor was fired because he said the truth everyone knows: Gantz can‚Äôt be a prime minister. We can. 2 more mandates to the Likkud and we are taking the country out of the plonter, preventing another election and form a government‚Äù&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;This came after the exposure of a secret recording of Gantz in a closed meeting, A few days before election day.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;wordcloud-and-bigrams&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Wordcloud and bigrams&lt;/h2&gt;
&lt;p&gt;Let‚Äôs have a look at two more text-related analyses:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;A word-cloud&lt;br /&gt;
&lt;/li&gt;
&lt;li&gt;Bigrams (two-words) from our text&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;We could try out more algorthims but I‚Äôll save them for a different post (feel free to try on your own).&lt;/p&gt;
&lt;div id=&#34;wordcloud&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Wordcloud&lt;/h3&gt;
&lt;p&gt;In order to tackle the wordcloud, I‚Äôll break up all the tweets into &lt;strong&gt;single words&lt;/strong&gt;, filter any Hebrew stop words (file found online) and all English words. The decision to filter English words is mainly because I‚Äôm interested in the Hebrew sentences, but also because most the common English words used in our data are those of Twitter user names cited when replying to a tweet:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;he_stopwords &amp;lt;- read_tsv(&amp;quot;https://raw.githubusercontent.com/gidim/HebrewStopWords/master/heb_stopwords.txt&amp;quot;, col_names = &amp;quot;word&amp;quot;)

election_token &amp;lt;- elections %&amp;gt;% 
  unnest_tokens(word, text) %&amp;gt;% 
  select(word) %&amp;gt;%
  anti_join(he_stopwords) %&amp;gt;% 
  count(word, sort = T) %&amp;gt;%
  filter(!grepl(&amp;quot;([a-z]+|◊ë◊ó◊ô◊®◊ï◊™)&amp;quot;, word), n&amp;gt;= 150)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now we can create a wordcloud of words appearing more than 150 times using &lt;code&gt;{wordcloud2}&lt;/code&gt; package&lt;a href=&#34;#fn1&#34; class=&#34;footnote-ref&#34; id=&#34;fnref1&#34;&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;wordcloud2::wordcloud2(election_token, color = &amp;quot;#1DA1F2&amp;quot;, shape = &amp;quot;circle&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;div class=&#34;figure&#34; style=&#34;text-align: center&#34;&gt;&lt;span id=&#34;fig:unnamed-chunk-13&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;wc.png&#34; alt=&#34;Wordcloud excludes Hebrew stop words and the word &#39;elections&#39;&#34; width=&#34;550&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 1: Wordcloud excludes Hebrew stop words and the word ‚Äòelections‚Äô
&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;What we can see is many of the words we‚Äôd expect: Political candidates, government, fourth (in the context of fourth elections), partis‚Äô names and more. I‚Äôll provide a more thorough discussion following our bigram plot below, as I believe it addresses many of the same words.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;common-bigrams&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Common Bigrams&lt;/h3&gt;
&lt;p&gt;Like we did before, we can break up our text data into &lt;strong&gt;two word&lt;/strong&gt; observations, also known as bigrams. In order to account for all combinations, we break up the sentence to fit all possible options. For example, assume we have the following sentence:&lt;/p&gt;
&lt;p&gt;‚ÄúDanny went to vote yesterday‚Äù&lt;/p&gt;
&lt;p&gt;Using the &lt;code&gt;unnest_tokens&lt;/code&gt; we‚Äôll break the sentence into the following bigrams:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Danny went&lt;br /&gt;
&lt;/li&gt;
&lt;li&gt;went to&lt;br /&gt;
&lt;/li&gt;
&lt;li&gt;to vote&lt;br /&gt;
&lt;/li&gt;
&lt;li&gt;vote yesterday&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Which gives us all possible options. We will also include two columns consisting of the bigram broken up into single words. This will help in filtering out bigrams containing Hebrew stop words or English words. I‚Äôll not run through the following code but instead will point you to &lt;a href=&#34;http://varianceexplained.org/&#34;&gt;David Ronbinson&lt;/a&gt; &amp;amp; &lt;a href=&#34;https://juliasilge.com/&#34;&gt;Julia Silge&lt;/a&gt; &lt;a href=&#34;https://www.tidytextmining.com/&#34;&gt;‚ÄòText Mining with R‚Äô Book&lt;/a&gt; for further reading.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;elec_bigram &amp;lt;- elections %&amp;gt;%
  select(text) %&amp;gt;% 
  unnest_tokens(bigram, text, token = &amp;quot;ngrams&amp;quot;, n = 2) %&amp;gt;%
  separate(bigram, into = c(&amp;quot;word1&amp;quot;, &amp;quot;word2&amp;quot;), sep = &amp;quot; &amp;quot;, remove = FALSE) %&amp;gt;% 
  filter(!word1 %in% he_stopwords$word,
         !word2 %in% he_stopwords$word,
         !grepl(&amp;quot;([a-z]+|◊ë◊ó◊ô◊®◊ï◊™)&amp;quot;, bigram)) %&amp;gt;% 
  count(word1, word2, sort = T) %&amp;gt;% 
  slice(1:45) %&amp;gt;%
  graph_from_data_frame()

p_arrow &amp;lt;- arrow(type = &amp;quot;closed&amp;quot;, length = unit(.1, &amp;quot;inches&amp;quot;))

ggraph(elec_bigram, layout = &amp;quot;fr&amp;quot;)+
  geom_edge_link(aes(edge_alpha = n), arrow = p_arrow, end_cap = circle(.04, &amp;quot;inches&amp;quot;), show.legend = FALSE)+
  geom_node_point(color = &amp;quot;lightblue&amp;quot;, size = 3)+
  geom_node_text(aes(label = name), vjust = 1, hjust = 1, family = &amp;quot;Calibri&amp;quot;)+
  theme_void()+
  labs(title = &amp;quot;Twitter text bigram&amp;quot;)+
  theme(text = element_text(family = &amp;quot;Calibri&amp;quot;),
        plot.title = element_text(hjust = 0.5 , face = &amp;quot;bold&amp;quot;, size = 18))&lt;/code&gt;&lt;/pre&gt;
&lt;div class=&#34;figure&#34; style=&#34;text-align: center&#34;&gt;&lt;span id=&#34;fig:unnamed-chunk-15&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;/post/elections-twitter/index_files/figure-html/unnamed-chunk-15-1.png&#34; alt=&#34;Word bigram excludes Hebrew stop words and the word &#39;elections&#39;&#34; width=&#34;672&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 2: Word bigram excludes Hebrew stop words and the word ‚Äòelections‚Äô
&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;How should we read this graph?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;First off, We only plotted the 45 most common bigrams (out of 100,000+). Every word is connected to another word with an arrow pointing to a given direction. The direction to which the arrow points is the way to read that bigram. In addition, bolder lines represent a higher frequency of that bigram throughout all our text.&lt;br /&gt;
For example, on the bottom of our graph we see the number ‚Äò2‚Äô connected to the words ‚Äòmandates‚Äô and ‚Äòcampagin‚Äô. The direction of the arrow signals that we should read the bigram as ‚Äò2 mandates‚Äô and ‚Äò2 campagins‚Äô.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;What does this all mean?&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;We have discussions regarding the &lt;strong&gt;number of chairs a govenrment will have (62/61/60/58)&lt;/strong&gt; connected to mentions of the number of election campaigns (2/3) we had, discussions of a united and/or minimal government and the forming of one in general.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;We see &lt;strong&gt;mentions of individuals&lt;/strong&gt; such as ‚ÄúBenjamin Netanyahu‚Äù, ‚ÄúAmit Segal‚Äù (Both we discussed earlier), ‚ÄúNatan Eshel‚Äù, &lt;strong&gt;but no mention of the main candidate running against Netanyahu - ‚ÄúBenny Gantz‚Äù&lt;/strong&gt;. That‚Äôs actually kind of odd, but more on that in a minute.&lt;br /&gt;
&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;We also see mentions of political parties such as ‚ÄúMeretz‚Äù, ‚ÄúGesher‚Äù and ‚ÄúLabor‚Äù who ran together this time around, ‚ÄúOtzma Yehudit‚Äù, ‚ÄúUnited Torah Judaism‚Äù, and the ‚ÄúJoint List‚Äù. &lt;strong&gt;There‚Äôs no mention of the two leading parties - ‚ÄúKahol Lavan‚Äù &amp;amp; ‚ÄúThe Likkud‚Äù.&lt;/strong&gt;, despite the mentioning of the latter‚Äôs leader.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Mentions of Netanyahu‚Äôs indicment and the personal law associated him.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Mentions I‚Äôd categorize as ‚Äòother‚Äô such as ‚ÄúTerrorist supporters‚Äù, ‚ÄúWill of the people‚Äù, ‚ÄúFake news‚Äù, &#34;Go vote‚Äô, etc.
&lt;br&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Actaully, this turned out more interesting than I thought. Several questions arose while looking at it: Several words are missing such as the main parties names (Likkud &amp;amp; Kahol-Lavan), The leading oponent running against Benjamin Netanyahu - Benny Gantz - and other questions such as with whom are specific terms associated. Before we close up I‚Äôll look at one question that troubles me - &lt;strong&gt;Why doesn‚Äôt Gantz appear in our list&lt;/strong&gt; üò±?&lt;/p&gt;
&lt;div id=&#34;benny-gantzs-disappearance&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Benny Gantz‚Äôs disappearance&lt;/h4&gt;
&lt;p&gt;In order to see why Benny Gantz doesn‚Äôt appear in our bigram plot I‚Äôll do the following: I‚Äôll break the text into bigrams and filter to &lt;strong&gt;have only the bigrams containing the word Gantz&lt;/strong&gt;. Once we have that we can see why he doesn‚Äôt appear in our bigram plot despite appearing in our wordcloud.&lt;br /&gt;
Before I run the analysis and give you the answer think for a moment - What was the process of coming up with the bigram? If I chose only the 50 most frequent bigrams, why would a word that appears many times in our text not appear in our bigram list? Alternatively, did we filter anything along the way? Maybe even give the previous chunk another glance before I answer it.&lt;br /&gt;
&lt;br&gt;
Let‚Äôs have a look:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;gantz &amp;lt;-elections %&amp;gt;%
  select(text) %&amp;gt;% 
  unnest_tokens(bigram, text, token = &amp;quot;ngrams&amp;quot;, n = 2) %&amp;gt;%
  separate(bigram, into = c(&amp;quot;word1&amp;quot;, &amp;quot;word2&amp;quot;), sep = &amp;quot; &amp;quot;, remove = FALSE) %&amp;gt;% 
  filter(word1 %in% &amp;quot;◊í◊†◊•&amp;quot; |
         word2 %in% &amp;quot;◊í◊†◊•&amp;quot;,
         !grepl(&amp;quot;([a-z]+|◊ë◊ó◊ô◊®◊ï◊™)&amp;quot;, bigram))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The code is similar to what we did earlier only this time we left &lt;strong&gt;bigrams that match the word we want&lt;/strong&gt; - bigrams containing the word Gantz. Now that we have our list of bigrams, let‚Äôs look at the count of bigrams containing the word ◊í◊†◊• (‚ÄòGantz‚Äô):&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;gantz %&amp;gt;% 
  count(bigram, sort = T)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 978 x 2
##    bigram         n
##    &amp;lt;chr&amp;gt;      &amp;lt;int&amp;gt;
##  1 ◊©◊ú ◊í◊†◊•       160
##  2 ◊ë◊†◊ô ◊í◊†◊•      138
##  3 ◊í◊†◊• ◊ú◊ê        90
##  4 ◊¢◊ú ◊í◊†◊•        70
##  5 ◊ê◊™ ◊í◊†◊•        69
##  6 ◊¢◊ù ◊í◊†◊•        61
##  7 ◊ê◊ù ◊í◊†◊•        41
##  8 ◊í◊†◊• ◊î◊ô◊î       25
##  9 ◊í◊†◊• ◊ê◊ï        19
## 10 ◊í◊†◊• ◊ú◊ô◊ë◊®◊û◊ü    19
## # ... with 968 more rows&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;AHA!&lt;/strong&gt; Now I see what happened. The first bigram is a stop word and the word Gantz (‚ÄòOf Gantz‚Äô). The second bigram should have been included as it is Gantz‚Äôs full name - Benny Gantz, which appears 138 times.&lt;br /&gt;
So, why has it been filtered? This is a great question which we can answer if we look at our stop words we initially used. Let‚Äôs see if it has the word ◊ë◊†◊ô (‚Äòbenny‚Äô in Hebrew):&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;he_stopwords %&amp;gt;% 
  filter(word == &amp;quot;◊ë◊†◊ô&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 1 x 1
##   word 
##   &amp;lt;chr&amp;gt;
## 1 ◊ë◊†◊ô&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Yes it does. At the time of writing this blog post it leaves me in a dilemma - Should I change the stop words file I used to a different one or maybe create my own? Or should I continue as is? I think leaving it will teach me (and hopefully whoever read this far) a valuable lesson of always checking your stop words. In a different context the specific bigram wouldn‚Äôt have got me thinking, but here it didn‚Äôt make sense that our leading candidate was filtered, thus my inquire into what happened. In hebrew the word Benny also means ‚Äòmy son‚Äô, which I wouldn‚Äôt describe as a stop word but whoever made the dataset I guess did.&lt;/p&gt;
&lt;p&gt;If you wish to give it a try yourself, you can find the data in the form of an &lt;code&gt;.rds&lt;/code&gt; or smaller &lt;code&gt;.csv&lt;/code&gt; (excludes list columns) in my &lt;a href=&#34;https://github.com/AmitLevinson/amitlevinson.com/blob/master/content/post/elections-twitter&#34;&gt;github repository&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Well then, that‚Äôs all for now folks! &lt;strong&gt;And remember, make sure to validate your stop words dataset!&lt;/strong&gt;
&lt;br&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;footnotes&#34;&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li id=&#34;fn1&#34;&gt;&lt;p&gt;The function &lt;code&gt;wordcloud2&lt;/code&gt; we wrote wasn‚Äôt actually run because it renders an html object which distorts the post. Instead I used the webshot of our rendered html file, read more about that &lt;a href=&#34;https://www.r-graph-gallery.com/196-the-wordcloud2-library.html&#34;&gt;here&lt;/a&gt;.&lt;a href=&#34;#fnref1&#34; class=&#34;footnote-back&#34;&gt;‚Ü©&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
      
            <category>rtweet</category>
      
            <category>tidytext</category>
      
      
            <category>R</category>
      
    </item>
    
    <item>
      <title>Making updating exam grades easy with R</title>
      <link>/post/update-exam-grades-easy-with-r/</link>
      <pubDate>Sat, 15 Feb 2020 00:00:00 +0000</pubDate>
      
      <guid>/post/update-exam-grades-easy-with-r/</guid>
      <description>
&lt;script src=&#34;/rmarkdown-libs/kePrint/kePrint.js&#34;&gt;&lt;/script&gt;


&lt;p&gt;&lt;strong&gt;TL;DR&lt;/strong&gt; Instead of each time searching for an id in the xlsx template the university provides we make our own xlsx and merge between the two. I then run through two options of either saving the new data frame as an &lt;code&gt;.xlsx&lt;/code&gt; using the &lt;code&gt;{xlsx}&lt;/code&gt; package, and I show another option where I extract the new column I need using &lt;code&gt;write_clip&lt;/code&gt; from the &lt;code&gt;{clipr}&lt;/code&gt; package.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;‚ÄúProgress isn‚Äôt made by early risers. It‚Äôs made by lazy men trying to find easier ways to do something.‚Äù &lt;/br&gt; ‚Äï Robert Heinlein&lt;/p&gt;
&lt;/blockquote&gt;
&lt;div id=&#34;whats-the-story&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;What‚Äôs the story?&lt;/h3&gt;
&lt;p&gt;The other day I had to update students‚Äô exams into a blank excel file. Every course exam each student gets an exam id. Their id is comprised from a number / number, for example, 26/1; 1/1; 42/15 and so forth. In our course of up to 70 students the left number goes all the way to the number of students in the exam class, and the right number goes up to 15 or 20 and starts again from 1.&lt;br /&gt;
This would make it easy to insert the grade for each id into the excel file that is already organized. However, since this is a new system and I was waiting to get access to download the excel I decided to open a new spreadsheet instead. Also, writing the id instead of looking it up in the excel file each time can save, in my opinion, a little time of searching.&lt;br /&gt;
So we have our spreadsheet which is not sorted, and we have the university‚Äôs spreadsheet which is sorted - how are we going to sync between them, considering our id column we wrote is recognized as a &lt;code&gt;character&lt;/code&gt; class? I know, let‚Äôs turn to &lt;code&gt;R&lt;/code&gt;&lt;a href=&#34;#fn1&#34; class=&#34;footnote-ref&#34; id=&#34;fnref1&#34;&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;looking-at-our-data&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Looking at our data&lt;/h3&gt;
&lt;p&gt;Let‚Äôs start off with loading our packages:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyverse)
# For reading xlsx files
library(readxl)
# To nicely display the tables in the following paragraph
library(kableExtra)
library(knitr)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now let‚Äôs read both files: Our spreadsheet with just the id and grade of each student we wrote in, and the other spreadsheet with the students‚Äô id and a numerical vector to sort by that the university provides.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;messy &amp;lt;- read_excel(&amp;quot;messy_grades.xlsx&amp;quot;)
clean &amp;lt;- read_excel(&amp;quot;clean.xlsx&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This gives us the following tables where on the left we have our &lt;strong&gt;messy&lt;/strong&gt; table we wrote and on the right our &lt;strong&gt;clean&lt;/strong&gt; table we want to merge to:&lt;/p&gt;
&lt;table class=&#34;table&#34; style=&#34;width: auto !important; float: left; margin-right: 10px;&#34;&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:center;&#34;&gt;
id
&lt;/th&gt;
&lt;th style=&#34;text-align:center;&#34;&gt;
grade
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
67/13
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
94
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
56/2
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
90
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
68/14
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
84
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
63/9
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
100
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
55/1
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
89
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
62/8
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
97
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;table class=&#34;table&#34; style=&#34;width: auto !important; margin-right: 0; margin-left: auto&#34;&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:center;&#34;&gt;
id
&lt;/th&gt;
&lt;th style=&#34;text-align:center;&#34;&gt;
participated
&lt;/th&gt;
&lt;th style=&#34;text-align:center;&#34;&gt;
number_for_sorting
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
1/1
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
V
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
1
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
2/2
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
V
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
2
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
3/3
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
V
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
3
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
4/4
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
NA
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
4
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
5/5
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
NA
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
5
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
6/6
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
V
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
6
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;&lt;br&gt;
So we now have several options:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Join between the two tables, save the clean table as a new xlsx and upload it to the University‚Äôs exam system.&lt;/li&gt;
&lt;li&gt;Join between the two tables, clip the column with the organized grades and paste it into the university‚Äôs sorted excel file.&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;div id=&#34;option-1---merge-and-write-to-a-new-excel-file&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Option 1 - Merge and write to a new excel file&lt;/h3&gt;
&lt;p&gt;So the first option will be to merge the two tables into the clean one and save that as a new excel file using the &lt;code&gt;{xlsx}&lt;/code&gt; package:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;joined_tables &amp;lt;- messy %&amp;gt;% 
    right_join(clean)

xlsx::write.xlsx(joined_tables, &amp;quot;010210078-29012020C.xlsx&amp;quot;, showNA = FALSE)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Below is a screen shot of our new table:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/img/ta_efficient/xl.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;However, going with this approach I encountered that the new .xlsx file is saved with a new column of id numbers that we see in the screenshot. We can just delete that column and have our file all ready to go.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;option-2---clip-the-sorted-column-into-the-excel-file&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Option 2 - Clip the sorted column into the excel file&lt;/h3&gt;
&lt;p&gt;This time around I‚Äôll write a function for what we‚Äôll be doing: I want to join the tables but this time around I want to clip the column I need and then manually paste it in the original template excel file:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;clip_grades &amp;lt;- function(messy, clean){
  messy %&amp;gt;% 
    right_join(clean) %&amp;gt;% 
    pull(grade) %&amp;gt;% 
    clipr::write_clip()
}

clip_grades(messy, clean)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;which gives us the following:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/img/ta_efficient/clipgif.gif&#34; /&gt;&lt;/p&gt;
&lt;p&gt;That‚Äôs it!&lt;/p&gt;
&lt;p&gt;Well, more or less. We need to delete the ‚ÄòNA‚Äô that are copied from the function. Unfortunately I wasn‚Äôt able to delete them from within &lt;code&gt;R&lt;/code&gt;, so I manually delete them.&lt;/p&gt;
&lt;p&gt;As to which option is better, I think the first option is more efficient as we only need to delete the id column. However, using the &lt;code&gt;{xlsx}&lt;/code&gt; package is dependent on &lt;code&gt;{rJava}&lt;/code&gt;and having java installed on the computer from what I encountered. Option two can be a little messy and possibly yield mistakes if we copy and paste the new grades and then manually delete the &lt;code&gt;NA&lt;/code&gt; - your call.&lt;/p&gt;
&lt;div id=&#34;so-what-did-i-learn-here&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;So what did I learn here?&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;How to read and write an excel file.&lt;/li&gt;
&lt;li&gt;Using the &lt;code&gt;write_clip&lt;/code&gt; function which is amazingly easy.&lt;/li&gt;
&lt;li&gt;How to make updating exams easier üí™&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;/br&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;footnotes&#34;&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li id=&#34;fn1&#34;&gt;&lt;p&gt;For confidentiality and other reasons I only left columns with information that can‚Äôt be linked to students (I also changed the grades altogether for this demonstration).&lt;a href=&#34;#fnref1&#34; class=&#34;footnote-back&#34;&gt;‚Ü©&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
      
            <category>xlsx</category>
      
            <category>clipr</category>
      
      
            <category>R</category>
      
    </item>
    
    <item>
      <title>Visualizing Eliud Kichoge&#39;s new marathon record</title>
      <link>/post/eliud-kichoge/</link>
      <pubDate>Fri, 07 Feb 2020 00:00:00 +0000</pubDate>
      
      <guid>/post/eliud-kichoge/</guid>
      <description>


&lt;div class=&#34;figure&#34; style=&#34;text-align: center&#34;&gt;&lt;span id=&#34;fig:unnamed-chunk-1&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;new_record.jpg&#34; alt=&#34;_Eliud Kipchoge breaks the two-hour marathon barrier. Photo from the &#39;New York Times&#39;. Leonhard Foeger/Reuters _&#34; width=&#34;512&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 1: &lt;em&gt;Eliud Kipchoge breaks the two-hour marathon barrier. Photo from the ‚ÄòNew York Times‚Äô. Leonhard Foeger/Reuters &lt;/em&gt;
&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;table-of-contents&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Table of Contents&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#background&#34;&gt;Background&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#retrieving-data-from-wikipedia&#34;&gt;Retrieving data from wikipedia&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#warngling-the-data&#34;&gt;Warngling the Data&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#plot&#34;&gt;Plot&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#ggimage&#34;&gt;ggimage&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#plot-aesthetics&#34;&gt;Plot Aesthetics&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#final-annotation&#34;&gt;Final annotation&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#appendix&#34;&gt;Appendix&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;background&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Background&lt;/h2&gt;
&lt;p&gt;On saturday October 12, 2019, Eliud Kipchoge broke (unofficially) the two-hour marathon barrier üèÜ&lt;br /&gt;
I saw &lt;a href=&#34;https://twitter.com/neilfws/status/1182958246753009665&#34;&gt;Neil Saunders‚Äô Twitter post&lt;/a&gt; visualizing the new record and wanted to try and reproduce it with runners instead of points. In this post I‚Äôll walk through how I obtained the data from a Wikipedia page with &lt;code&gt;{rvest}&lt;/code&gt;, wrangled and tidied it and eventually plotted it using &lt;code&gt;{ggimage}&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;When I initially created the plot I mistakenly took the &lt;a href=&#34;https://en.wikipedia.org/wiki/Marathon_year_rankings&#34;&gt;Marathon year rankings&lt;/a&gt; from the Wikipedia webpage. That page showcases The yearly rankings and not the world records in general. In addition, I also changed the method of obtaining the data from first creating the plot to now. When I first did it I copied and pasted the table from Wikipedia into a &lt;code&gt;.csv&lt;/code&gt; file and worked with that. For that specific time point, where my experience with &lt;code&gt;R&lt;/code&gt; was extremly novice, I think it was adequate. This time around I gave scraping Wikipedia‚Äôs webpage a try which also renders a reproducible example.&lt;/p&gt;
&lt;p&gt;Let‚Äôs start with loading the packages we‚Äôll need:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyverse)
library(rvest)
library(janitor)
library(lubridate)
library(ggimage)
library(hrbrthemes)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We‚Äôll use &lt;code&gt;{tidyverse}&lt;/code&gt; for tidy manipulation and plotting, &lt;code&gt;{janitor}&lt;/code&gt; for cleaning the column names, &lt;code&gt;{lubridate}&lt;/code&gt; for working with dates, &lt;code&gt;{ggimage}&lt;/code&gt; for a plot with images and &lt;code&gt;{hrbrthemes}&lt;/code&gt; for a nice quick aesthetic theme.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;retrieving-data-from-wikipedia&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Retrieving data from wikipedia&lt;/h2&gt;
&lt;p&gt;In order to view the new record in comparison to other world records, We‚Äôll turn to Wikipedia and see what we can find there.&lt;/p&gt;
&lt;div class=&#34;figure&#34; style=&#34;text-align: center&#34;&gt;&lt;span id=&#34;fig:wiki-ss&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;wikipage.png&#34; alt=&#34;_Wikipedia page of marathon world records_&#34; width=&#34;807&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 2: &lt;em&gt;Wikipedia page of marathon world records&lt;/em&gt;
&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;Here we can see that the webpage contains information about marathon records, where in the screenshot we see the men section. We only want the table with men‚Äôs records, so let‚Äôs get that:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#The Wikipage we&amp;#39;ll need
wiki_url &amp;lt;- &amp;quot;https://en.wikipedia.org/wiki/Marathon_world_record_progression&amp;quot;

runners_wiki &amp;lt;- wiki_url %&amp;gt;% 
  read_html() %&amp;gt;% 
  html_nodes(xpath=&amp;#39;//*[@id=&amp;quot;mw-content-text&amp;quot;]/div/table[1]&amp;#39;) %&amp;gt;% 
  html_table(fill = TRUE) %&amp;gt;% 
  as.data.frame()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Using the &lt;code&gt;{rvest}&lt;/code&gt; package we are able to scrape the Wikipedia page for the table we wanted. Frankly, this is the first time I used &lt;code&gt;rvest&lt;/code&gt;, but I found a good example from &lt;a href=&#34;https://r-tastic.co.uk/post/exploring-london-crime-with-r-heat-maps/&#34;&gt;Kasia Kulma‚Äôs blog post&lt;/a&gt; exploring London crime with R heat maps. I used the &lt;em&gt;SelctorGadget&lt;/em&gt; which identified the page‚Äôs content as ‚Äúmw-content-text‚Äù. Using that id we looked for the tables (/div/table), specifically the first table &lt;code&gt;[1]&lt;/code&gt; of men world records we saw earlier in figure &lt;a href=&#34;#fig:wiki-ss&#34;&gt;2&lt;/a&gt;. Once we have the table we turn it into a dataframe for us to use.&lt;/p&gt;
&lt;p&gt;Alternatively, you can also use the following method to extract a table by extracting all tables from the Wikipedia page and choosing the first one:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;runners_wiki_alternative &amp;lt;- wiki_url %&amp;gt;% 
  read_html() %&amp;gt;%
  html_table(fill = TRUE) %&amp;gt;%
  .[[1]]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This option extracts all table from the html page using &lt;code&gt;html_table()&lt;/code&gt;. Using this on the whole page parses the html tables into data frames nesting within a list object. Like before, &lt;code&gt;{rvest}&lt;/code&gt; makes it easy for us and if the tables have inconsistent number of values it requires (or demands?) us to fill them. Once we have the tables in a list object we can extract the one we need using &lt;code&gt;.[[1]]&lt;/code&gt;. The &lt;code&gt;.&lt;/code&gt; acts as a place holder for the previous object passed, here a list of tables we scraped. The &lt;code&gt;[[1]]&lt;/code&gt; following that calls for the first object within the list, but in the form of its core class - data.frame. If we‚Äôd use one square bracket &lt;code&gt;[1]&lt;/code&gt; it would return an object with the original class from which it was drawn, in this case a list which is not good for us here since we want it as a dataframe to continue our data preparation.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;warngling-the-data&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Warngling the Data&lt;/h2&gt;
&lt;p&gt;Let‚Äôs look at our table to see what we have and what we‚Äôll need to do:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;head(runners_wiki, n = 3)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##        Time          Name   Nationality              Date
## 1 2:55:18.4  Johnny Hayes United States     July 24, 1908
## 2 2:52:45.4 Robert Fowler United States   January 1, 1909
## 3 2:46:52.8   James Clark United States February 12, 1909
##                    Event.Place   Source
## 1       London, United Kingdom IAAF[53]
## 2  Yonkers,[nb 5]United States IAAF[53]
## 3 New York City, United States IAAF[53]
##                                                                                                                                                                                            Notes
## 1 Time was officially recorded as 2:55:18 2/5.[54]Italian Dorando Pietri finished in 2:54:46.4, but was disqualified for receiving assistance from race officials near the finish.[55] Note.[56]
## 2                                                                                                                                                                                      Note.[56]
## 3                                                                                                                                                                                      Note.[56]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;A little messey but that‚Äôs OK. What we‚Äôll need to visualize Eliud Kipchoge‚Äôs record is the Name, Time and Date of all runners. We‚Äôll start with cleaning our data:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;runners_clean &amp;lt;- runners_wiki %&amp;gt;% 
  clean_names() %&amp;gt;% 
  select(1,2,4)
str(runners_clean)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## &amp;#39;data.frame&amp;#39;:    50 obs. of  3 variables:
##  $ time: chr  &amp;quot;2:55:18.4&amp;quot; &amp;quot;2:52:45.4&amp;quot; &amp;quot;2:46:52.8&amp;quot; &amp;quot;2:46:04.6&amp;quot; ...
##  $ name: chr  &amp;quot;Johnny Hayes&amp;quot; &amp;quot;Robert Fowler&amp;quot; &amp;quot;James Clark&amp;quot; &amp;quot;Albert Raines&amp;quot; ...
##  $ date: chr  &amp;quot;July 24, 1908&amp;quot; &amp;quot;January 1, 1909&amp;quot; &amp;quot;February 12, 1909&amp;quot; &amp;quot;May 8, 1909&amp;quot; ...&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The &lt;code&gt;clean_names&lt;/code&gt; function cleans the column names making them easier to use. I then picked the columns we‚Äôll need using &lt;code&gt;select&lt;/code&gt;. Lastly, we want to look at the variables structure to see if we they need any manipulations. Yes, it seems both the time and date are not recognized appropriately (In this case they‚Äôre characters) - let‚Äôs fix that:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;runners_mutate &amp;lt;- runners_clean %&amp;gt;% 
  add_row(time = &amp;quot;1:59:40&amp;quot;, name = &amp;quot;Eliud Kipchoge&amp;quot;, date = &amp;quot;November 12, 2019&amp;quot;) %&amp;gt;% 
  mutate(run_period_raw = hms(time),
         run_duration = as.numeric(run_period_raw, &amp;quot;minutes&amp;quot;),
         run_year = year(mdy(date))) %&amp;gt;% 
  select(c(-date,-time))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning: 1 failed to parse.&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let‚Äôs see what we did here. First I add Eliud Kipchoge‚Äôs new unofficial record as an observation into our dataframe. I then turned to the &lt;code&gt;{lubridate}&lt;/code&gt; package where I used the &lt;code&gt;hms&lt;/code&gt; function to mutate the time variable we had into a new variable called ‚Äòr_period_raw‚Äô. Although this cleans the variable, &lt;code&gt;hms&lt;/code&gt; transforms it into a &lt;code&gt;period&lt;/code&gt; object which I found a little difficult to use when we want to plot. What we need is to turn it into a numeric class which we did in our new variable ‚Äòrun_duration‚Äô. This will help us in plotting but I retained the period class variable as it makes it easier to read in this case.&lt;/p&gt;
&lt;p&gt;I then turned the date column into a Month-Day-Year variable using the &lt;code&gt;mdy&lt;/code&gt; function, which eventually I only extracted the year using &lt;code&gt;year&lt;/code&gt;. Lastly I discarded the old columns we don‚Äôt need anymore. We also recieved a warning sign that one observation didn‚Äôt parse. This was because the value in the cell didn‚Äôt match the pattern of the &lt;code&gt;hms&lt;/code&gt; fuction. The original pattern looked like this: May 26, 1909[nb 6]. All we want is the specific year which we‚Äôll probably anyway filter later so it‚Äôs no big deal, but let‚Äôs go ahead and manually add it if we decide to use it later:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;runners_mutate[5,4] &amp;lt;- 1909&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This brings us the following:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(runners_mutate,aes(x = run_year, y = run_duration))+
  geom_point()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/eliud-kichoge/index_files/figure-html/unnamed-chunk-9-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Great, that‚Äôs a good start. Now we want to make it a little less crowded so we can easily insert an image of runners instead of points and not have it cluttered. In order to do that we‚Äôll look at each several years and lastly at 2019, the current record. First, let‚Äôs look at the years we have:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;runners_mutate %&amp;gt;% 
  pull(run_year)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  [1] 1908 1909 1909 1909 1909 1909 1913 1913 1914 1920 1925 1929 1935 1935 1935
## [16] 1935 1947 1952 1953 1953 1954 1956 1958 1960 1963 1963 1963 1964 1964 1965
## [31] 1967 1969 1970 1974 1978 1980 1981 1984 1985 1988 1998 1999 2002 2003 2007
## [46] 2008 2011 2013 2014 2018 2019&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Using the &lt;code&gt;pull&lt;/code&gt; function we were able to extract the column we wanted, much similar to using the &lt;code&gt;runners_mutate$column_name&lt;/code&gt; approach. ‚ÄòUnfortunately‚Äô, we can‚Äôt filter exactly by round intervals (for example every exact 10 years) so we‚Äôll create a vector with specific years to filter by. Although it might sound trivial, make sure you‚Äôre assigning years that are observed in your data set, otherwise it‚Äôll filter only by the years you do have and not those you don‚Äôt.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;year_sub &amp;lt;- c(1908, 1920, 1929, 1947, seq(1960,1980,10), 1999, 2011, 2019)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here we created a vector with values for every 15+- years. Now we can filter our new dataframe according to the years we want:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;runners_mutate &amp;lt;- runners_mutate %&amp;gt;% 
 filter(run_year %in% year_sub)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Using the &lt;code&gt;filter&lt;/code&gt; function with &lt;code&gt;%in%&lt;/code&gt; we discard anything from the &lt;code&gt;run_year&lt;/code&gt; column that‚Äôs not in the &lt;code&gt;year_sub&lt;/code&gt; vector. I find &lt;code&gt;%in%&lt;/code&gt; facsinating and extremely helpful when you want to look/filter several parameters. Basically, you can read it as ‚ÄúKeep all rows in ‚Äòrun_year‚Äô that match values in ‚Äòyear_sub‚Äô‚Äù.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;plot&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Plot&lt;/h2&gt;
&lt;div id=&#34;ggimage&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;ggimage&lt;/h3&gt;
&lt;p&gt;In order for us to plot a runner icon instead of points we need to load the images into our data frame as values for each observation. To do that we use the &lt;code&gt;{ggimage}&lt;/code&gt; package which we‚Äôll also use for the plot.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;runners_mutate &amp;lt;- runners_mutate %&amp;gt;% 
  mutate(run_image = &amp;quot;run.png&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;And now let‚Äôs look at our new plot:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;g &amp;lt;- ggplot(runners_mutate, aes(x = run_year, y = run_duration))+
  geom_image(aes(image = run_image), size = 0.05)+
  theme_ipsum_rc()
g&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/eliud-kichoge/index_files/figure-html/unnamed-chunk-14-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Not bad. I like the icons although the whole graph might be a bit misleading if readers perceive that these are the only records there are. However, this is a tutorial and we‚Äôll also add that note into our plot momentarily. You can adjust the size and other parameters of the images we plot, here for example I chose to adjust the size from its default. I also added &lt;code&gt;theme_ipsum_rc&lt;/code&gt; from the &lt;code&gt;{hrbrthemes}&lt;/code&gt; package for a quick aesthetic theme I like.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;plot-aesthetics&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Plot Aesthetics&lt;/h3&gt;
&lt;p&gt;So the plot so far looks nice, but we want it to be aesthetic and also to easily understand the progress of records across years. In order to do that, let‚Äôs turn to adjust both the y and x axis, and following that add some information to understand what we‚Äôre looking at:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;g1 &amp;lt;-  g +
    scale_x_continuous(name = &amp;quot;Year&amp;quot;,
                        limits = c(1900,2025),
                        breaks = seq(1900,2020,10),
                        labels = c(&amp;quot;1900&amp;quot;, paste0(&amp;quot;&amp;#39;&amp;quot;, seq(10,90,10)),&amp;quot;2000&amp;quot;,
&amp;quot;&amp;#39;10&amp;quot;,&amp;quot;&amp;#39;19&amp;quot;))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here we added some nice x labels in a format that‚Äôs both concise and informative. I remember taking this from &lt;a href=&#34;https://github.com/LiamDBailey/TidyTuesday/blob/master/R/17_09_2019.R&#34;&gt;Liam Bailey‚Äôs&lt;/a&gt; &lt;a href=&#34;https://github.com/rfordatascience/tidytuesday&#34;&gt;#Tidytuesday&lt;/a&gt; plot a while back when i first made this visualiztion. What we did was teak the &lt;code&gt;scale_x_continuous&lt;/code&gt; by assigning a &lt;code&gt;name&lt;/code&gt; to the axis, expanding its &lt;code&gt;limits&lt;/code&gt;, added specific &lt;code&gt;breaks&lt;/code&gt; and then a &lt;code&gt;label&lt;/code&gt; for each break using &lt;code&gt;paste0&lt;/code&gt;. Note that you must have the same number of labels and breaks for the plot to render so it‚Äôs important to have the sequences identical in length; otherwise it‚Äôll return an error. With the &lt;code&gt;paste0&lt;/code&gt; we can add any value or observation and then ‚Äòstick‚Äô to it whatever else we want. Using that we are able to create years in the format of ‚Äô10 and so on. It is also possible to use the &lt;code&gt;{glue}&lt;/code&gt; package which I heard is very intuitive, maybe next post I‚Äôll give that a try.&lt;/p&gt;
&lt;p&gt;Next, let‚Äôs change our y duration axis:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;g2 &amp;lt;- 
  g1 + scale_y_time(name = &amp;quot;Time (hours)&amp;quot;,
                  limits = c(100,180),
                  breaks = seq(100,180,10),
                  labels = c(&amp;quot;1:40&amp;quot;,&amp;quot;1:50&amp;quot;, &amp;quot;2:00&amp;quot;, &amp;quot;2:10&amp;quot;,&amp;quot;2:20&amp;quot;,&amp;quot;2:30&amp;quot;,&amp;quot;2:40&amp;quot;, &amp;quot;2:50&amp;quot;, &amp;quot;3:00&amp;quot;))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If you recall, we previously mutated the column we read from Wikipedia into a period class and a duration of minutes. using the &lt;code&gt;scale_*_time&lt;/code&gt; (either x or y instead of *) we can work with an &lt;code&gt;hms&lt;/code&gt; object. What we did is add a &lt;code&gt;name&lt;/code&gt;, expand a little the &lt;code&gt;limits&lt;/code&gt;, add &lt;code&gt;breaks&lt;/code&gt; and &lt;code&gt;labels&lt;/code&gt; same as before. This time around we used our breaks as minutes, so every 60 minutes represents an hour. I initially used hours as the numeric value, but then it makes it harder to break every 10 minutes (that‚Äôll mean breaks every 0.166‚Ä¶). For the labels I was having some problems automating it so I comprimised on manually inputting it; I guess sometimes you just have to choose your battles between automating and manualy inserting.&lt;/p&gt;
&lt;p&gt;Let‚Äôs finish up by adding a title, subtitle and integrating last aesthetics to our plot.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;g3 &amp;lt;- g2 +
  labs(title = &amp;quot;How does Eliud Kipchoge marathon score compare to previous yearly records?&amp;quot;,
       subtitle = &amp;quot;Points are world records for every 10-15 years. \nEliud Kipchoge is the first to break the two-hour barrier (unofficially), Great job!&amp;quot;)+
    theme(
      panel.grid.minor = element_blank(),
      panel.grid.major = element_line(colour = &amp;quot;gray75&amp;quot;, size = 0.1, linetype = &amp;quot;dashed&amp;quot;),
      plot.title = element_text(size = 14),
      plot.subtitle = element_text(size = 10)
      )&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;After adding some labs I tweaked a bit the gridlines using &lt;code&gt;panel.grid&lt;/code&gt; minor or major. You can play around with them to see which minimilize your plot in the perfect way. I chose to leave the major grid lines since I find it easier to read the values with them. Although we defined a theme earlier on we can still tweak it by adding another &lt;code&gt;theme&lt;/code&gt; argument to the previous one as we just did.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;final-annotation&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Final annotation&lt;/h3&gt;
&lt;p&gt;Lastly, we want the new record to be evident and stand out in a first glance. Here I was somewhat debating between using a regular &lt;code&gt;geom_point&lt;/code&gt; instead of the &lt;code&gt;geom_image&lt;/code&gt; because then we could easily use a vertical line to highlight the 2:00 hour threshold. Since a line in this case will cut right through the icon, let‚Äôs use an arrow annotation instead.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;g4 &amp;lt;- g3 +
  geom_curve(aes(x = 2018, y = 120, xend = 2015, yend = 113),
             colour = &amp;quot;black&amp;quot;, size = 0.9, curvature = 0.5,
             arrow = arrow(length = unit(2,&amp;quot;mm&amp;quot;), type = &amp;quot;closed&amp;quot;))+
  annotate(&amp;quot;text&amp;quot;, x=2010, y= 105, 
           label = &amp;quot;Eliud Kipchoge\n12.10.2019\n1:59:40&amp;quot;,
           color = &amp;quot;black&amp;quot;, size = 3, hjust = 0)

g4&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/eliud-kichoge/index_files/figure-html/unnamed-chunk-18-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;And voila!&lt;/p&gt;
&lt;p&gt;In our final touches we added both an arrow and text to explain what we‚Äôre seeing. I decided to go with a &lt;code&gt;geom_curve&lt;/code&gt; arrow where we can set the start and end of the arrow along with the kind of curve we want. We then set the curve to be &lt;code&gt;arrow&lt;/code&gt; and adjust its length. You can also use a closed head arrow, for more on that read on &lt;code&gt;?arrow&lt;/code&gt; as part of the &lt;code&gt;geom_curve&lt;/code&gt; or &lt;code&gt;geom_segment&lt;/code&gt; you can use here.&lt;/p&gt;
&lt;p&gt;That‚Äôs it, seems like were good to go. &lt;strong&gt;Great job for Elihud Kipchoge&lt;/strong&gt; üëè&lt;br /&gt;
&lt;/br&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;appendix&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Appendix&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;When I initially created this visulization I was just starting with &lt;code&gt;R&lt;/code&gt;. I first created 11 slots, added 1921, the sequence of 1930-2010 and then a 2019 (reminder: When I first created this viz I took a different dataset altogether). Little did I know how to properly use the &lt;code&gt;c()&lt;/code&gt; function that we used in the current post.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;year.sub &amp;lt;- vector (&amp;quot;double&amp;quot;, 11)
year.sub[1] &amp;lt;- 1921
year.sub[2:10] &amp;lt;- seq(1930,2010,10)
year.sub[11] &amp;lt;- 2019&lt;/code&gt;&lt;/pre&gt;
&lt;center&gt;
&lt;a href=&#34;#top&#34;&gt; Back to top&lt;/a&gt;
&lt;/center&gt;
&lt;/div&gt;
</description>
      
            <category>ggimage</category>
      
            <category>rvest</category>
      
      
            <category>R</category>
      
    </item>
    
    <item>
      <title>Mapping bomb shelters in Be&#39;er-Sheva, IL</title>
      <link>/post/bomb-shelters/</link>
      <pubDate>Tue, 14 Jan 2020 00:00:00 +0000</pubDate>
      
      <guid>/post/bomb-shelters/</guid>
      <description>
&lt;script src=&#34;/rmarkdown-libs/htmlwidgets/htmlwidgets.js&#34;&gt;&lt;/script&gt;
&lt;script src=&#34;/rmarkdown-libs/jquery/jquery.min.js&#34;&gt;&lt;/script&gt;
&lt;link href=&#34;/rmarkdown-libs/leaflet/leaflet.css&#34; rel=&#34;stylesheet&#34; /&gt;
&lt;script src=&#34;/rmarkdown-libs/leaflet/leaflet.js&#34;&gt;&lt;/script&gt;
&lt;link href=&#34;/rmarkdown-libs/leafletfix/leafletfix.css&#34; rel=&#34;stylesheet&#34; /&gt;
&lt;script src=&#34;/rmarkdown-libs/Proj4Leaflet/proj4-compressed.js&#34;&gt;&lt;/script&gt;
&lt;script src=&#34;/rmarkdown-libs/Proj4Leaflet/proj4leaflet.js&#34;&gt;&lt;/script&gt;
&lt;link href=&#34;/rmarkdown-libs/rstudio_leaflet/rstudio_leaflet.css&#34; rel=&#34;stylesheet&#34; /&gt;
&lt;script src=&#34;/rmarkdown-libs/leaflet-binding/leaflet.js&#34;&gt;&lt;/script&gt;


&lt;div id=&#34;update-from-march-21-2020&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;&lt;a id=&#34;update&#34;&gt;&lt;/a&gt;&lt;strong&gt;Update from March 21, 2020&lt;/strong&gt;&lt;/h4&gt;
&lt;p&gt;I‚Äôve been wanting to return to this post and make this map more interactive. As a matter of fact it was easier than I thought, I just never got around to doing it. I won‚Äôt be going through the code for the leaflet map below but will leave it for whoever would like to review it:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(leaflet)
library(magrittr)
readr::read_csv(&amp;quot;shelters.csv&amp;quot;) %&amp;gt;% 
leaflet() %&amp;gt;% 
  addTiles() %&amp;gt;% 
  setView(34.7913, 31.25181,zoom = 13) %&amp;gt;% 
  addCircles(radius = 4, color = &amp;quot;red&amp;quot;, fill = TRUE)&lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;htmlwidget-1&#34; style=&#34;width:672px;height:480px;&#34; class=&#34;leaflet html-widget&#34;&gt;&lt;/div&gt;
&lt;script type=&#34;application/json&#34; data-for=&#34;htmlwidget-1&#34;&gt;{&#34;x&#34;:{&#34;options&#34;:{&#34;crs&#34;:{&#34;crsClass&#34;:&#34;L.CRS.EPSG3857&#34;,&#34;code&#34;:null,&#34;proj4def&#34;:null,&#34;projectedBounds&#34;:null,&#34;options&#34;:{}}},&#34;calls&#34;:[{&#34;method&#34;:&#34;addTiles&#34;,&#34;args&#34;:[&#34;//{s}.tile.openstreetmap.org/{z}/{x}/{y}.png&#34;,null,null,{&#34;minZoom&#34;:0,&#34;maxZoom&#34;:18,&#34;tileSize&#34;:256,&#34;subdomains&#34;:&#34;abc&#34;,&#34;errorTileUrl&#34;:&#34;&#34;,&#34;tms&#34;:false,&#34;noWrap&#34;:false,&#34;zoomOffset&#34;:0,&#34;zoomReverse&#34;:false,&#34;opacity&#34;:1,&#34;zIndex&#34;:1,&#34;detectRetina&#34;:false,&#34;attribution&#34;:&#34;&amp;copy; &lt;a href=\&#34;http://openstreetmap.org\&#34;&gt;OpenStreetMap&lt;\/a&gt; contributors, &lt;a href=\&#34;http://creativecommons.org/licenses/by-sa/2.0/\&#34;&gt;CC-BY-SA&lt;\/a&gt;&#34;}]},{&#34;method&#34;:&#34;addCircles&#34;,&#34;args&#34;:[[31.259018768,31.2597950910001,31.2592493760001,31.2573049280001,31.2574797520001,31.258424086,31.254246294,31.255173057,31.2556456460001,31.253666206,31.2537330280001,31.2569812850001,31.2570752350001,31.257608025,31.2575462630001,31.2700855640001,31.2702905,31.2695012190001,31.2658482960001,31.259725431,31.2646111440001,31.266921333,31.2604320470001,31.2596058720001,31.262762868,31.263789048,31.261942934,31.2638102080001,31.2632644710001,31.263504445,31.262432614,31.2708580180001,31.2711996570001,31.270725645,31.2712747300001,31.2718630300001,31.2720855330001,31.2726126790001,31.272427384,31.2704051920001,31.266736774,31.2628119170001,31.2455927190001,31.248994151,31.250188747,31.2459192310001,31.24676858,31.2479923820001,31.2478412890001,31.249363784,31.250175772,31.248725547,31.248610904,31.251846362,31.2518736100001,31.250946067,31.253505949,31.257884104,31.2540270010001,31.2559118880001,31.2710156910001,31.271205497,31.270356722,31.269982842,31.2698227120001,31.269304209,31.269990116,31.2687801060001,31.2682527530001,31.2683451450001,31.270695158,31.27323147,31.2733343200001,31.2737922810001,31.2728741080001,31.274249592,31.2746572610001,31.272394175,31.2739432400001,31.2744760080001,31.2717045570001,31.2729638220001,31.2757156430001,31.2741278410001,31.2746324350001,31.274596902,31.271494137,31.2718457610001,31.275141078,31.275589424,31.2759297440001,31.2649303200001,31.270103943,31.2659409890001,31.2669887380001,31.2697833300001,31.2704315440001,31.266575474,31.266131746,31.221500807,31.224164194,31.223587228,31.2258336940001,31.222024801,31.2197570490001,31.221235918,31.224912993,31.224000586,31.2261810250001,31.223021734,31.2238066,31.2209769780001,31.222977471,31.2321786260001,31.2324369340001,31.2431229220001,31.235916617,31.238806793,31.237065128,31.248259068,31.251491909,31.2504392380001,31.2504933070001,31.2518083160001,31.246153377,31.250630739,31.25563615,31.256696517,31.254957205,31.257290099,31.25460144,31.256567933,31.255031301,31.2575173870001,31.2579245330001,31.2579056490001,31.2226565670001,31.224564345,31.2426843960001,31.240248009,31.2399610090001,31.267945217,31.2638298580001,31.270236595,31.269875347,31.265144816,31.2690648330001,31.2681699770001,31.269377084,31.265713315,31.270953035,31.2722040720001,31.2645033750001,31.271228369,31.2715811,31.256697924,31.257101161,31.269666226,31.275281239,31.274241079,31.269109099,31.272405753,31.2598086400001,31.250707328,31.251678113,31.2525321090001,31.2486613240001,31.2554510690001,31.2490194850001,31.2670087990001,31.2682888350001,31.269309503,31.250801417,31.268097267,31.2679672850001,31.267170004,31.261017135,31.2604871560001,31.261171405,31.260286697,31.260856463,31.258957008,31.2669088850001,31.264312395,31.2644250700001,31.262675503,31.262628384,31.266354108,31.2378694280001,31.237196182,31.2522976180001,31.2551276230001,31.2555366730001,31.271009238,31.2697859260001,31.248898803,31.250194783,31.250113594,31.248703956,31.2482595930001,31.247643195,31.252913957,31.2562439010001,31.250285107,31.2561600820001,31.2510416860001,31.249713179,31.258150351,31.2590159170001,31.2581197600001,31.25558649,31.2571552970001,31.254918092,31.2545320300001,31.2465726200001,31.2520725430001,31.261599754,31.2721663470001,31.270971611,31.2616581070001,31.264973377,31.2658491130001,31.2688886780001,31.2683040970001,31.2707125310001,31.271428288,31.254769068,31.257070663,31.2465921090001,31.2692059390001,31.2443915640001,31.2314304170001,31.269208157,31.269363072,31.270308577,31.2595743270001,31.2700607460001,31.2564875580001,31.2479156200001,31.2481547410001,31.254812955,31.266566502,31.2504635080001,31.2541823770001,31.2683791260001,31.271307544,31.2482951640001,31.2737507580001,31.272590789,31.2570404000001,31.252699315,31.2517972700001,31.2663416,31.2709878490001,31.2660293330001,31.25408293,31.2701366480001,31.2402113960001,31.2646567780001,31.2463304390001,31.2347788370001,31.252428283],[34.808214546,34.8078915740001,34.809368438,34.8093634950001,34.810975436,34.810334568,34.805622382,34.8026935250001,34.804473768,34.8095251010001,34.8075520780001,34.808555186,34.7651206140001,34.76364862,34.7628454490001,34.778674504,34.7777073990001,34.7762277100001,34.770138844,34.7869285290001,34.795184944,34.7997725760001,34.7940810010001,34.7916208840001,34.7906790590001,34.79068958,34.7927366010001,34.79231459,34.7927492380001,34.792743929,34.7905235110001,34.7883433920001,34.7867639080001,34.785602972,34.7852056130001,34.787070295,34.787588817,34.7939116200001,34.7924905290001,34.7941080740001,34.8014500780001,34.792161772,34.7956425140001,34.7970971840001,34.7970376860001,34.7936621440001,34.7914949660001,34.784890746,34.787583614,34.7873553950001,34.7857147330001,34.779422326,34.7796193240001,34.7841264850001,34.7828629210001,34.7779282610001,34.782604796,34.7832538050001,34.7900850610001,34.7963698560001,34.806238138,34.8089210210001,34.809585251,34.8086036290001,34.809357278,34.8091260510001,34.8067740120001,34.8089092710001,34.8086805990001,34.8079129270001,34.804323229,34.800439962,34.8088667390001,34.8085067230001,34.809244467,34.808135475,34.8077908650001,34.8096215740001,34.8069308010001,34.806502575,34.8037255140001,34.802776814,34.802732577,34.8024465140001,34.8025668970001,34.804125362,34.802364975,34.8055181280001,34.8074389120001,34.8070149960001,34.8064910510001,34.7604294980001,34.7639117040001,34.765621094,34.7664207510001,34.7608955090001,34.7630732060001,34.7598450790001,34.7611068100001,34.775540274,34.7789006310001,34.7774103090001,34.778600274,34.7726666250001,34.773724196,34.7718155210001,34.775669044,34.780235401,34.780269586,34.7763314490001,34.775308716,34.7743583120001,34.7736159990001,34.7923748970001,34.78033953,34.779738781,34.784380764,34.7847258240001,34.7817552400001,34.792498504,34.7868126540001,34.789983894,34.789128825,34.787953332,34.7942136410001,34.7945532960001,34.7903546790001,34.7959888820001,34.7941725480001,34.795403445,34.7891443610001,34.793907173,34.7965042000001,34.7892947980001,34.781547866,34.783656121,34.7751580730001,34.780917947,34.781822312,34.781617833,34.783344335,34.7623511370001,34.762642929,34.771785949,34.7709577200001,34.7636153930001,34.7619093160001,34.7644633370001,34.764872847,34.7577812160001,34.770979358,34.765673103,34.7643799360001,34.767547478,34.769529817,34.773305672,34.773415368,34.805293723,34.805267585,34.800891017,34.808306987,34.806721075,34.8088518340001,34.8019847370001,34.8065743970001,34.810013871,34.8081264090001,34.7780347280001,34.782620235,34.7707858410001,34.7723190740001,34.774894422,34.77918176,34.7948698500001,34.7970907800001,34.795007293,34.79702511,34.7965872190001,34.795338615,34.7935443280001,34.7929193080001,34.7929353970001,34.7939598030001,34.795247798,34.797073217,34.7944644400001,34.7974265890001,34.792997642,34.785492021,34.7889980750001,34.804945893,34.8100962180001,34.808716111,34.783396279,34.7850399330001,34.795998975,34.805139436,34.8028419560001,34.8061534390001,34.804344707,34.807264053,34.81146289,34.806185188,34.806567597,34.81037658,34.80369548,34.8085987040001,34.811233825,34.809843013,34.8094435680001,34.8080172910001,34.810334235,34.8070907060001,34.8035195030001,34.8086055260001,34.802861137,34.7937478430001,34.7956186760001,34.793586141,34.7919188430001,34.79992937,34.801187988,34.8012474810001,34.792345267,34.7969623870001,34.794766757,34.8086733960001,34.80687017,34.776587685,34.79294107,34.791070099,34.7935582580001,34.7954634880001,34.7948531970001,34.7868227160001,34.7857320460001,34.795653003,34.791395838,34.7796560830001,34.781008178,34.77898084,34.794873022,34.780569927,34.8023777060001,34.7968104340001,34.7835412000001,34.7892121180001,34.807038339,34.8080192800001,34.8085495820001,34.779316936,34.8025542940001,34.7698468990001,34.8072728060001,34.8008629990001,34.8116456560001,34.7842904080001,34.7950379800001,34.7961240610001,34.8083349930001,34.7825503890001,34.808149154],4,null,null,{&#34;interactive&#34;:true,&#34;className&#34;:&#34;&#34;,&#34;stroke&#34;:true,&#34;color&#34;:&#34;red&#34;,&#34;weight&#34;:5,&#34;opacity&#34;:0.5,&#34;fill&#34;:true,&#34;fillColor&#34;:&#34;red&#34;,&#34;fillOpacity&#34;:0.2},null,null,null,{&#34;interactive&#34;:false,&#34;permanent&#34;:false,&#34;direction&#34;:&#34;auto&#34;,&#34;opacity&#34;:1,&#34;offset&#34;:[0,0],&#34;textsize&#34;:&#34;10px&#34;,&#34;textOnly&#34;:false,&#34;className&#34;:&#34;&#34;,&#34;sticky&#34;:true},null,null]}],&#34;setView&#34;:[[31.25181,34.7913],13,[]],&#34;limits&#34;:{&#34;lat&#34;:[31.2197570490001,31.2759297440001],&#34;lng&#34;:[34.7577812160001,34.8116456560001]}},&#34;evals&#34;:[],&#34;jsHooks&#34;:[]}&lt;/script&gt;
&lt;/div&gt;
&lt;div id=&#34;original-post&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;&lt;strong&gt;Original post&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;I‚Äôve been wanting to learn how to use maps in R for a while before creating the map in this post. Seeing dataframes with longitude and latitude coordinates on various occasions on &lt;a href=&#34;https://github.com/rfordatascience/tidytuesday&#34;&gt;#Tidytuesday&lt;/a&gt; encouraged me to do so.&lt;br /&gt;
A day before this visualizaiton, I discovered our municupality‚Äôs open access data &lt;a href=&#34;https://www.beer-sheva.muni.il/OpenData/Pages/default.aspx&#34;&gt;website&lt;/a&gt;. In this website you can find various datasets like street light coordinates, bomb shelters spread out in the city and more. A day after discovering it Israel, the country I live in, was fired missiles at. I decided to take the opportunity and map some of the shelters around my house. You know, just in case.&lt;/p&gt;
&lt;p&gt;Let‚Äôs begin with the packages (üì¶) we‚Äôll need:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#for data manipulation
library(tidyverse)
#for a nice map
library(ggmap)
#for reading and working with .geojson file
library(geojsonio)
library(sp)
#for integrating a nice font
library(extrafont)&lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;loading-and-tidying-the-data&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Loading and tidying the data&lt;/h3&gt;
&lt;p&gt;I initially tried using the &lt;code&gt;.csv&lt;/code&gt; file they have on their webiste but I was having too much trouble with the Hebrew so I decided to try and work with the &lt;code&gt;geojsonio&lt;/code&gt; package. I had no idea how to work with a &lt;code&gt;.geojson&lt;/code&gt; file or frankly how to work with maps in general. To my save, i found this incredible blog by &lt;a href=&#34;https://randomjohn.github.io/r-geojson-srt/&#34;&gt;John Johnson&lt;/a&gt; to help me transform a ‚Äògeomjson‚Äô file to a dataframe you can work with.&lt;/p&gt;
&lt;p&gt;Let‚Äôs begin:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#read the .geojson file
my_geojson &amp;lt;- &amp;quot;shelters.geojson&amp;quot;
#convert the .geojson file to an sp object
data_json &amp;lt;- geojson_read(my_geojson, what = &amp;quot;sp&amp;quot;)
#now we can convert it to a nice data frame
shelters &amp;lt;- as.data.frame(data_json)
#last tidying of the column names
names(shelters)[6:7] &amp;lt;- c(&amp;quot;long&amp;quot;, &amp;quot;lat&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;What we did was load the geojson file, read it as an ‚Äòsp‚Äô object and then turn it into to a dataframe. I changed the names so that it‚Äôll be easier to read the columns. we could also use &lt;code&gt;dplyr::rename&lt;/code&gt; but I liked the base R function Johnson used in his blog so I‚Äôll stick with that.&lt;br /&gt;
let‚Äôs look at the top 3 observations of our new data frame:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;##       long      lat        name X.U.05E7..U.05D5..U.05D3._.U.05E1.
## 1 34.80821 31.25902  &amp;lt;U+05D2&amp;gt;/2                                 17
## 2 34.80789 31.25980  &amp;lt;U+05D2&amp;gt;/1                                 17
## 3 34.80937 31.25925 &amp;lt;U+05D2&amp;gt;/25                                 17
##                elc group_ F_.U.05E6..U.05D5..U.05D5..U.05EA.
## 1 &amp;lt;U+05D9&amp;gt;&amp;lt;U+05E9&amp;gt;      0                                   
## 2 &amp;lt;U+05D9&amp;gt;&amp;lt;U+05E9&amp;gt;      0                                   
## 3 &amp;lt;U+05D9&amp;gt;&amp;lt;U+05E9&amp;gt;      0&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Ugh, well the names weren‚Äôt read well into &lt;code&gt;R&lt;/code&gt;. While this isn‚Äôt a big issue to resolve, I don‚Äôt find it necessary for the final piece. the names of the shelters are anyway in Hebrew and only represnt a letter and some sort of number (for e.g, A/23, only in Hebrew). Therefore we‚Äôll leave it as is since what I‚Äôm interested in is the longitude and latitude coordinations and for that we don‚Äôt need the character column.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;retrieving-the-map&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Retrieving the map&lt;/h3&gt;
&lt;p&gt;So we have our data frame with long and lat points, let‚Äôs get our map. I want a map that can be readable in terms of streets and roads, therefore I‚Äôll give the &lt;code&gt;ggmap&lt;/code&gt; package a try&lt;a href=&#34;#fn1&#34; class=&#34;footnote-ref&#34; id=&#34;fnref1&#34;&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt;. Google requires you to register in order to recieve an API key to pull maps to plot. Unfortunately I won‚Äôt cover how to regiser in this blog post but I‚Äôm sure you can find plenty of tutorials addressing it online.&lt;br /&gt;
Let‚Äôs get Be‚Äôer-Sheva‚Äôs map:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;b7_map &amp;lt;- get_map(location = c(34.7913 , 31.25181), 
              zoom = 13, scale = 2, maptype = &amp;quot;roadmap&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;What we did here was use the &lt;code&gt;get_map&lt;/code&gt; function to pull the map according to the long and lat coordinates I gave it of Be‚Äôer-Sheva. You should first pass the longitude and then the latitude in the &lt;code&gt;location&lt;/code&gt; argument. In addition you can change other features such as the zoom level, the maptype and more as we saw here (See &lt;code&gt;?get_map&lt;/code&gt; for more info).&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;plot&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Plot&lt;/h3&gt;
&lt;p&gt;Now that we have our data set ready and the map as an object we can go on to plot it. ggmap extends ggplot features so we can run the data frame smoothly into the &lt;code&gt;ggmap&lt;/code&gt; function:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggmap(b7_map)+
  geom_point(shelters, mapping = aes(long,lat),
            color = &amp;quot;red&amp;quot;, size = 0.3, shape = 15)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/bomb-shelters/index_files/figure-html/unnamed-chunk-6-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;What we did was pass the b7_map as an object into the &lt;code&gt;ggmap&lt;/code&gt; function and add a geom, in this case &lt;code&gt;geom_point&lt;/code&gt; representing our shelter coordinates. However, this map doesn‚Äôt really help me in a time of need since it doesn‚Äôt show &lt;em&gt;my address&lt;/em&gt; clearly.&lt;/p&gt;
&lt;p&gt;Let‚Äôs try zooming in so that we can see what we‚Äôre looking at:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#retreiving a new map with a greater `zoom`
b7_map_zoom &amp;lt;- get_map(location = c(34.7913 , 31.25181), 
                    zoom = 16, scale = 2, maptype = &amp;quot;roadmap&amp;quot;)

p &amp;lt;- ggmap(b7_map_zoom)+
geom_point(shelters, mapping = aes(long,lat), color = &amp;quot;red&amp;quot;,
           size = 3, shape = 15)
p&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/bomb-shelters/index_files/figure-html/unnamed-chunk-7-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Much nicer and clearer. Using the zoom option in &lt;code&gt;get_map&lt;/code&gt; enables to center more on where I want. Great, this shows me some bomb shetlers I have around me in a time of need. Let‚Äôs add some fine tuning for our theme:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;p+  
#for the title, caption and removing the X and Y axis
  labs (title = &amp;quot;Neighborhood B, Beer-Sheva, Israel, bomb shelters&amp;quot;,
        x = NULL, y = NULL,
        caption = &amp;quot;data: www.beer-sheva.muni.il | @Amit_Levinson&amp;quot;)+
  theme_minimal()+
  theme(text = element_text(family = &amp;quot;Microsoft Tai Le&amp;quot;),
    #Changing the position of the title
    plot.title = element_text(hjust = 0.5, size = 20, face = &amp;quot;bold&amp;quot;),
    axis.text = element_blank(),
    plot.caption = element_text(size = 9, face = &amp;quot;italic&amp;quot;, hjust = 0),
    panel.border = element_rect(color = &amp;quot;black&amp;quot;, size=2, fill = NA)
  )&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/bomb-shelters/index_files/figure-html/unnamed-chunk-8-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Perfect, I can now save the plot and distribute it if someone needs it.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggsave(&amp;quot;shelters_b_eng.png&amp;quot;, width = 8, height = 8)&lt;/code&gt;&lt;/pre&gt;
&lt;center&gt;
&lt;blockquote class=&#34;twitter-tweet&#34;&gt;&lt;p lang=&#34;en&#34; dir=&#34;ltr&#34;&gt;1.  I&amp;#39;ve been wanting to learn to plot maps in &lt;a href=&#34;https://twitter.com/hashtag/rstats?src=hash&amp;amp;ref_src=twsrc%5Etfw&#34;&gt;#rstats&lt;/a&gt;&lt;br&gt;2. Yesterday I encountered open data our municipality publishes&lt;br&gt;3. Today missiles are fired towards Israel in response to assassination of a top terrorist.&lt;br&gt;&lt;br&gt;1+2+3: Plotting bomb shelter locations near where I live&lt;a href=&#34;https://twitter.com/hashtag/ggmap?src=hash&amp;amp;ref_src=twsrc%5Etfw&#34;&gt;#ggmap&lt;/a&gt; &lt;a href=&#34;https://t.co/4Irz0ZKuZr&#34;&gt;pic.twitter.com/4Irz0ZKuZr&lt;/a&gt;&lt;/p&gt;&amp;mdash; Amit Levinson (@Amit_Levinson) &lt;a href=&#34;https://twitter.com/Amit_Levinson/status/1194274713759039488?ref_src=twsrc%5Etfw&#34;&gt;November 12, 2019&lt;/a&gt;&lt;/blockquote&gt;
&lt;script async src=&#34;https://platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;

&lt;/center&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;footnotes&#34;&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li id=&#34;fn1&#34;&gt;&lt;p&gt;For a first map I decided to go with a static one, but an interactive one can defenitely be a 2.0 version of this blog (as you saw with the &lt;a href=&#34;#update&#34;&gt;March 21st update&lt;/a&gt;. Hopefully we won‚Äôt need it.&lt;a href=&#34;#fnref1&#34; class=&#34;footnote-back&#34;&gt;‚Ü©&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
      
            <category>ggmap</category>
      
            <category>rvest</category>
      
      
            <category>R</category>
      
    </item>
    
  </channel>
</rss>
