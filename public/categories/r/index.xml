<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>R on Amit Levinson</title>
    <link>https://amitlevinson.com/categories/r/</link>
    <description>Recent content in R on Amit Levinson</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>&amp;copy; Amit Levinson {year}</copyright>
    <lastBuildDate>Sun, 03 Jan 2021 00:00:00 +0000</lastBuildDate>
    
        <atom:link href="https://amitlevinson.com/categories/r/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Automatic Rendering of a Plot with GitHub Actions</title>
      <link>https://amitlevinson.com/blog/automated-plot-with-github-actions/</link>
      <pubDate>Sun, 03 Jan 2021 00:00:00 +0000</pubDate>
      
      <guid>https://amitlevinson.com/blog/automated-plot-with-github-actions/</guid>
      <description>


&lt;style type=&#34;text/css&#34;&gt;
newcaption {
  font-size: 0.9em;
  text-align: center;
}
&lt;/style&gt;
&lt;img src=&#34;outcome.png&#34; /&gt;
&lt;center&gt;
&lt;newcaption&gt;Final outcome of a plot in my GitHub README automatically updating and rendering on a specific git commit.&lt;/newcaption&gt;
&lt;/center&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;TL;DR&lt;/strong&gt;: In this post we setup the code for having a GitHub Action automatically update a plot. I start by showing step by step how I plot my most frequently used R packages for &lt;a href=&#34;https://github.com/rfordatascience/tidytuesday&#34;&gt;#Tidytuesday&lt;/a&gt;, a weekly data visualization project for the R community. However, I constantly add more information to that data every time I participate and will have to re-run the script to update the plot. For that purpose we turn to GitHub Actions and provide a workflow to run the R script on a specific Git commit. You can find a live example of the outcome in my &lt;a href=&#34;https://github.com/AmitLevinson/TidyTuesday&#34;&gt;#TidyTuesday repository&lt;/a&gt;.&lt;/p&gt;
&lt;div id=&#34;preface&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Preface&lt;/h3&gt;
&lt;p&gt;Before we begin, I want to thank &lt;a href=&#34;https://ramikrispin.github.io/&#34;&gt;Rami Krispin&lt;/a&gt; and &lt;a href=&#34;https://geobgu.xyz/&#34;&gt;Michael Dorman&lt;/a&gt;. I contacted Rami a while back asking for advice on working with GitHub Actions. Rami helped me solve several problems I had and saved me a lot of frustration.&lt;/p&gt;
&lt;p&gt;As to Michael, I thank him for the example I use in this blog post. A while back Michael posted in an Israeli R Facebook group a plot detailing &lt;a href=&#34;https://gist.github.com/michaeldorman/ad8d89136f03769105ccc6199a913f0b?fbclid=IwAR09u1WFD9YpKYb7nvezOKqGGZac3AuVWC7ogDItSv9sXXS0x2KpYxg3aU0&#34;&gt;the packages he frequently used at that time&lt;/a&gt;. I found it a great starting point to learn more about automation by trying out GitHub Actions to update a plot with my frequently used packages in #TidyTuesday submissions.&lt;/p&gt;
&lt;p&gt;Speaking of automation, let’s dive in!&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://media.giphy.com/media/XCxcmEQWxDdc8qsd2R/giphy.gif&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;why-automate&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Why automate?&lt;/h3&gt;
&lt;p&gt;To answer that let’s start with understanding what GitHub Actions are. Taken from GitHub’s website:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Automate, customize, and execute your software development workflows right in your repository with GitHub Actions. You can discover, create, and share actions to perform any job you’d like, including CI/CD, and combine actions in a completely customized workflow.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Reading up on various things you can automate made me eager to try it out - &lt;strong&gt;For example, automating a script to run every Sunday morning, sending an email, updating a dashboard hosted online, collecting data and more.&lt;/strong&gt; One example that I saw while working on this post was &lt;a href=&#34;https://www.hvitfeldt.me/&#34;&gt;Emil Hvitfeldt’s&lt;/a&gt; awesome paletter &lt;a href=&#34;https://twitter.com/BotPaletteer&#34;&gt;bot&lt;/a&gt;. A Twitter bot that automatically tweets color palette packages once a day.&lt;/p&gt;
&lt;center&gt;
&lt;blockquote class=&#34;twitter-tweet&#34;&gt;&lt;p lang=&#34;en&#34; dir=&#34;ltr&#34;&gt;Hello Twitter! It is Saturday and I have a gorgeous color palette for you!&lt;br&gt;&lt;br&gt;🔥The spinifex palette from the {colRoz} package🔥&lt;br&gt;&lt;br&gt;colRoz is available on Github:&lt;a href=&#34;https://t.co/qChZrvdXY0&#34;&gt;https://t.co/qChZrvdXY0&lt;/a&gt; &lt;a href=&#34;https://t.co/nAiKTy0dH1&#34;&gt;pic.twitter.com/nAiKTy0dH1&lt;/a&gt;&lt;/p&gt;&amp;mdash; paletteerBot (@BotPaletteer) &lt;a href=&#34;https://twitter.com/BotPaletteer/status/1340376626853535745?ref_src=twsrc%5Etfw&#34;&gt;December 19, 2020&lt;/a&gt;&lt;/blockquote&gt;
&lt;script async src=&#34;https://platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;

&lt;newcaption&gt;&lt;a href=&#34;https://www.hvitfeldt.me/&#34;&gt;Emil Hvitfeldt’s&lt;/a&gt; Twitter bot tweeting color palettes once a day using GitHub Actions&lt;/newcaption&gt;
&lt;/center&gt;
&lt;/div&gt;
&lt;div id=&#34;my-tidytuesday-packages&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;My TidyTuesday packages&lt;/h2&gt;
&lt;p&gt;In this post I’ll go through creating the infrastructure to automatically update a plot I present in my TidyTuesday GitHub README file. In short, &lt;a href=&#34;https://github.com/rfordatascience/tidytuesday&#34;&gt;#Tidytuesday&lt;/a&gt; is an awesome weekly data project where individuals analyze and visualize new data every week. I find it a terrific learning experience and try to participate when I can.&lt;/p&gt;
&lt;p&gt;Every TidyTuesday I open a new R session resulting in the creation of multiple R files, and more relevant to us, the packages we use each session. In the following section we’ll read in the R scripts from all my Tidytuesday participations and see what plots do I frequently use. Following that we’ll turn to GitHub Actions to configure a &lt;em&gt;workflow&lt;/em&gt; that automatically updates our created plot when we add new data.&lt;/p&gt;
&lt;p&gt;Side note: if you just want to quickly visualize your #TidyTuesday code without any GitHub Actions, I highly recommend exploring &lt;a href=&#34;https://karaman.is/&#34;&gt;Georgis Karmanis&lt;/a&gt; &lt;a href=&#34;https://twitter.com/geokaramanis/status/1334437358519902209&#34;&gt;Tweet&lt;/a&gt; and code. There he used the &lt;code&gt;{textreadr}&lt;/code&gt; package to do a lot of the heavy lifting reading in and cleaning the files we’ll do below.&lt;/p&gt;
&lt;div id=&#34;reading-the-files&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Reading the files&lt;/h3&gt;
&lt;p&gt;&lt;a href=&#34;https://gist.github.com/michaeldorman/ad8d89136f03769105ccc6199a913f0b?fbclid=IwAR09u1WFD9YpKYb7nvezOKqGGZac3AuVWC7ogDItSv9sXXS0x2KpYxg3aU0&#34;&gt;Michael’s original script&lt;/a&gt; is fantastic, but what’s a blog post without challenges and tears. Therefore I decided to re-write his R-code using purrr and other friends. Let’s start by loading the packages and reading in the files:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# For map iterations
library(purrr)
# Cleaning text
library(stringr)
# Data manipulation
library(dplyr)
# For plotting
library(ggplot2)
# Adding a font for the plot
library(extrafont)

# Ideally, use the here package and not complete paths
tt_path &amp;lt;- &amp;quot;C:/Users/amitl/R_code/tidytuesday&amp;quot;
files &amp;lt;- list.files(path = &amp;quot;C:/Users/amitl/R_code/tidytuesday&amp;quot;, pattern = &amp;quot;\\.R$|.Rmd$&amp;quot;, recursive = TRUE)
files &amp;lt;- files[!str_detect(files, &amp;quot;packages-plot.R&amp;quot;)]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;First and most important, I recommend using R projects and the &lt;a href=&#34;https://github.com/jennybc/here_here&#34;&gt;{here}&lt;/a&gt; package by &lt;a href=&#34;https://jennybryan.org/&#34;&gt;Jennifer Bryan&lt;/a&gt; for file management. The only reason I’m using a direct path is because when writing this post my R session assumes I’m within my website directory, making it a challenge to move between project roots&lt;a href=&#34;#fn1&#34; class=&#34;footnote-ref&#34; id=&#34;fnref1&#34;&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;We started by listing all files ending in &lt;code&gt;.R&lt;/code&gt; or &lt;code&gt;.Rmd&lt;/code&gt; in that specific path, also searching through sub-folder using &lt;code&gt;recursive = TRUE&lt;/code&gt;. I then filter the script I use to produce the plot, since I don’t want to include it in the analysis.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;head(files)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;2019/1_Week38_Number of Visitors/National Parks.R&amp;quot;       
## [2] &amp;quot;2019/2_Week39_SchoolDiversity/School_Diversity.R&amp;quot;        
## [3] &amp;quot;2019/2_Week39_SchoolDiversity/School_Diversity_Updated.R&amp;quot;
## [4] &amp;quot;2019/Week40_All the Pizza/Barstool_Top_2_Percent.R&amp;quot;      
## [5] &amp;quot;2019/Week40_All the Pizza/BarstoolPizza.R&amp;quot;               
## [6] &amp;quot;2019/Week41_Power_lifting/ipf.R&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We now have the R scripts I used to produce my TidyTuesdays. Unfortunately, it also returns R scripts I opened but didn’t completely follow through with the analysis. That is, sometimes I worked on a TidyTuesday but stopped and left it incomplete without producing a plot (shame on me). There’s not that many of them and for the purpose of the post we’ll leave them there.&lt;/p&gt;
&lt;p&gt;Before we dig in I want to get the names of the files. It’s not required for the analysis, but I found in nicer to look at the packages used corresponding to where they came from.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Get names
file_names &amp;lt;- str_extract(files, &amp;#39;[^/]+(?=\\.)&amp;#39;)

head(file_names)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;National Parks&amp;quot;           &amp;quot;School_Diversity&amp;quot;        
## [3] &amp;quot;School_Diversity_Updated&amp;quot; &amp;quot;Barstool_Top_2_Percent&amp;quot;  
## [5] &amp;quot;BarstoolPizza&amp;quot;            &amp;quot;ipf&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Our original string was “2019/1_Week38_Number of Visitors/National Parks.R”, and we wanted everything after the last &lt;code&gt;/&lt;/code&gt; but before the period - “National Parks”. We achieve this using the regular expression &lt;code&gt;[^/]+&lt;/code&gt; and a positive look ahead &lt;code&gt;(?=\\.)&lt;/code&gt;, capturing everything between the last backslash and a period. Voila, our final output of “National Parks”, my first #TidyTuesday R script.&lt;/p&gt;
&lt;p&gt;Next we read in the text from all the R scripts:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;tt_path &amp;lt;- paste0(tt_path, &amp;quot;\\/&amp;quot; ,files)
file_lines &amp;lt;- map(tt_path, readLines)

head(file_lines[[1]])&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;library(tidyverse)&amp;quot;                                                                                                                              
## [2] &amp;quot;library(png)&amp;quot;                                                                                                                                    
## [3] &amp;quot;library(gridGraphics)&amp;quot;                                                                                                                           
## [4] &amp;quot;&amp;quot;                                                                                                                                                
## [5] &amp;quot;park_visits &amp;lt;- readr::read_csv(\&amp;quot;https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2019/2019-09-17/national_parks.csv\&amp;quot;)&amp;quot;
## [6] &amp;quot;&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;After creating a complete path of our scripts we iterated across them using &lt;code&gt;map&lt;/code&gt; and the function &lt;code&gt;readLines&lt;/code&gt;. You can read &lt;code&gt;map&lt;/code&gt; as follows: Run function &lt;code&gt;readLines&lt;/code&gt; on every element of vector &lt;code&gt;tt_path&lt;/code&gt; (The vector containing our path to each &lt;code&gt;.R&lt;/code&gt;/&lt;code&gt;.Rmd&lt;/code&gt; script). For a better understanding of the &lt;code&gt;map&lt;/code&gt; function and family I highly recommend exploring the &lt;a href=&#34;https://purrr.tidyverse.org/reference/map.html&#34;&gt;documentation&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;cleaning&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Cleaning&lt;/h3&gt;
&lt;p&gt;If my memory is correct, I mainly use functions either by loading the library through &lt;code&gt;library()&lt;/code&gt; or reference specific argument with &lt;code&gt;library::&lt;/code&gt;. We’ll capture both options using some regex and return them as a dataframe:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;names(file_lines) &amp;lt;- file_names
file_packages &amp;lt;- map_dfr(file_lines, ~ tibble(package = str_extract(.x, &amp;quot;((?&amp;lt;=library\\().+(?=\\))|\\w+(?=::))&amp;quot;)),
  .id = &amp;quot;tidytuesday&amp;quot;) %&amp;gt;% 
  filter(!is.na(package)) %&amp;gt;% 
  distinct(tidytuesday, package)

head(file_packages)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 6 x 2
##   tidytuesday      package     
##   &amp;lt;chr&amp;gt;            &amp;lt;chr&amp;gt;       
## 1 National Parks   tidyverse   
## 2 National Parks   png         
## 3 National Parks   gridGraphics
## 4 National Parks   readr       
## 5 School_Diversity tidyverse   
## 6 School_Diversity stringr&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Perfect! First I added the original script names for each sub list within the &lt;code&gt;file_lines&lt;/code&gt;. As to the analysis Let’s tackle each element at a time from the inside out, starting with the regular expressions.&lt;/p&gt;
&lt;p&gt;We can split the regex into two parts. the first one &lt;code&gt;(?&amp;lt;=library\\()).+(?=\\())&lt;/code&gt; is both a positive look behind capturing everything after &lt;code&gt;library\\(&lt;/code&gt; until it reaches &lt;code&gt;(?=\\))&lt;/code&gt;, a positive look ahead for a parentheses. That way we capture everything in between the parentheses but remove the string &lt;code&gt;library()&lt;/code&gt;. The second section of the regex &lt;code&gt;\\w+(?=::))&lt;/code&gt; mimics the previous one but this time only with a positive look ahead – Capture the word (&lt;code&gt;\\w+&lt;/code&gt;) before the two &lt;code&gt;::&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;We iterate the &lt;code&gt;str_extract&lt;/code&gt; across each sub-list of the &lt;code&gt;file_lines&lt;/code&gt; object using &lt;code&gt;map_dfr&lt;/code&gt; to return a tibble. Since it returns a tibble, a variant of a data frame, we can assign the new vector a column name - &lt;code&gt;package&lt;/code&gt;. In addition, &lt;code&gt;map_df*&lt;/code&gt; (map_df, map_dfr, etc.) can take an &lt;code&gt;.id&lt;/code&gt; argument that will return the names of the original sub-lists as a column, in our case the names of our files we earlier cleaned. After filtering any &lt;code&gt;NA&lt;/code&gt; values such as empty lines, I used &lt;code&gt;distinct&lt;/code&gt; to remove duplicate called packages in each week. For example, if I called &lt;code&gt;stringr::&lt;/code&gt; twice in an R script I only want to count it once per session.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;plot&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;plot&lt;/h3&gt;
&lt;p&gt;We’re at the last part of reading-processing-plotting journey. The plot is pretty straight forward – we count the frequency of packages, relevel them by a descending order, take the top 15 frequently used and plot it as a bar plot.&lt;/p&gt;
&lt;p&gt;If you decide to try another plot remember that you want it to automatically update, so make sure you create something that won’t require you to constantly return to the code for editing.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Uncomment if you use it in a GitHub Action
# library(showtext)
# font_add_google(&amp;quot;Roboto Condensed&amp;quot;, &amp;quot;Roboto&amp;quot;)
# showtext_auto()

file_packages %&amp;gt;% 
  count(package, sort = T) %&amp;gt;%
  mutate(package = factor(package, levels = rev(package))) %&amp;gt;% 
  slice(1:15) %&amp;gt;% 
  ggplot()+
  geom_col(aes(y= package, x = n), fill = &amp;quot;gray45&amp;quot;)+
  labs(title = &amp;quot;Frequently used packages in #Tidytuesday&amp;quot;,
       subtitle = &amp;quot;Plot is rendered on every &amp;#39;initial commit&amp;#39; to this repository, showing my 15 most frequently\nused packages in #TidyTuesday&amp;quot;,
       x = &amp;quot;Number of times used&amp;quot;, y = &amp;quot;Package name&amp;quot;,
       caption = paste0(&amp;quot;Total scripts: &amp;quot;, length(file_names),
                        &amp;quot;\nLast updated:&amp;quot;,format(Sys.Date(), &amp;quot;%b %d, %Y&amp;quot;)))+
  theme_minimal()+
  theme(
    text = element_text(family = &amp;quot;Roboto Condensed&amp;quot;),
    plot.title = element_text(size = 18),
    plot.title.position = &amp;quot;plot&amp;quot;,
    plot.subtitle = element_text(size = 11, color = &amp;quot;gray20&amp;quot;),
    plot.caption = element_text(size = 8, color = &amp;quot;gray30&amp;quot;, face = &amp;quot;italic&amp;quot;),
    axis.title = element_text(color = &amp;quot;gray40&amp;quot;, size = 9),   
    axis.text.x = element_text(size = 10),
    axis.text.y = element_text(size = 11),
    panel.grid.major.y = element_blank(),
    plot.margin = unit(c(4,2,2,4), &amp;quot;mm&amp;quot;)
  )

# Save somewhere you&amp;#39;ll reference later
# ggsave (&amp;quot;extra/packages-used.png&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://amitlevinson.com/blog/automated-plot-with-github-actions/index_files/figure-html/unnamed-chunk-7-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;A few specific things to note:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;In the GitHub Action I use &lt;a href=&#34;https://github.com/yixuan/showtext&#34;&gt;&lt;code&gt;showtext&lt;/code&gt;&lt;/a&gt; as it enables to load a font that’s not currently installed on the computer. You might need to play around with the text size if you use this package, as the appearance of graphs might vary across operating systems. Here I used the&lt;code&gt;{extrafont}&lt;/code&gt; package since for some reason showtext wasn’t properly rendering for me in this blog post.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;In the caption I added two additional pieces of information we can insert to take advantage of the automation. The First part is the number of scripts used to produce the plot (&lt;code&gt;file_names&lt;/code&gt;) when the Action ran. Second, the caption adds the current date on every render, i.e. every time we push to our #TidyTuesday repository and activate the GitHub Action.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;An issue that’s left is when calling &lt;code&gt;tidyverse&lt;/code&gt; essentially we load packages such as &lt;code&gt;readr&lt;/code&gt;, &lt;code&gt;ggplot2&lt;/code&gt;, etc. We see that &lt;code&gt;readr&lt;/code&gt; dominates the plot of packages because I call it explicitly when loading the data, compared to ggplot2 that I only load using &lt;code&gt;tidyverse&lt;/code&gt;. So do I want to leave &lt;code&gt;readr&lt;/code&gt; or remove it if I already load it with the &lt;code&gt;tidyverse&lt;/code&gt;? Here I leave it as is, but let me know if you decided to tackle it (One option was parsing all scripts and seeing which package is used, but that can take a good while).&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;github-actions&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;GitHub Actions&lt;/h2&gt;
&lt;p&gt;&lt;em&gt;See additional learning resources below&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://media.giphy.com/media/CmFMWpEa4IFtS/giphy.gif&#34; /&gt;&lt;/p&gt;
&lt;p&gt;GitHub actions provide an easy way to automate various &lt;em&gt;workflows&lt;/em&gt;. We give GitHub a set of instructions on when to run the Action, and what exactly does it need to do. Instructions are aggregated as a script, in our case a ‘YAML’ file with specific commands&lt;a href=&#34;#fn2&#34; class=&#34;footnote-ref&#34; id=&#34;fnref2&#34;&gt;&lt;sup&gt;2&lt;/sup&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;We have our plot showcasing our most frequently used packages, but we want it rendered for us on every participation in #TidyTuesday. That’s exactly where GitHub Actions can help us by running the script for us. &lt;strong&gt;We setup the code once, like we did before, and have our workflow run it when we choose.&lt;/strong&gt;&lt;/p&gt;
&lt;div id=&#34;action-setup&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Action Setup&lt;/h3&gt;
&lt;p&gt;If you’re not an R user, or don’t want to use the &lt;code&gt;usethis&lt;/code&gt; R package to create the Action workflow (our set of instructions), you can setup an action manually. Just click on the Action tab in your repository &lt;i class=&#34;fas fa-arrow-right&#34;&gt;&lt;/i&gt; ‘new workflow’ and setup a workflow template yourself.&lt;/p&gt;
&lt;p&gt;The &lt;code&gt;{usethis}&lt;/code&gt; package makes it easy to work with GitHub Actions by providing great &lt;code&gt;yaml&lt;/code&gt; templates and folder setups for various occasions. I found the &lt;code&gt;README&lt;/code&gt; template adequate so we’ll create that. Assuming you have the package installed, just type the following in the r console in the root folder of your Tidytuesday repository (or whatever repository you want the Action to run on):&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;usethis::use_github_action()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;And you should receive the following output, only with your information instead:&lt;/p&gt;
&lt;pre class=&#34;yaml&#34;&gt;&lt;code&gt;√ Setting active project to &amp;#39;C:/Users/amitl/R_code/TidyTuesday&amp;#39;
√ Creating &amp;#39;.github/&amp;#39;
√ Adding &amp;#39;^\\.github$&amp;#39; to &amp;#39;.Rbuildignore&amp;#39;
√ Adding &amp;#39;*.html&amp;#39; to &amp;#39;.github/.gitignore&amp;#39;
√ Creating &amp;#39;.github/workflows/&amp;#39;
√ Writing &amp;#39;.github/workflows/R-CMD-check.yaml&amp;#39;
* Copy and paste the following lines into &amp;#39;C:/Users/amitl/R_code/TidyTuesday/README.md&amp;#39;:
  &amp;lt;!-- badges: start --&amp;gt;
  [![R build status](https://github.com/AmitLevinson/TidyTuesday/workflows/R-CMD-check/badge.svg)](https://github.com/AmitLevinson/TidyTuesday/actions)
  &amp;lt;!-- badges: end --&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Perfect. We’re not going to copy the lines suggested as they’re helpful for package checks, and we’re only interested in producing a plot. The important outcome for us from running the line is the newly produced output under &lt;code&gt;./github/workflows/R-CMD-check.yaml&lt;/code&gt;. We’re going to use that yaml file to write up what we want GitHub to do.&lt;/p&gt;
&lt;p&gt;Feel free to rename the &lt;em&gt;worflow&lt;/em&gt; file; as long as it’s a &lt;code&gt;yaml&lt;/code&gt; file under &lt;code&gt;/.github/workflows&lt;/code&gt; GitHub will process it as an Action automatically. No further setup is needed, unless you need to use &lt;a href=&#34;https://docs.github.com/en/free-pro-team@latest/actions/learn-github-actions/security-hardening-for-github-actions#using-secrets&#34;&gt;‘secretes’&lt;/a&gt;, e.g. for confidential API keys.&lt;/p&gt;
&lt;p&gt;Below are the instructions we implement in our yaml file:&lt;/p&gt;
&lt;pre class=&#34;yaml&#34;&gt;&lt;code&gt;name: Render library update

on: [push]

jobs:
  build:
    runs-on: macOS-latest
    if: &amp;quot;contains(github.event.head_commit.message, &amp;#39;initial commit&amp;#39;)&amp;quot;
    steps:
      - uses: actions/checkout@v2
      - uses: r-lib/actions/setup-r@v1
      - name: Install packages
        run:
          Rscript -e &amp;quot;install.packages(c(&amp;#39;tidyverse&amp;#39;, &amp;#39;showtext&amp;#39;))&amp;quot;
      - name: Render r plot file
        run:
          Rscript -e &amp;quot;source(&amp;#39;extra/packages-plot.R&amp;#39;)&amp;quot;
      - name: Commit results
        run: |
          git config --local user.email &amp;quot;actions@github.com&amp;quot;
          git config --local user.name &amp;quot;GitHub Actions&amp;quot;
          git add extra/packages-used.png
          git commit -m &amp;#39;Re-build package plot&amp;#39; || echo &amp;quot;No changes to commit&amp;quot;
          git push origin || echo &amp;quot;No changes to commit&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;whats-all-this-yaml&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;What’s all this yaml&lt;/h3&gt;
&lt;p&gt;Let’s break up the script piece by piece:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;code&gt;name: Render library update&lt;/code&gt; - The name of the the GitHub Action. This is what will appear under the workflows section in the GitHub actions tab.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;code&gt;on: [push]&lt;/code&gt; - Tells GitHub when to activate the workflow. There’s a plethora of options to use here; for example, you can have it run on every commit, a pull request or both. In addition, you can have it run at a specific time frame using &lt;code&gt;schedule&lt;/code&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;code&gt;jobs: build:&lt;/code&gt; - I’m not sure what exactly they do. I know that for rendering README files you’d have &lt;code&gt;render&lt;/code&gt; instead of build, but honestly I don’t know what either does exactly.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;code&gt;runs-on: macOS-latest&lt;/code&gt; This tells GitHub on which operating system to run the r environment. You can also choose to run the action on several OS such as Ubuntu and Windows. It can be extremely useful when you’re checking out packages and want to verify they work on various operating systems. I use on my own computer Windows and left it with &lt;code&gt;macOS-latest&lt;/code&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;code&gt;if: &#34;contains(github.event.head_commit.message, &#39;initial commit&#39;)&#34;&lt;/code&gt; - Here we provide GitHub an ‘exit’ option when running the Action, and the condition on when to run it. &lt;code&gt;github.event.head_commit.message&lt;/code&gt; captures the commit message you used before pushing using a regex we provide - ‘initial commit’. Without a logical condition the Action will follow through on every push; however, we usually want it to run only when adding new scripts and not when modifying existing ones.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    We use a conditional statement to tell GitHub when to run our Action. If the condition returns true, in this case matching our commit message, GitHub Actions will complete the workflow. Otherwise the Action will stop and exit before processing the following steps.
  &lt;/div&gt;
&lt;/div&gt;
&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;code&gt;steps...&lt;/code&gt; - Telling GitHub the exact steps we want the action to do. The script starts with &lt;code&gt;uses&lt;/code&gt; and then transitions between &lt;code&gt;name&lt;/code&gt; and &lt;code&gt;run&lt;/code&gt;.&lt;br /&gt;
&lt;code&gt;uses&lt;/code&gt; tells GitHub what to use: In the first one, &lt;a href=&#34;https://github.com/actions/checkout&#34;&gt;&lt;code&gt;actions/checkout@v2&lt;/code&gt;&lt;/a&gt;, GitHub checks out our repository on which the workflow will run. Next we set up a &lt;a href=&#34;https://github.com/jimhester/setup-r&#34;&gt;remote r environment&lt;/a&gt; using the &lt;code&gt;r-lib/actions/setup-r@v1&lt;/code&gt; argument.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;In the following steps we provide a name for the step and the actual code. In the first step we install the packages we’ll be using and in the next one we &lt;code&gt;source&lt;/code&gt; the R code that produces our plot&lt;a href=&#34;#fn3&#34; class=&#34;footnote-ref&#34; id=&#34;fnref3&#34;&gt;&lt;sup&gt;3&lt;/sup&gt;&lt;/a&gt;, located in my ‘extra’ folder in the repository. Lastly we provide Git commands in which we config the built-in token (to run the git commands), add our rendered plot, commit and push the results.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;That’s it! Just upload the newly edited files, and activate the Action with the specific commit. &lt;strong&gt;Don’t forget to reference the plot from your repository’s README so it shows up.&lt;/strong&gt; If you’re not sure how you can see how I did it &lt;a href=&#34;https://raw.githubusercontent.com/AmitLevinson/TidyTuesday/master/README.md&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;div class=&#34;figure&#34; style=&#34;text-align: center&#34;&gt;&lt;span id=&#34;fig:unnamed-chunk-11&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;success.png&#34; alt=&#34;The different steps the Action does when activated&#34; width=&#34;960&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 1: The different steps the Action does when activated
&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;When the Action is running you will see an orange dot at the top of the root folder of your repository. Once it’s complete it should turn green if all went well, or red if not. After it finishes give it a few minutes as it might take some time until the plot is actually updated. You can explore this action in &lt;a href=&#34;https://github.com/AmitLevinson/TidyTuesday&#34;&gt;my #Tidytuesday repository&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;div class=&#34;alert alert-warning&#34;&gt;
  &lt;div&gt;
    If the above GitHub Action follows through with the workflow and produces a new plot, it will only be updated in your remote repository. You might be required to merge the remote content locally next time you want to push.
  &lt;/div&gt;
&lt;/div&gt;
&lt;/p&gt;
&lt;p&gt;In order to debug, i.e. if the orange dot turned to a red x, go into the ‘Actions’ tab in your repository, click on ‘Render library update’ or however you called the action &lt;i class=&#34;fas fa-arrow-right&#34;&gt;&lt;/i&gt; click on the last commit message &lt;i class=&#34;fas fa-arrow-right&#34;&gt;&lt;/i&gt; ‘build’ and then you should see a list of actions GitHub took. Try to identify where it stopped by locating the red x it marked the break with. Unfortunately debugging is a post in and of itself, and I’m currently not the one to write it.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Disabling:&lt;/strong&gt; If you wish to disable the GitHub Action, you can either remove the &lt;code&gt;.yaml&lt;/code&gt; file or disable the action from the repository’s settings.&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;&lt;span id=&#34;fig:unnamed-chunk-12&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;outcome.png&#34; alt=&#34;Final outcome of a plot automatically updating in our README file.&#34; width=&#34;509&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 2: Final outcome of a plot automatically updating in our README file.
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;closing-remarks&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Closing remarks&lt;/h3&gt;
&lt;p&gt;This was my first time using GitHub actions and hopefully not the last. I used here only one example, and might be a little niche for some people, but I hope you got the big picture – GitHub Actions are easy to use, and they can automate a lot of your work. Do let me know if you made an Action as a result of reading the post, I would love to see what you came up with.&lt;/p&gt;
&lt;p&gt;You can find the above example live in &lt;a href=&#34;https://github.com/AmitLevinson/TidyTuesday&#34;&gt;my #TidyTuesday repository&lt;/a&gt;, the complete R script I used &lt;a href=&#34;https://github.com/AmitLevinson/TidyTuesday/blob/master/extra/packages-plot.R&#34;&gt;here&lt;/a&gt;, and the YAML script for the GitHub Action &lt;a href=&#34;https://github.com/AmitLevinson/TidyTuesday/blob/master/.github/workflows/render-plot.yaml&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Lastly, I would like to highlight several resources I found extremely useful for learning more about GitHub Actions:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;Jim Hester’s &lt;a href=&#34;https://www.jimhester.com/talk/2020-rsc-github-actions/&#34;&gt;rstudio talk&lt;/a&gt; on GitHub actions - Great introduction and to get you excited about the opportunities available with GitHub Actions.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Gavin Simpson’s blog post &lt;a href=&#34;https://fromthebottomoftheheap.net/2020/04/30/rendering-your-readme-with-github-actions/&#34;&gt;‘Rendering your README with GitHub Actions’&lt;/a&gt; - A more hands-on approach on setting everything up, learning about the &lt;code&gt;yaml&lt;/code&gt; commands and implementing it in a README file.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;As I was finalizing this blog post I noticed Simon Couch published a fantastic blog post on using &lt;a href=&#34;https://blog.simonpcouch.com/blog/r-github-actions-commit/&#34;&gt;GitHub actions to run an R script on a schedule&lt;/a&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;That’s it for now. Hope you enjoyed this blog post and found it useful!&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;footnotes&#34;&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li id=&#34;fn1&#34;&gt;&lt;p&gt;If you decide to automate this with GitHub Actions, be alert if file paths throw an error.&lt;a href=&#34;#fnref1&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn2&#34;&gt;&lt;p&gt;For the purpose of this post, and if you don’t use GitHub Actions frequently, you should be good with the free plan (default unless you subscribed). In any case, you can read more about the plans &lt;a href=&#34;https://docs.github.com/en/free-pro-team@latest/github/setting-up-and-managing-billing-and-payments-on-github/about-billing-for-github-actions&#34;&gt;here&lt;/a&gt;.&lt;a href=&#34;#fnref2&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn3&#34;&gt;&lt;p&gt;The more dependencies you have, the more susceptible your code is to breaking, e.g. if a function was deprecated. If you’re concerned you can use less dependencies than I have or try &lt;a href=&#34;https://colinfay.me/docker-r-reproducibility/&#34;&gt;Docker&lt;/a&gt;.&lt;a href=&#34;#fnref3&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
      
            <category>TidyTuesday</category>
      
            <category>GitHub</category>
      
      
            <category>R</category>
      
            <category>GitHub</category>
      
    </item>
    
    <item>
      <title>My year in R</title>
      <link>https://amitlevinson.com/blog/my-year-in-r/</link>
      <pubDate>Thu, 15 Oct 2020 00:00:00 +0000</pubDate>
      
      <guid>https://amitlevinson.com/blog/my-year-in-r/</guid>
      <description>


&lt;style type=&#34;text/css&#34;&gt;
newcaption {
  font-size: 0.9em;
  text-align: center
}
&lt;/style&gt;
&lt;p align=&#34;center&#34;&gt;
&lt;img alt = &#39;A figure of an animated R Icon&#39; src = &#39;/img/year-in-r/r-img.png&#39;&gt;
&lt;newcaption&gt; Image by &lt;a href=&#34;https://allisonhorst.github.io/&#34;&gt;Allison Horst&lt;/a&gt;&lt;/newcaption&gt;
&lt;/p&gt;
&lt;p&gt;Learning R for a little over a year now was and still is a great experience. But a year isn’t a lot, so why make a blog post about it?&lt;/p&gt;
&lt;p&gt;I believe that pausing what one is doing and periodically evaluating if this pursuit is the right direction &lt;em&gt;for him or her&lt;/em&gt; - is a healthy process. Doing so can help you &lt;strong&gt;acknowledge your accomplishments and think of where you’re heading&lt;/strong&gt;. And I’m glad I have the opportunity to do it in the following post.&lt;/p&gt;
&lt;div id=&#34;the-journey&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;The Journey&lt;/h3&gt;
&lt;p&gt;I was wondering how to summarize my past year: A list of resources? a story? a listicle? I decided to go with more of an item-list somewhat chronologically ordered that I believe captures my experience. Of course like a lot of many other things in life, the timeline discussed isn’t completely rigid as some items I did concurrently or jumped back and forth.&lt;/p&gt;
&lt;div id=&#34;hearing-about-r&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;1. Hearing about R&lt;/h4&gt;
&lt;p&gt;I first heard of R when it was used in a hierarchical linear models workshop I attended. The workshop focused more on the statistics part of the analysis so we didn’t go in depth into the code. Subsequently I heard about R twice – Once from a friend studying Psychology, Yarden Ashur, and from my sister, Maayan Levinson, a statistician with the CBS. I’ll admit it took me some time to pick it up, but eventually I did.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;first-learning-steps&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;2. First learning steps&lt;/h4&gt;
&lt;p&gt;My friend from Psychology also told me there’s a recorded R course for psychology available on moodle (A platform for online course information) I could use freely. The course was led by &lt;a href=&#34;https://kesslerlab.wordpress.com/&#34;&gt;Yoav Kessler&lt;/a&gt; and proved to be a fantastic introduction. I followed along with the course and did the different assignments until we reached ggplot, a library for plotting in R.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;joining-the-tidytuesday-community&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;3. Joining the TidyTuesday community&lt;/h4&gt;
&lt;p&gt;By the time we reached ggplot in the psychology course I was already somewhat familiar with Twitter where the &lt;a href=&#34;https://github.com/rfordatascience/tidytuesday&#34;&gt;#Tidytuesday&lt;/a&gt; mostly takes place. &lt;a href=&#34;https://github.com/rfordatascience/tidytuesday&#34;&gt;#Tidytuesday&lt;/a&gt; is an amazing project where every week a new dataset is published for the #rstats community to analyze, visualize and post their results on Twitter. My excitement and motivation to participate were extremely high: So many professional and experienced R-users working on the same dataset, conjuring amazing visualizations and posting their code for others to explore (and all this for free)?! I was blown away. So I followed along on Twitter for a week or two until I said OK, let’s give it a try.&lt;/p&gt;
&lt;p&gt;It was &lt;a href=&#34;https://github.com/rfordatascience/tidytuesday/tree/master/data/2019/2019-09-17&#34;&gt;week 38&lt;/a&gt; in 2019 and we were working on visualizing national parks. I wasn’t really sure what to do, so I did a minimal exploration of the data and noticed an interesting increase of visitors in national parks across years, which seemed intuitive and perfect for a first visualization. Following the basic area-graph I made I remembered a visualization a week earlier from &lt;a href=&#34;https://twitter.com/ariamsita&#34;&gt;Ariane Aumaitre&lt;/a&gt; using roller-coaster icons in her graph. Knowing nothing on how to integrate icons, I adapted her code into my visualization to create a nice scenery for the mountain the data displayed (see tweet below). I was pretty satisfied at the time and the feedback from the #rstats community was incredible - I was hooked on the project.&lt;/p&gt;
&lt;center&gt;
&lt;blockquote class=&#34;twitter-tweet&#34;&gt;&lt;p lang=&#34;en&#34; dir=&#34;ltr&#34;&gt;My first attempt at plotting in &lt;a href=&#34;https://twitter.com/hashtag/rstats?src=hash&amp;amp;ref_src=twsrc%5Etfw&#34;&gt;#rstats&lt;/a&gt; 👏 &lt;br&gt;took the &lt;a href=&#34;https://twitter.com/hashtag/tidytuesday?src=hash&amp;amp;ref_src=twsrc%5Etfw&#34;&gt;#tidytuesday&lt;/a&gt; opportunity to get some practice and plot the average number of visitors at national parks per year.&lt;br&gt;icon inspiration from &lt;a href=&#34;https://twitter.com/ariamsita?ref_src=twsrc%5Etfw&#34;&gt;@ariamsita&lt;/a&gt;, thanks! &lt;a href=&#34;https://t.co/sLGUkRl80f&#34;&gt;pic.twitter.com/sLGUkRl80f&lt;/a&gt;&lt;/p&gt;&amp;mdash; Amit Levinson (@Amit_Levinson) &lt;a href=&#34;https://twitter.com/Amit_Levinson/status/1174364639427272706?ref_src=twsrc%5Etfw&#34;&gt;September 18, 2019&lt;/a&gt;&lt;/blockquote&gt;
&lt;script async src=&#34;https://platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;

&lt;newcaption&gt;First visualization and participation in TidyTuesday&lt;/newcaption&gt;
&lt;/center&gt;
&lt;p&gt;&lt;br&gt;
I believe the project was a fantastic introduction to continuously analyzing and visualizing data in R. &lt;strong&gt;Participating in the project provides a safe, motivating and rich setting to practice and learn R&lt;/strong&gt;. Additionally, I didn’t have anything that ‘forced’ me to learn R, so knowing that every week I had a new data set to analyze and visualize along with others provided me with a sense of &lt;strong&gt;routine and commitment&lt;/strong&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;opening-a-github-account&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;4. Opening a GitHub account&lt;/h4&gt;
&lt;p&gt;Following the first visualization for &lt;a href=&#34;https://github.com/rfordatascience/tidytuesday&#34;&gt;#Tidytuesday&lt;/a&gt; I wanted to share the code I wrote. At the time I was only using GitHub to read code written by others. Using the &lt;a href=&#34;https://happygitwithr.com/&#34;&gt;Happy git with r&lt;/a&gt; guide I was able to properly upload my code and synchronize future work. Since then, &lt;strong&gt;using GitHub taught me so much: Reading others’ code and discovering new functions; Organizing my own code so others can easily read it and thus ‘forcing me’ to clean it once I finished a project; and having a place to host all my efforts.&lt;/strong&gt; I sincerely believe that opening a GitHub account to share everything I did was an important and pivoting moment learning R. Although I still have so much more to learn when it comes to cleaning code and project management, a lot of what I know now is attributed to having GitHub repositories and code as accessible as possible for others to explore and learn.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;visualizing-things-i-was-interested-in&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;5. Visualizing things I was interested in&lt;/h4&gt;
&lt;p&gt;As I was participating in #Tidytuesday Eliud Kipchoge broke (unofficially) the two-hour marathon barrier. I found this amazing and wanted to visualize the comparison between the new record and older ones. I manually copied the marathon record values from Wikipedia and used that to plot running icons representing the different records. It wasn’t an aesthetic plot but it was definitely rewarding. I’ve since improved the visualization by making it reproducible and eventually wrote a &lt;a href=&#34;https://amitlevinson.com/post/eliud-kichoge/&#34;&gt;blog post&lt;/a&gt; explaining the process of how I made it. Similarly, a month or two later I &lt;a href=&#34;https://amitlevinson.com/post/bomb-shelters/&#34;&gt;plotted bomb shelter locations around my house&lt;/a&gt; amidst missiles fired towards Israel, all in R while using Google maps. &lt;strong&gt;I finally took an opportunity to make a visualization that related to my daily routine.&lt;/strong&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;continuous-learning&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;6. Continuous learning&lt;/h4&gt;
&lt;p&gt;&lt;img src=&#34;https://media.giphy.com/media/8dYmJ6Buo3lYY/giphy.gif&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Well, it’s kind of redundant to say this, as we’re always learning, but it is important: After I joined the #Tidytuesday community, I started again to actively learn about visualizations and data wrangling in addition to solidifying my basic knowledge of R. For this I relied on the following sources:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;R &amp;amp; R4DS book&lt;/strong&gt; - I decided to start with the &lt;a href=&#34;https://r4ds.had.co.nz/&#34;&gt;R for data science online book&lt;/a&gt;. Every morning I would spend 30-60 minutes either reading and learning something new or attempting to answer the book’s questions. &lt;strong&gt;I’d re-write the code into my R console while following along, exploring and trying to understand what was going on.&lt;/strong&gt; To validate my answers to the questions or understand those I didn’t know I cross-checked them with the &lt;a href=&#34;https://jrnold.github.io/r4ds-exercise-solutions/&#34;&gt;Excercise solutions book&lt;/a&gt; by Jeffrey Arnold. I also bought &lt;a href=&#34;https://socviz.co/&#34;&gt;Data Visualizaiton, A practical introduction&lt;/a&gt; by Kieran Healy and &lt;a href=&#34;https://www.tidytextmining.com/&#34;&gt;Text Mining with R, A tidy Approach&lt;/a&gt; by Julia Silge and David Robinson, but I used them more on the go and less of a sit-down.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Joining online courses&lt;/strong&gt; - While reading the R4DS book I decided to seek out some other courses at my university, mainly for recreational as I had no course credits left to take. The friend who introduced me to R told me about a workshop on ‘algorithms and research in the intersection of Psychology and big data’ by &lt;a href=&#34;https://t.co/3hLoUnYbPa?amp=1&#34;&gt;Michael Gilead&lt;/a&gt;. Getting an approval to sit in on some classes I found it a great introduction to working with big data. While joining in I met &lt;a href=&#34;https://almogsi.com/&#34;&gt;Almog Simchon&lt;/a&gt; who led a fantastic ‘text analysis in R’ workshop the following semester. I’d also join &lt;a href=&#34;https://sites.google.com/view/mattansb&#34;&gt;Mattan Ben-Shachar’s&lt;/a&gt; TA class and learn more about data wrangling and statistical methods. Additionally, a friend told me about &lt;a href=&#34;https://www.john-ros.com/&#34;&gt;Jonathan Rosenblatt’s&lt;/a&gt; R course in the department of industrial engineering and management (recorded lectures in Hebrew are freely available). &lt;strong&gt;Although I didn’t understand some of what was going on in these courses, I was glad to expose myself to new things I could follow up on later.&lt;/strong&gt; If any of you are reading this, thank you very much for the opportunity.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Other visualization books&lt;/strong&gt; - I found myself leaning towards books that are not related to the R ecosystem, but that I discovered during my R journey. That is, I found a strong liking towards visualizing data and bought books on that topic too. &lt;strong&gt;These books immensely improved my visualizations and how I look at visualizing data&lt;/strong&gt;. I still have a lot more to learn - both theoretically and technically - but these books definitely inspired and opened my mind when it comes to visualizations. I highly recommend reading &lt;a href=&#34;http://www.storytellingwithdata.com/books&#34;&gt;Storytelling with data&lt;/a&gt; by Cole Nussbaumer Knaflic and &lt;a href=&#34;http://albertocairo.com/&#34;&gt;How Charts Lie&lt;/a&gt; by Alberto Cairo which I started with.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Watch live webinars &amp;amp; videos; attended meetups&lt;/strong&gt; - An invaluable source of learning was participating in various online meetups and webinars. A good place I found many of them was on Twitter, but I’m sure you can also find them in Facebook groups, Rstudio news letter, etc. I Sometimes didn’t understand what they were talking about but just exposing myself to it felt great (seems like a reoccurring theme). It motivated me to want to learn more in order to succeed in doing what was presented. I also highly recommend exploring the &lt;a href=&#34;https://rstudio.com/resources/webinars/&#34;&gt;Rstudio Videos&lt;/a&gt; page.
&lt;br&gt;
Luckily, I started learning R before the COVID-19 prevailed so I was able to join 2 Israeli R-meetups. This was a great experience and although being a novice when I attended them, the community was great, people were welcoming, there was great pizza and beer and the presentations were fantastic. It was a great source of inspiration of what was to come. &lt;strong&gt;Plus, seeing so many people enthusiastically talking about R made me understand that I’m not alone in liking this world.&lt;/strong&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;making-my-own-website&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;7. Making my own website&lt;/h4&gt;
&lt;p&gt;A month or two into learning R I noticed people had their own websites they made in R. Again I was fascinated at how this was possible - Not only can I wrangle data and beautifully visualize it but I can also build my own website? and for 10$ I can use my own domain? This was crazy!&lt;/p&gt;
&lt;iframe height=&#34;500&#34; width=&#34;95%&#34; title=&#34;Intentionally blank&#34; src=&#34;https://amitlevinson.com/&#34;&gt;
&lt;/iframe&gt;
&lt;center&gt;
&lt;newcaption&gt; Opening my own website - A motivation to learn and write&lt;/newcaption&gt;
&lt;/center&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Nearing mid January (4+- months into R) I decided it’s time open my own website.&lt;/strong&gt; I had a few things I already made (&lt;a href=&#34;https://amitlevinson.com/post/eliud-kichoge/&#34;&gt;Eliud Kipchoge’s record&lt;/a&gt; and the &lt;a href=&#34;https://amitlevinson.com/post/bomb-shelters/&#34;&gt;bomb shelters around my house&lt;/a&gt;) and also wanted a place for others to learn more about me. I scrolled and followed along the &lt;a href=&#34;https://bookdown.org/yihui/blogdown/&#34;&gt;blogdown book&lt;/a&gt; for creating websites with R, viewed some of &lt;a href=&#34;https://alison.rbind.io/project/up-running-blogdown/&#34;&gt;Allison Hill’s blogdown workshops&lt;/a&gt; and other resources. Eventually, I was setup and had my website live, done in R, hosted for free on Netlify and GitHub with an elegant Hugo Academic theme, and my own domain for only 10$! &lt;strong&gt;I was amazed at how easy and rewarding this was.&lt;/strong&gt; I mean, I had no knowledge (and still don’t) of HTML, CSS or anything else to build a website and here I conjured one, and pretty easily!&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;I highly recommend creating a website.&lt;/strong&gt; Even if you’re not an R user, I think a personal website is a great motivator for writing blogs; a platform for others to learn more about you and a not so difficult thing to do today. &lt;strong&gt;Opening a website has definitely motivated me to learn much more by writing about it&lt;/strong&gt; (here’s a great talk by David Robinson on &lt;a href=&#34;https://rstudio.com/resources/rstudioconf-2019/the-unreasonable-effectiveness-of-public-work/&#34;&gt;The unreasonable effectiveness of public work&lt;/a&gt;).&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;giving-a-talk-about-r&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;8. Giving a talk about R&lt;/h4&gt;
&lt;p&gt;During Passover (April) 2020, the &lt;a href=&#34;https://israel2050.co.il/en/home/&#34;&gt;Israel-2050&lt;/a&gt; fellows group sent out a call inviting individuals to talk about anything they wanted. I decided to take the opportunity and give a talk there, and following that to a group of friends of mine that meet periodically with someone presenting something. Although being only ~7 months into learning R, I wanted to share its amazing abilities for wrangling and visualizing data, the extreme difference of using it compared to SPSS I learned and how it helped me explore intriguing questions I had. So I sat down, wrote an outline, and made a presentation using the &lt;a href=&#34;https://github.com/yihui/xaringan&#34;&gt;{Xaringan}&lt;/a&gt; package. You can find the &lt;a href=&#34;&#34;&gt;slides here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;The talk was great (I think) and some of the participants even followed up inquiring about resources to get started, how can they do this in R, etc. However, more importantly, &lt;strong&gt;making and giving the talk forced me to think about what is it in R that I like.&lt;/strong&gt; Organizing these thoughts and communicating them in a way that is appealing to the audience was a fantastic opportunity to stop and think about exactly that: Why do I like working in R and why should they join it.&lt;/p&gt;
&lt;center&gt;
&lt;iframe height=&#34;500&#34; width=&#34;100%&#34; src=&#34;https://amitlevinson.github.io/slides-israel-2050/index.html#1&#34;&gt;
&lt;/iframe&gt;
&lt;newcaption&gt;
&lt;a href=&#34;https://amitlevinson.github.io/slides-israel-2050/index.html#1&#34;&gt;My first talk about R&lt;/a&gt; (use your keyboard arrow to scroll through it).&lt;/newcaption&gt;
&lt;/center&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;integrating-r-into-my-daily-work&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;9. Integrating R into my daily work&lt;/h4&gt;
&lt;p&gt;&lt;strong&gt;Using R as a research assistant&lt;/strong&gt; - I was very fortunate that the researcher I work for, &lt;a href=&#34;https://www.jenniferoser.com/&#34;&gt;Dr. Jennifer Oser&lt;/a&gt;, was (and still is) very supportive of integrating R into our daily work. I remember as we started analyzing our data and trying to make sense of it I was debating whether to open SPSS, Excel or R. Luckily, I knew how to do some of what we wanted to in R so I turned to use that. I believe we’ve greatly progressed since, so much that I find it absurd to use something else now. &lt;strong&gt;If you can integrate R into your daily work it’s definitely a bonus, I know I learned a lot (I mean a lot) about rmarkdown and version control once I started using R in my research assistant position.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Integrating R into my thesis&lt;/strong&gt; - The reason I initially started learning R was so that I could analyze my thesis’ findings and finish my MA with a new skill. No one forced me to use R, and I’m sure I could have done OK with SPSS (or maybe not?), but I was keen on using R in my thesis; it was an exciting and challenging experience. &lt;strong&gt;Prior to my thesis I’ve mostly done visualizations and descriptive reports so it was great working on regression models, reliability and other forms of reports.&lt;/strong&gt; I also learned more about version control, using the same functions I wrote for the pilot study and my main analysis and so forth. I couldn’t imagine producing SPSS tables and integrating them every time in a separate text document; plus, it was very rewarding trying to automate the process as much as possible.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;blog-and-then-blog-some-more&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;10. Blog, and then blog some more&lt;/h4&gt;
&lt;p&gt;I imagine you’ve heard this saying a lot, but I definitely agree with it: &lt;strong&gt;If you like it then you should &lt;s&gt;put a ring on it&lt;/s&gt; write about it. Don’t write for others, write for yourself.&lt;/strong&gt; While I mentioned earlier that I wrote about my visualizations, I also wanted to learn about the statistical analyses I came across. &lt;strong&gt;I would read about a topic and think of an example I can easily use to explain the concept. For myself, not for others.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;For example, to learn what was Term frequency inverse document frequency (tf-idf) I implemented it in &lt;a href=&#34;https://amitlevinson.com/post/learning-tfidf-with-political-theorists/&#34;&gt;analyzing the tfidf of 4 books by political theorists’&lt;/a&gt; I like. At one point I wanted to learn more about Bernoulli trials so I explored the &lt;a href=&#34;https://amitlevinson.com/post/uncertainty-in-the-israeli-lottery/&#34;&gt;uncertainty in the Israeli lottery&lt;/a&gt;. &lt;strong&gt;Alternatively, write about a challenge you faced and how you solved it.&lt;/strong&gt;In another example, I wrote about presenting a static summary of categorical variables from my thesis pilot survey (&lt;a href=&#34;https://amitlevinson.com/post/printing-survey-table/&#34;&gt;found here&lt;/a&gt;).&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Don’t write for others to click on your website; rather, write for you to learn or communicate something you want to share with your future self and the world, no matter who reads it.&lt;/strong&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;summary&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Summary&lt;/h3&gt;
&lt;p&gt;So this was a not-so-short recap of my last year, which I hope was of value. A lot of the above is owed to the amazing R community - Any and every one who blogs, shares his code, interacts about R on social media and was forthcoming. &lt;strong&gt;I’m very grateful to the many people I’ve reached out to with random questions, wanting to join their course or inquire about further reading.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://media.giphy.com/media/YRuFixSNWFVcXaxpmX/giphy.gif&#34; /&gt;&lt;/p&gt;
&lt;p&gt;It’s interesting to think back about something you’ve done and if and how would you have done it differently. As to the latter, I’m not sure, and I’m kind of glad that it happened the way it did.&lt;/p&gt;
&lt;p&gt;I think my main takeaways are:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;Learn a bit from a course, website, blog, book, etc. Don’t get too caught on in my opinion, rather try and implement it on examples that could be similar to those in a book, but aren’t given on a silver platter (for example search for your own dataset from &lt;a href=&#34;https://www.kaggle.com/&#34;&gt;Kaggle&lt;/a&gt; and the like).&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Find a community and project that will keep you hooked. If you’re interested in R, then join the &lt;a href=&#34;https://github.com/rfordatascience/tidytuesday&#34;&gt;#Tidytuesday&lt;/a&gt; project!&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Share what you learn – whether by uploading your code to GitHub, opening your own website or giving a talk. One of the main motivators I had to overcome of writing after using R for only 4 months ( an “imposter syndrome”) was discovering that a Youtube video on &lt;a href=&#34;https://www.youtube.com/watch?v=r9hpiyzOOTY&#34;&gt;how to unzip a file&lt;/a&gt; had 650,000 views. That is, there’s an audience for everything. Don’t think “I’m not good enough to write about it”. One of the best ways I learned something was understanding it in a way that I could then communicate it to others.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Have fun! The more – The better!&lt;/strong&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;If you’re looking for a place to start, &lt;a href=&#34;https://oscarbaruffa.com/&#34;&gt;Oscar Baruffa&lt;/a&gt; compiled a fantastic resouce aggregating &lt;a href=&#34;https://www.bigbookofr.com/&#34;&gt;~100 books about R&lt;/a&gt; (most are free).&lt;/p&gt;
&lt;div id=&#34;whats-next-for-me&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;What’s next for me&lt;/h4&gt;
&lt;p&gt;Great question! Honestly I don’t know. I hope to finish my thesis soon and search for a job that’ll require me to work with R and visualize data. In addition, I’ll probably also try and learn some Tableau and improve my SQL skills as they are somewhat sought after in various jobs I looked at. As to R, I hope to learn some new concepts and statistical analyses; incorporate more #Tidytuesdays into my weekly routine; and analyze some data I have waiting around for a blog post. Of course everything is flexible, and in that case I really don’t know what’s waiting but I’m definitely excited about it!&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
      
      
            <category>R</category>
      
    </item>
    
    <item>
      <title>Survey categorical variables with KableExtra</title>
      <link>https://amitlevinson.com/blog/printing-survey-table/</link>
      <pubDate>Tue, 01 Sep 2020 00:00:00 +0000</pubDate>
      
      <guid>https://amitlevinson.com/blog/printing-survey-table/</guid>
      <description>
&lt;link href=&#34;index_files/anchor-sections/anchor-sections.css&#34; rel=&#34;stylesheet&#34; /&gt;
&lt;script src=&#34;index_files/anchor-sections/anchor-sections.js&#34;&gt;&lt;/script&gt;
&lt;script src=&#34;index_files/kePrint/kePrint.js&#34;&gt;&lt;/script&gt;
&lt;link href=&#34;index_files/lightable/lightable.css&#34; rel=&#34;stylesheet&#34; /&gt;


&lt;style&gt;
p.caption {
  font-size: 0.8em;
}
&lt;/style&gt;
&lt;p&gt;In my in-progress thesis I decided I’ll analyze my survey results in something other than SPSS we learned in undergrad, which eventually led me to begin using R. The time came and I started analyzing my pilot survey data from Qualtrics&lt;a href=&#34;#fn1&#34; class=&#34;footnote-ref&#34; id=&#34;fnref1&#34;&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt;. In this post I’ll address how I used &lt;code&gt;{KableExtra}&lt;/code&gt; to nicely print a frequency table of the categorical &amp;amp; ordinal questions I had in my survey. You can also do what I describe below in other packages, however I enjoy using &lt;code&gt;{KableExtra}&lt;/code&gt; for its rich vignette and clearly defomed functions.&lt;/p&gt;
&lt;div id=&#34;the-problem&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;The Problem&lt;/h3&gt;
&lt;p&gt;In my pilot survey I had ~20 questions that were categorical, ordinal and were simple constructs not requiring a thorough analysis but only a quick review at the distribution of responses. I wanted to print all of these variables in one formatted table and address any anomalies if needed.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;The issue was, how can I format printing of all categorical variables in their chronological order, along with the original question and the distribution of responses?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Basically, how can I achieve the following output:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;table.png&#34; width=&#34;80%&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;explore-our-data&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Explore our data&lt;/h3&gt;
&lt;p&gt;First, let’s load the packages we’ll need and look at our data:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyverse)
library(here)
library(readxl)
library(knitr)
library(kableExtra)
library(janitor)
library(scales)
df_survey &amp;lt;- read_xlsx(here(&amp;quot;content&amp;quot;, &amp;quot;post&amp;quot;, &amp;quot;printing-survey-table&amp;quot;, &amp;quot;data&amp;quot;,&amp;quot;survey.xlsx&amp;quot;))
head(df_survey)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 6 x 12
##   Q2     Q4     Q6     Q7    Q11_1   Q13_1   Q15   Q26   Q27   Q30   Q32   Q37  
##   &amp;lt;chr&amp;gt;  &amp;lt;chr&amp;gt;  &amp;lt;chr&amp;gt;  &amp;lt;chr&amp;gt; &amp;lt;chr&amp;gt;   &amp;lt;chr&amp;gt;   &amp;lt;chr&amp;gt; &amp;lt;chr&amp;gt; &amp;lt;chr&amp;gt; &amp;lt;chr&amp;gt; &amp;lt;chr&amp;gt; &amp;lt;chr&amp;gt;
## 1 Female Salar~ Simil~ Cent~ Once a~ At lea~ Never No    &amp;lt;NA&amp;gt;  &amp;lt;NA&amp;gt;  No    Yes  
## 2 Female Salar~ Below~ South Severa~ Every ~ Never No    &amp;lt;NA&amp;gt;  &amp;lt;NA&amp;gt;  No    Yes  
## 3 Female Stude~ Below~ South Betwee~ I&amp;#39;ve n~ Never No    &amp;lt;NA&amp;gt;  &amp;lt;NA&amp;gt;  No    Yes  
## 4 Female Unemp~ Simil~ Cent~ Once a~ At lea~ Last~ No    &amp;lt;NA&amp;gt;  &amp;lt;NA&amp;gt;  No    Yes  
## 5 Male   Salar~ Simil~ Cent~ Once a~ At lea~ Last~ Yes   2-3 ~ Yes   No    No   
## 6 Female Salar~ Simil~ Cent~ Severa~ Last t~ Never No    &amp;lt;NA&amp;gt;  &amp;lt;NA&amp;gt;  No    Yes&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;So we have a lot of information describing our sample data records. While this data is fabricated, it mirrors a common survey dataset: Each row represents a respondent with answers to various questions. With respect to continuous variables I did a different analysis, so for the purpose of the following post we’ll need only character columns. Let’s start by removing anything other than the relevant columns:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;df_char &amp;lt;- df_survey %&amp;gt;% 
  janitor::clean_names () %&amp;gt;% 
  select_if(is.character) &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We’ll need to change the data to a long form so that we can print it for efficient reading. An easy approach will be to use the &lt;code&gt;pivot_longer&lt;/code&gt; argument, rendering all our columns in one long table:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;df_long &amp;lt;- df_char %&amp;gt;% 
  pivot_longer(q2:q37, names_to = &amp;quot;question&amp;quot;) %&amp;gt;% 
  count(question, value) %&amp;gt;%
  group_by(question) %&amp;gt;% 
  mutate(pct = percent(n/sum(n)))
# Print table
kbl(df_long) %&amp;gt;% 
  kable_styling() %&amp;gt;% 
  scroll_box(height = &amp;quot;550px&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;div style=&#34;border: 1px solid #ddd; padding: 0px; overflow-y: scroll; height:550px; &#34;&gt;
&lt;table class=&#34;table&#34; style=&#34;margin-left: auto; margin-right: auto;&#34;&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:left;position: sticky; top:0; background-color: #FFFFFF;&#34;&gt;
question
&lt;/th&gt;
&lt;th style=&#34;text-align:left;position: sticky; top:0; background-color: #FFFFFF;&#34;&gt;
value
&lt;/th&gt;
&lt;th style=&#34;text-align:right;position: sticky; top:0; background-color: #FFFFFF;&#34;&gt;
n
&lt;/th&gt;
&lt;th style=&#34;text-align:left;position: sticky; top:0; background-color: #FFFFFF;&#34;&gt;
pct
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
q11_1
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Between once a week to once in a month
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
5
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
14.7%
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
q11_1
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Less than once a month
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
4
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
11.8%
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
q11_1
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Once a day
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
6
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
17.6%
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
q11_1
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Once a week
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
4
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
11.8%
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
q11_1
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Several times a day
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
6
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
17.6%
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
q11_1
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Several times a week
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
8
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
23.5%
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
q11_1
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
NA
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
2.9%
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
q13_1
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
About every week
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
2.9%
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
q13_1
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
At least once a month
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
13
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
38.2%
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
q13_1
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Every three months
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
5
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
14.7%
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
q13_1
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Every three to six months
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
4
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
11.8%
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
q13_1
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
I’ve never bought
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
5
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
14.7%
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
q13_1
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Last time I bought was over half a year ago
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
6
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
17.6%
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
q15
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Last half a year
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
3
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
8.8%
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
q15
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Last month
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
5.9%
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
q15
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Last three months
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
3
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
8.8%
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
q15
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Last week
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
2.9%
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
q15
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Last year
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
2.9%
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
q15
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Never
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
18
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
52.9%
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
q15
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Over a year ago
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
6
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
17.6%
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
q2
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Female
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
15
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
44.1%
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
q2
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Male
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
17
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
50.0%
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
q2
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Other
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
5.9%
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
q26
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
No
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
20
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
59%
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
q26
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Yes
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
13
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
38%
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
q26
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
NA
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
3%
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
q27
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
2-3 times
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
4
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
11.8%
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
q27
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
5-7 times
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
4
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
11.8%
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
q27
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Over 7 times
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
5
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
14.7%
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
q27
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
NA
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
21
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
61.8%
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
q30
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
No
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
3%
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
q30
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Yes
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
12
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
35%
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
q30
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
NA
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
21
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
62%
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
q32
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
No
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
27
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
79.4%
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
q32
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Yes
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
5
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
14.7%
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
q32
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
NA
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
5.9%
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
q37
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
No
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
6
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
17.6%
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
q37
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Yes
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
25
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
73.5%
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
q37
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
NA
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
3
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
8.8%
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
q4
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Salaried employee
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
18
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
52.9%
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
q4
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Self employed
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
3
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
8.8%
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
q4
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Student
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
8
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
23.5%
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
q4
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Student,Salaried employee
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
3
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
8.8%
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
q4
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Student,Unemployed
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
2.9%
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
q4
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Unemployed
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
2.9%
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
q6
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Above average
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
5
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
14.7%
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
q6
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Below average
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
16
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
47.1%
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
q6
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Similar to average
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
13
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
38.2%
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
q7
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Center
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
16
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
47.1%
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
q7
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
South
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
18
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
52.9%
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Great, this prints nicely, but we’re left with several issues to address&lt;a href=&#34;#fn2&#34; class=&#34;footnote-ref&#34; id=&#34;fnref2&#34;&gt;&lt;sup&gt;2&lt;/sup&gt;&lt;/a&gt;:&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;1. We’re missing the actual questions&lt;/strong&gt; - Notice how we only have “q11_1” but not a description of what the actual question or what the variable is. You can add a question label within Qualtrics, but I still wanted to have the question itself presented along with the question number.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;2. Questions were reorganized alphabetically&lt;/strong&gt; - Once we ran the &lt;code&gt;pivot_longer&lt;/code&gt; R sorted our dataframe alphabetically according to the question column, but we might want it ordered according to the survey layout. Of course this is contingent on your data; I wanted to present it aligned to the order of the survey questions.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;3. Some of the responses aren’t ordered&lt;/strong&gt; - Notice how some of the responses are randomly ordered, when ideally we’d want them to be ordered by hierarchy. For example question q11_1 describes frequency responses that aren’t hierarchically ordered.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;4. Remove duplicated information&lt;/strong&gt; - Our question column, and if we add another one with the question’s text, will have duplicate information. While the value changes within questions printing the question column for each row is redundant. In addition, once we’ll add the question title it’ll be even more cluttered and any additional irrelevant text should be removed.&lt;/p&gt;
&lt;p&gt;So then, let’s address these issues individually.&lt;/p&gt;
&lt;div id=&#34;adding-information-to-our-questions&#34; class=&#34;section level5&#34;&gt;
&lt;h5&gt;Adding information to our questions&lt;/h5&gt;
&lt;p&gt;When you download the survey data from Qualtrics you also receive it with the original questions. When I personally analyzed the data I removed it, but here it’s perfect for our display of additional information. There’s also a great function for doing exactly that from the &lt;a href=&#34;https://cran.r-project.org/web/packages/qualtRics/vignettes/qualtRics.html&#34;&gt;&lt;code&gt;{qualtRics}&lt;/code&gt;&lt;/a&gt; package, but I was having trouble connecting to the platform’s API through my Qualtrics user.&lt;/p&gt;
&lt;p&gt;Adding the questions was straightforward: Just combine the current &lt;em&gt;data_long&lt;/em&gt; with a dataset containing my questions. We’ll use a copy of the original survey data (of course fabricated for purpose of the survey) that only contains the questions:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;df_questions &amp;lt;- read_xlsx(here(&amp;quot;content&amp;quot;, &amp;quot;post&amp;quot;, &amp;quot;printing-survey-table&amp;quot;, &amp;quot;data&amp;quot;, &amp;quot;questions.xlsx&amp;quot;))
df_questions[,1:3]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 1 x 3
##   Q2     Q4                   Q6                                                
##   &amp;lt;chr&amp;gt;  &amp;lt;chr&amp;gt;                &amp;lt;chr&amp;gt;                                             
## 1 Gender What&amp;#39;s your occupat~ The average income for an individual is X, you&amp;#39;re~&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Great! we see our question’s text as values with the variables being the questions themselves. Now let’s render it in a long format so that each row is a question id with the corresponding text as a value, and then we’ll join it with our current dataset of answers:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;df_q_clean &amp;lt;- df_questions %&amp;gt;% 
  clean_names() %&amp;gt;% 
  pivot_longer(cols = q2:q37, names_to = &amp;quot;question&amp;quot;, values_to = &amp;quot;text&amp;quot;) 
df_long_joined &amp;lt;-  left_join(df_long, df_q_clean)
head(df_long_joined)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 6 x 5
## # Groups:   question [1]
##   question value                           n pct   text                         
##   &amp;lt;chr&amp;gt;    &amp;lt;chr&amp;gt;                       &amp;lt;int&amp;gt; &amp;lt;chr&amp;gt; &amp;lt;chr&amp;gt;                        
## 1 q11_1    Between once a week to onc~     5 14.7% Every how often do you consu~
## 2 q11_1    Less than once a month          4 11.8% Every how often do you consu~
## 3 q11_1    Once a day                      6 17.6% Every how often do you consu~
## 4 q11_1    Once a week                     4 11.8% Every how often do you consu~
## 5 q11_1    Several times a day             6 17.6% Every how often do you consu~
## 6 q11_1    Several times a week            8 23.5% Every how often do you consu~&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Perfect. However, as you can see, our new text column provides the same information across the same questions, which seems kind of redundant. We’ll keep it for now and address it soon when we turn to print our table.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;reordering-within-and-across-questions&#34; class=&#34;section level5&#34;&gt;
&lt;h5&gt;Reordering within and across questions&lt;/h5&gt;
&lt;p&gt;The next issue on the list is that we want some of our questions to be organized not by the count frequency or some randomness, but by hierarchy of the answer options. For example ‘a few times a day’, ‘Once a day’, ‘several times a week’ and so on as a hierarachal structure in my ordinal variables.&lt;/p&gt;
&lt;p&gt;Alas, I don’t have a magical automated method and would be grateful to hear about other options you encountered or thought of. I thought of using factors to reorder the levels, but once I pivot my data into a long format the answers are again sorted alphabetically. Instead I decided to manually combine my current dataframe with an identical one I saved where I ranked each relevant ordinal question manually. Though a tedious task, this manual workload is more efficient than automating everything.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Save the sorted response file and use that to rank
# write_csv(df_long_q_sorted, here(&amp;quot;content&amp;quot;, &amp;quot;post&amp;quot;, &amp;quot;printing-survey-table&amp;quot;, &amp;quot;data&amp;quot;, &amp;quot;answers_hir.csv&amp;quot;))
answer_hir &amp;lt;- read_csv(here(&amp;quot;content&amp;quot;, &amp;quot;post&amp;quot;, &amp;quot;printing-survey-table&amp;quot;, &amp;quot;data&amp;quot;, &amp;quot;answers_hir.csv&amp;quot;))
head(answer_hir)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 6 x 3
##   question value                      rank
##   &amp;lt;chr&amp;gt;    &amp;lt;chr&amp;gt;                     &amp;lt;dbl&amp;gt;
## 1 q2       Female                       NA
## 2 q2       Male                         NA
## 3 q4       Salaried employee            NA
## 4 q4       Self employed                NA
## 5 q4       Student                      NA
## 6 q4       Student,Salaried employee    NA&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We now have our new guide in which we ranked our questions. Notice that the first answers are NA, but that’s because the nominal variables have no intrinsic hierarchy. Now let’s use this dataframe to create a value with which to sort our answers:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;df_long_ranked &amp;lt;- left_join(x = df_long_joined, y = answer_hir) 
head(df_long_ranked)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 6 x 6
## # Groups:   question [1]
##   question value                         n pct   text                       rank
##   &amp;lt;chr&amp;gt;    &amp;lt;chr&amp;gt;                     &amp;lt;int&amp;gt; &amp;lt;chr&amp;gt; &amp;lt;chr&amp;gt;                     &amp;lt;dbl&amp;gt;
## 1 q11_1    Between once a week to o~     5 14.7% Every how often do you c~     5
## 2 q11_1    Less than once a month        4 11.8% Every how often do you c~     6
## 3 q11_1    Once a day                    6 17.6% Every how often do you c~     2
## 4 q11_1    Once a week                   4 11.8% Every how often do you c~     4
## 5 q11_1    Several times a day           6 17.6% Every how often do you c~     1
## 6 q11_1    Several times a week          8 23.5% Every how often do you c~     3&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We’ll reorder by rank along with solving the next issue which is the way our questions are ordered. Basically, we want it to be ordered by the question value and not using an alphabetic sort. For example, we’d like q_5 to appear before q11_1, similar to how it appeared in the survey. I’ll apply some regex (regular expression) manipulation to capture only the numbers and use that to sort by.
Displayed as follows:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;df_long_q_sorted &amp;lt;- df_long_ranked %&amp;gt;% 
  mutate(q_num = str_remove_all(question, &amp;quot;[a-z]&amp;quot;),
    q_num = str_replace_all(q_num, &amp;quot;_&amp;quot;, &amp;quot;.&amp;quot;),
    q_num = str_remove(q_num, &amp;quot;\\.$&amp;quot;),
    q_num = as.numeric(q_num)) %&amp;gt;% 
  group_by(question) %&amp;gt;% 
  arrange(q_num, rank) %&amp;gt;% 
  ungroup() %&amp;gt;% 
  select(-c(q_num, rank)) %&amp;gt;% 
  relocate(text, .after = question)
head(df_long_q_sorted)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 6 x 5
##   question text                    value                 n pct  
##   &amp;lt;chr&amp;gt;    &amp;lt;chr&amp;gt;                   &amp;lt;chr&amp;gt;             &amp;lt;int&amp;gt; &amp;lt;chr&amp;gt;
## 1 q2       Gender                  Female               15 44.1%
## 2 q2       Gender                  Male                 17 50.0%
## 3 q2       Gender                  Other                 2 5.9% 
## 4 q4       What&amp;#39;s your occupation? Salaried employee    18 52.9%
## 5 q4       What&amp;#39;s your occupation? Self employed         3 8.8% 
## 6 q4       What&amp;#39;s your occupation? Student               8 23.5%&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Fantastic!&lt;/p&gt;
&lt;p&gt;I found that first using the regex and then sorting by rank doesn’t properly work, so instead I implemented it along with sorting the questions. Again, if the order of questions and answers doesn’t matter in your data you can just skip past some of the stages.&lt;/p&gt;
&lt;p&gt;Great, now that we have all our data formatted properly, we can turn to the printing!&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;kableextra&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;KableExtra&lt;/h4&gt;
&lt;p&gt;In my initial round of exploring the pilot survey I used &lt;code&gt;{KableExtra}&lt;/code&gt; and its powerful features. You might find other packages better to work with when knitting to Word. With that said, it’s possible (and very effective) to knit to Html and copy that into a Word document. Despite the copy + paste requirement, I found it to be the better approach for keeping all the aesthetics and formatting integrated in the original document. Oh, and I also had my questions originally in Hebrew which was easier to knit to Html altogether.&lt;/p&gt;
&lt;div id=&#34;removing-redundant-information&#34; class=&#34;section level5&#34;&gt;
&lt;h5&gt;removing redundant information&lt;/h5&gt;
&lt;p&gt;As we saw earlier, the argument is pretty straight forward. We can address the redundant information we have - question and text column appearing with each answer (our final issue) - within the KableExtra object using &lt;code&gt;collapse_rows&lt;/code&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;df_long_q_sorted %&amp;gt;% 
  kbl(col.names = c(&amp;quot;Question&amp;quot;, &amp;quot;Text&amp;quot;, &amp;quot;Answer&amp;quot;, &amp;quot;n&amp;quot;, &amp;quot;%&amp;quot;)) %&amp;gt;% 
  kable_styling(full_width = F) %&amp;gt;% 
  column_spec(1, bold = T) %&amp;gt;% 
  collapse_rows(columns = c(1,2), valign = &amp;quot;top&amp;quot;) %&amp;gt;% 
  scroll_box(height = &amp;quot;750px&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;div style=&#34;border: 1px solid #ddd; padding: 0px; overflow-y: scroll; height:750px; &#34;&gt;
&lt;table class=&#34;table&#34; style=&#34;width: auto !important; margin-left: auto; margin-right: auto;&#34;&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:left;position: sticky; top:0; background-color: #FFFFFF;&#34;&gt;
Question
&lt;/th&gt;
&lt;th style=&#34;text-align:left;position: sticky; top:0; background-color: #FFFFFF;&#34;&gt;
Text
&lt;/th&gt;
&lt;th style=&#34;text-align:left;position: sticky; top:0; background-color: #FFFFFF;&#34;&gt;
Answer
&lt;/th&gt;
&lt;th style=&#34;text-align:right;position: sticky; top:0; background-color: #FFFFFF;&#34;&gt;
n
&lt;/th&gt;
&lt;th style=&#34;text-align:left;position: sticky; top:0; background-color: #FFFFFF;&#34;&gt;
%
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;font-weight: bold;vertical-align: top !important;&#34; rowspan=&#34;3&#34;&gt;
q2
&lt;/td&gt;
&lt;td style=&#34;text-align:left;vertical-align: top !important;&#34; rowspan=&#34;3&#34;&gt;
Gender
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Female
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
15
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
44.1%
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Male
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
17
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
50.0%
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Other
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
5.9%
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;font-weight: bold;vertical-align: top !important;&#34; rowspan=&#34;6&#34;&gt;
q4
&lt;/td&gt;
&lt;td style=&#34;text-align:left;vertical-align: top !important;&#34; rowspan=&#34;6&#34;&gt;
What’s your occupation?
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Salaried employee
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
18
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
52.9%
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Self employed
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
3
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
8.8%
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Student
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
8
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
23.5%
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Student,Salaried employee
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
3
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
8.8%
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Student,Unemployed
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
2.9%
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Unemployed
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
2.9%
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;font-weight: bold;vertical-align: top !important;&#34; rowspan=&#34;3&#34;&gt;
q6
&lt;/td&gt;
&lt;td style=&#34;text-align:left;vertical-align: top !important;&#34; rowspan=&#34;3&#34;&gt;
The average income for an individual is X, you’re income is:
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Below average
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
16
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
47.1%
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Similar to average
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
13
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
38.2%
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Above average
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
5
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
14.7%
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;font-weight: bold;vertical-align: top !important;&#34; rowspan=&#34;2&#34;&gt;
q7
&lt;/td&gt;
&lt;td style=&#34;text-align:left;vertical-align: top !important;&#34; rowspan=&#34;2&#34;&gt;
Where do you live in Israel
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Center
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
16
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
47.1%
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
South
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
18
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
52.9%
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;font-weight: bold;vertical-align: top !important;&#34; rowspan=&#34;7&#34;&gt;
q11_1
&lt;/td&gt;
&lt;td style=&#34;text-align:left;vertical-align: top !important;&#34; rowspan=&#34;7&#34;&gt;
Every how often do you consume chocolate?
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Several times a day
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
6
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
17.6%
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Once a day
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
6
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
17.6%
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Several times a week
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
8
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
23.5%
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Once a week
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
4
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
11.8%
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Between once a week to once in a month
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
5
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
14.7%
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Less than once a month
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
4
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
11.8%
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
NA
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
2.9%
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;font-weight: bold;vertical-align: top !important;&#34; rowspan=&#34;6&#34;&gt;
q13_1
&lt;/td&gt;
&lt;td style=&#34;text-align:left;vertical-align: top !important;&#34; rowspan=&#34;6&#34;&gt;
Every how often do you buy chocolate?
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
About every week
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
2.9%
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
At least once a month
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
13
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
38.2%
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Every three months
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
5
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
14.7%
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Every three to six months
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
4
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
11.8%
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Last time I bought was over half a year ago
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
6
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
17.6%
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
I’ve never bought
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
5
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
14.7%
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;font-weight: bold;vertical-align: top !important;&#34; rowspan=&#34;7&#34;&gt;
q15
&lt;/td&gt;
&lt;td style=&#34;text-align:left;vertical-align: top !important;&#34; rowspan=&#34;7&#34;&gt;
When did you last attend a party?
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Last week
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
2.9%
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Last month
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
5.9%
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Last three months
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
3
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
8.8%
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Last half a year
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
3
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
8.8%
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Last year
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
2.9%
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Over a year ago
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
6
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
17.6%
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Never
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
18
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
52.9%
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;font-weight: bold;vertical-align: top !important;&#34; rowspan=&#34;3&#34;&gt;
q26
&lt;/td&gt;
&lt;td style=&#34;text-align:left;vertical-align: top !important;&#34; rowspan=&#34;3&#34;&gt;
Do you think 2020 was a good year?
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
No
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
20
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
59%
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Yes
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
13
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
38%
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
NA
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
3%
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;font-weight: bold;vertical-align: top !important;&#34; rowspan=&#34;4&#34;&gt;
q27
&lt;/td&gt;
&lt;td style=&#34;text-align:left;vertical-align: top !important;&#34; rowspan=&#34;4&#34;&gt;
How many times did you decide to stay home instead of going out this year?
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
2-3 times
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
4
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
11.8%
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
5-7 times
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
4
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
11.8%
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Over 7 times
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
5
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
14.7%
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
NA
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
21
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
61.8%
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;font-weight: bold;vertical-align: top !important;&#34; rowspan=&#34;3&#34;&gt;
q30
&lt;/td&gt;
&lt;td style=&#34;text-align:left;vertical-align: top !important;&#34; rowspan=&#34;3&#34;&gt;
Should Amit buy a new computer?
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
No
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
3%
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Yes
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
12
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
35%
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
NA
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
21
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
62%
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;font-weight: bold;vertical-align: top !important;&#34; rowspan=&#34;3&#34;&gt;
q32
&lt;/td&gt;
&lt;td style=&#34;text-align:left;vertical-align: top !important;&#34; rowspan=&#34;3&#34;&gt;
Do you really mean that?
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
No
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
27
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
79.4%
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Yes
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
5
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
14.7%
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
NA
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
5.9%
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;font-weight: bold;vertical-align: top !important;&#34; rowspan=&#34;3&#34;&gt;
q37
&lt;/td&gt;
&lt;td style=&#34;text-align:left;vertical-align: top !important;&#34; rowspan=&#34;3&#34;&gt;
Would you like to participate in a follow up study?
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
No
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
6
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
17.6%
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Yes
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
25
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
73.5%
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
NA
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
3
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
8.8%
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;That easy? Yes!&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The trick here that I love is collapsing the column, an argument also common in other packages such as &lt;a href=&#34;https://github.com/renkun-ken/formattable&#34;&gt;{formattable}&lt;/a&gt; I look forward to explore. Collapsing a column makes printing in rmarkdown really easy and efficient, something I found lacking in other platforms I learned such as SPSS. I also added a &lt;code&gt;column_spec&lt;/code&gt; to bold the first column. Of course you can also remove the scroll box by not using the &lt;code&gt;scroll_box&lt;/code&gt; option at the end, which will print your whole table.&lt;/p&gt;
&lt;p&gt;If you’re looking for a word format, you can just copy &amp;amp; paste your html output (Perfect for working with Hebrew text for example, a little more on that below). You can find an additional example by &lt;a href=&#34;https://haozhu233.github.io/kableExtra/kableExtra_and_word.html&#34;&gt;Hao Zhu&lt;/a&gt;, the creator of the package, or here’s my attempt below:&lt;/p&gt;
&lt;div class=&#34;figure&#34; style=&#34;text-align: center&#34;&gt;&lt;span id=&#34;fig:unnamed-chunk-11&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;html-copy.gif&#34; alt=&#34;Just select all and copy it into a word document&#34; width=&#34;80%&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 1: Just select all and copy it into a word document
&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;If you want the table to be a little more formally formatted I recommend exploring aesthetic arguments such as &lt;code&gt;kable_classic&lt;/code&gt;, &lt;code&gt;kable_minimal&lt;/code&gt; and others from the &lt;a href=&#34;http://haozhu233.github.io/kableExtra/awesome_table_in_html.html&#34;&gt;{KableExtra}&lt;/a&gt; family. Here’s a short example using the &lt;code&gt;kable_minimal&lt;/code&gt; with an Html output:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;df_long_q_sorted %&amp;gt;% 
  head(10) %&amp;gt;% 
  kbl() %&amp;gt;% 
  kable_styling(full_width = F) %&amp;gt;% 
  column_spec(1, bold = T) %&amp;gt;% 
  collapse_rows(columns = c(1,2), valign = &amp;quot;top&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;div class=&#34;figure&#34; style=&#34;text-align: center&#34;&gt;&lt;span id=&#34;fig:unnamed-chunk-13&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;kable_minimal.png&#34; alt=&#34;Example using kable_minimal as a table theme&#34; width=&#34;80%&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 2: Example using kable_minimal as a table theme
&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;wait-but-what-if-i-want-a-simple-table-in-word&#34; class=&#34;section level5&#34;&gt;
&lt;h5&gt;Wait, but what if I want a simple table in word?&lt;/h5&gt;
&lt;p&gt;Let’s say you want a simple kable table when knitting in word, you can just add ‘df_print: kable’ to the YAML of your document or alternatively, you can explore other options in the &lt;a href=&#34;https://bookdown.org/yihui/rmarkdown/html-document.html&#34;&gt;Rmarkdown book&lt;/a&gt; that elegantly print dataframes. How do we remove the redundant information when printing? Just replace the duplicated values with an empty string:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;df_long_q_sorted %&amp;gt;% 
  mutate(across(c(question, text), ~ ifelse(duplicated(.x), &amp;quot; &amp;quot;, .x))) %&amp;gt;% 
  select(`Question` = question, `Text` = text, `Value` = value, n, `%` = pct) %&amp;gt;% 
  head(20)&lt;/code&gt;&lt;/pre&gt;
&lt;div class=&#34;figure&#34; style=&#34;text-align: center&#34;&gt;&lt;span id=&#34;fig:unnamed-chunk-15&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;df_print.png&#34; alt=&#34;Outputting a table using df_print: kable in the YAML section&#34; width=&#34;80%&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 3: Outputting a table using df_print: kable in the YAML section
&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;Voila!&lt;/p&gt;
&lt;p&gt;I first removed redundant text by using the &lt;code&gt;across&lt;/code&gt; along with a conditional argument to remove duplicated text. Basically the formula (&lt;code&gt;~ ifelse&lt;/code&gt;) reads as take anyone of the specified columns and pass it to a conditional statement that if true (if the word is duplicated), add a space character instead. Below is a screenshot when rendered to word, and you can continue to format it with or without other packages.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;right-to-left-languages&#34; class=&#34;section level5&#34;&gt;
&lt;h5&gt;Right-to-Left languages?&lt;/h5&gt;
&lt;p&gt;I found it difficult knitting Hebrew characters to a Word output but easily done when rendering Html documents. Here’s a short example, without the whole pre-processing, using some Hebrew questions:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;hebrew_example &amp;lt;- read_xlsx(here(&amp;quot;content&amp;quot;, &amp;quot;post&amp;quot;, &amp;quot;printing-survey-table&amp;quot;, &amp;quot;data&amp;quot;, &amp;quot;hebrew_example.xlsx&amp;quot;))
hebrew_example %&amp;gt;% 
  select(pct, n, value, text, question) %&amp;gt;% 
  mutate(pct = percent(pct)) %&amp;gt;% 
  # Reverse the order of questions
  kbl(col.names = c(&amp;quot;%&amp;quot;, &amp;quot;שכיחות&amp;quot;, &amp;quot;תשובה&amp;quot;, &amp;quot;שאלה&amp;quot;, &amp;quot;פריט&amp;quot;), align = &amp;#39;r&amp;#39;) %&amp;gt;% 
  kable_styling(full_width = F) %&amp;gt;% 
  column_spec(5, bold = T) %&amp;gt;% 
  collapse_rows(columns = c(4,5), valign = &amp;quot;top&amp;quot;) %&amp;gt;% 
  scroll_box(height = &amp;quot;500px&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;div style=&#34;border: 1px solid #ddd; padding: 0px; overflow-y: scroll; height:500px; &#34;&gt;
&lt;table class=&#34;table&#34; style=&#34;width: auto !important; margin-left: auto; margin-right: auto;&#34;&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:right;position: sticky; top:0; background-color: #FFFFFF;&#34;&gt;
%
&lt;/th&gt;
&lt;th style=&#34;text-align:right;position: sticky; top:0; background-color: #FFFFFF;&#34;&gt;
שכיחות
&lt;/th&gt;
&lt;th style=&#34;text-align:right;position: sticky; top:0; background-color: #FFFFFF;&#34;&gt;
תשובה
&lt;/th&gt;
&lt;th style=&#34;text-align:right;position: sticky; top:0; background-color: #FFFFFF;&#34;&gt;
שאלה
&lt;/th&gt;
&lt;th style=&#34;text-align:right;position: sticky; top:0; background-color: #FFFFFF;&#34;&gt;
פריט
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
47.1%
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
16
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
נקבה
&lt;/td&gt;
&lt;td style=&#34;text-align:right;vertical-align: top !important;&#34; rowspan=&#34;2&#34;&gt;
מגדר
&lt;/td&gt;
&lt;td style=&#34;text-align:right;font-weight: bold;vertical-align: top !important;&#34; rowspan=&#34;2&#34;&gt;
q2
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
52.9%
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
18
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
זכר
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
52.9%
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
18
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
שכיר
&lt;/td&gt;
&lt;td style=&#34;text-align:right;vertical-align: top !important;&#34; rowspan=&#34;6&#34;&gt;
סטטוס תעסוקתי
&lt;/td&gt;
&lt;td style=&#34;text-align:right;font-weight: bold;vertical-align: top !important;&#34; rowspan=&#34;6&#34;&gt;
q4
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
8.8%
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
3
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
עצמאי
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
23.5%
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
8
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
סטודנט
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
8.8%
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
3
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
סטודנט, שכיר
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2.9%
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
סטודנט, מובטל
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2.9%
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
מובטל
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
17.6%
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
6
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
לא
&lt;/td&gt;
&lt;td style=&#34;text-align:right;vertical-align: top !important;&#34; rowspan=&#34;3&#34;&gt;
האם תסכים להשתתף במחקר המשך?
&lt;/td&gt;
&lt;td style=&#34;text-align:right;font-weight: bold;vertical-align: top !important;&#34; rowspan=&#34;3&#34;&gt;
q37
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
73.5%
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
25
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
כן
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
8.8%
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
3
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
חסר
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Notice how I reversed the columns direction to have it compatible for a right-to-left reading. I also changed the locale setting (not shown) using the &lt;code&gt;Sys.setlocale&lt;/code&gt; argument. While this still needs some additional work (for example the ‘?’ isn’t aligned), it’s definitely a good start.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;wrapping-up&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Wrapping up&lt;/h4&gt;
&lt;p&gt;So this is how I approached the issue of providing a simple descriptive table of my categorical variables. While it was somewhat tedious and some parts required manual work, you might not require all the stages. I hope you were able to take something from this post, I certainly enjoy working with the &lt;a href=&#34;https://cran.r-project.org/web/packages/kableExtra/vignettes/awesome_table_in_html.html&#34;&gt;&lt;code&gt;{KableExtra}&lt;/code&gt;&lt;/a&gt; package and look forward to sharing additional things I’ve learned while using it for reporting findings from my thesis.&lt;/p&gt;
&lt;p&gt;Now then, time to get back to writing!&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;footnotes&#34;&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li id=&#34;fn1&#34;&gt;&lt;p&gt;If you use Qualtrics as your survey platform, check out the corresponding package to work with such data &lt;a href=&#34;https://cran.r-project.org/web/packages/qualtRics/vignettes/qualtRics.html&#34;&gt;here&lt;/a&gt;.&lt;a href=&#34;#fnref1&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn2&#34;&gt;&lt;p&gt;Of course there are many other issues I won’t address here, such as response options that are missing if no one chose them in my current pilot population.&lt;a href=&#34;#fnref2&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
      
      
            <category>R</category>
      
    </item>
    
    <item>
      <title>Defining uncertainty in the Israeli lottery</title>
      <link>https://amitlevinson.com/blog/uncertainty-in-the-israeli-lottery/</link>
      <pubDate>Thu, 09 Jul 2020 00:00:00 +0000</pubDate>
      
      <guid>https://amitlevinson.com/blog/uncertainty-in-the-israeli-lottery/</guid>
      <description>


&lt;blockquote&gt;
&lt;p&gt;“I have a great idea to get rich. All we need is a lot of money.” &lt;/br&gt; ― A meme on the internet&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;A while ago I was reading about Bernoulli trials and decided that I wanted to explore them further. I was wondering what would be an interesting case study for such a topic, and then it hit me (💡): Why not explore lottery probabilities? Little did I know that this topic would lead me down the geometric distribution road and help me better understand the uncertainty in lotteries. So, shall we?&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;hunger.jpg&#34; width=&#34;85%&#34; height=&#34;85%&#34; /&gt;&lt;/p&gt;
&lt;div id=&#34;the-rules&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;The rules&lt;/h3&gt;
&lt;p&gt;We’ll be exploring the probability of winning the Israeli lottery held by the &lt;a href=&#34;https://www.pais.co.il/&#34;&gt;Pais organization&lt;/a&gt;, a well-known lottery enterprise in Israel. The ticket we’ll be discussing is applicable to the main lottery called ‘Loto’ and costs 5.8 New Israeli Shekels (NIS), approximately ~1.6 dollars.&lt;/p&gt;
&lt;p&gt;Pais holds their lotteries twice a week with the regular prize of 5 million NIS, equivalent to $1,420,000. If there’s no winner for a given week, the prize accumulates to the following lottery. For the sake of the post I’ll only discuss winning a first place prize (be the only winner). In addition I won’t be discussing the effect of prize increase on when to participate.&lt;/p&gt;
&lt;p&gt;When filling out a lottery form you choose 6 numbers in the range of 1–37 and a ‘strong’ number in the range of 1–7&lt;a href=&#34;#fn1&#34; class=&#34;footnote-ref&#34; id=&#34;fnref1&#34;&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt;. In order to win first place you have to &lt;del&gt;get&lt;/del&gt; guess correctly both the 6 number set and the strong number. Luckily, the order of the 6 numbers doesn’t matter, and therefore if you wrote &lt;span class=&#34;math inline&#34;&gt;\(6,12...n\)&lt;/span&gt; or &lt;span class=&#34;math inline&#34;&gt;\(12,6,...n\)&lt;/span&gt; you’re good on both (Also known as combinations, more on that in a minute).&lt;/p&gt;
&lt;p&gt;Another ‘luck’ in our favor is that in each lottery ticket we have the option to fill out two sets of numbers, thus doubling our odds of winning. I’m assuming we’re on the same page and you won’t use both of your attempts to guess the same sets of numbers, so that somewhat increases our odds of winning. Speaking of odds, let’s have a look at them.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;the-odds&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;The odds&lt;/h3&gt;
&lt;p&gt;To understand the lottery probabilities, we need to calculate &lt;strong&gt;the probability of guessing a combination of 6 numbers out of 37 options along with one strong number out of 7 possible numbers.&lt;/strong&gt; In order to do this, we can turn to combinations:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;In mathematics, a combination is a selection of items from a collection, such that (unlike permutations) the order of selection does not matter. &lt;/br&gt; ― &lt;a href=&#34;https://en.wikipedia.org/wiki/Combination&#34;&gt;Wikipedia&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;That’s exactly what we need. We want to calculate the probability of randomly guessing six numbers regardless of their order; If the order was important we’d want to look at permutations. In addition, each number is drawn without replacement, and therefore there can’t be repetition of the same number.&lt;/p&gt;
&lt;p&gt;The formula to calculate combinations is as follows: &lt;span class=&#34;math inline&#34;&gt;\(C(n,k) = \frac{n!}{k!(n-k)!}\)&lt;/span&gt; Where &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; is the number of options to choose from and &lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt; is the number of choices we make.&lt;/p&gt;
&lt;p&gt;Inputting our numbers we get &lt;span class=&#34;math inline&#34;&gt;\(C(37,6) = \frac{37!}{6!(37-6)!}\)&lt;/span&gt;, and now we just calculate away. However, don’t forget we also need to guess another number out of 7 possible numbers (the strong one), so we’ll multiply our outcome by &lt;span class=&#34;math inline&#34;&gt;\(\frac1 7\)&lt;/span&gt;, yielding a probability of &lt;span class=&#34;math inline&#34;&gt;\(p = \frac{1}{16273488}\)&lt;/span&gt;. ‘Luckily’ we choose two sets of numbers in a given ticket, so we multiply the probability by 2.&lt;/p&gt;
&lt;p&gt;Therefore, the probability of winning the lottery is &lt;strong&gt;&lt;span class=&#34;math inline&#34;&gt;\(p = \frac{1}{8136744}\)&lt;/span&gt;.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Wow! that’s a very low probability. How low? Let’s try and visualize it.&lt;/p&gt;
&lt;p&gt;Sometimes when we receive a probability it’s hard to grasp the odds and numbers thrown at us. Therefore, I’ll try to visualize it for us. Imagine there’s a pool filled with 8,136,744 balls. One of those balls is red and choosing that exact red ball blindly will win you the lottery:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(ggplot2)
library(dplyr)
library(scattermore)
library(extrafont)
library(ggtext)

set.seed(123)

df_viz &amp;lt;- data.frame(x = rnorm(8136743, mean = 1000, sd = 1000),y = rnorm(8136743))
point_highlight &amp;lt;- data.frame(x = -1811.674, y = -2.268505588)

lot_p &amp;lt;- ggplot()+
  geom_scattermost(df_viz, pointsize = 0.1, pixels = c(2000,2000))+
  geom_point(data = point_highlight, aes(x = x, y = y), size = 0.3, color = &amp;quot;red&amp;quot;)+
  annotate(geom = &amp;quot;curve&amp;quot;, x = -2750, xend = -1860, y = -3.10, yend = -2.29,
    curvature = -.2, color = &amp;quot;grey25&amp;quot;, size = 0.75, arrow = arrow(length = unit(1.5, &amp;quot;mm&amp;quot;)))+
  annotate(&amp;quot;text&amp;quot; ,x = -2750, y = -3.30, label = &amp;quot;Winner&amp;quot;, family = &amp;quot;Roboto Condensed&amp;quot;, size = 3)+
  labs(title = &amp;quot;Winning the Israeli lottery&amp;quot;, subtitle = &amp;quot;To win, imagine trying to randomly choose a &amp;lt;b&amp;gt;&amp;lt;span style=&amp;#39;color:red&amp;#39;&amp;gt;specific ball&amp;lt;/span&amp;gt;&amp;lt;/b&amp;gt; out of 8,136,744 balls&amp;quot;)+
  theme_void()+
  theme(text = element_text(family = &amp;quot;Roboto Condensed&amp;quot;),
        plot.title.position = &amp;quot;plot&amp;quot;,
        plot.title = element_text(size = 16, face= &amp;quot;bold&amp;quot;),
        plot.subtitle = element_markdown(family = &amp;quot;Roboto Condensed&amp;quot;,size = 12),
        plot.margin = margin(4,2,2,4, unit = &amp;quot;mm&amp;quot;))

lot_p&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://amitlevinson.com/post/uncertainty-in-the-israeli-lottery/index_files/figure-html/unnamed-chunk-2-1.png&#34; width=&#34;672&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Not easy is it?&lt;/p&gt;
&lt;p&gt;Now that we know the probability of winning at each attempt, let’s see how it manifests across multiple attempts.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;multiple-attempts---geometric-distribution&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Multiple attempts - Geometric distribution&lt;/h3&gt;
&lt;p&gt;A Bernoulli trial is a random experiment with exactly two outcomes - such as success\failure, heads\tails - in which the probability for each outcome is the same every time &lt;a href=&#34;https://en.wikipedia.org/wiki/Bernoulli_trial&#34;&gt;(Wikipedia)&lt;/a&gt;. This sets the ground for discussing an outcome of a lottery in which you either win or lose.&lt;/p&gt;
&lt;p&gt;But we want to learn more about the &lt;em&gt;distribution of attempts&lt;/em&gt;, and this brings us to the geometric distribution. A &lt;a href=&#34;https://en.wikipedia.org/wiki/Geometric_distribution&#34;&gt;Geometric distribution&lt;/a&gt; enables us to calculate the probability distribution of a number of failures before the first success&lt;a href=&#34;#fn2&#34; class=&#34;footnote-ref&#34; id=&#34;fnref2&#34;&gt;&lt;sup&gt;2&lt;/sup&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Before we begin, we must meet several conditions to use the geometric distribution:&lt;/p&gt;
&lt;p&gt;✔️ Each trial is independent from one another - succeeding in one trial doesn’t affect the next trial. We know this is true since winning in one lottery won’t affect your chances of winning the next round.&lt;/p&gt;
&lt;p&gt;✔️ Every trial has an outcome of a success or failure. This assumption is true in our case where each lottery you participate in you either win or lose.&lt;/p&gt;
&lt;p&gt;✔️ The probability of success &lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt; is the same every trial - This is also true given the lottery probabilities are consistent across each game.&lt;/p&gt;
&lt;p&gt;Now that we got the technicalities out of the way we can start exploring some of the uncertainty surrounding the lottery.&lt;/p&gt;
&lt;div id=&#34;winning-at-a-given-trial&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Winning &lt;strong&gt;at&lt;/strong&gt; a given trial&lt;/h4&gt;
&lt;p&gt;We can denote the probability of winning as &lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt;, which in the case of a lottery game is equal to &lt;span class=&#34;math inline&#34;&gt;\(p = \frac{1}{8136744}\)&lt;/span&gt;. What if we wanted to know the probability of winning the lottery on the third try? That means we need two failures and then a success. If the probability of success - guessing the correct numbers - is &lt;span class=&#34;math inline&#34;&gt;\(p = \frac{1}{8136744}\)&lt;/span&gt;, so the probability of a failure is &lt;span class=&#34;math inline&#34;&gt;\(q = 1 - p\)&lt;/span&gt;, in this case &lt;span class=&#34;math inline&#34;&gt;\(q = \frac{8136743}{8136744}\)&lt;/span&gt;. In order to win the lottery on the third try, this means getting two failures and then a success, resulting in a total of &lt;span class=&#34;math inline&#34;&gt;\(k = 3\)&lt;/span&gt; attempts. Thus, the probability of winning on the third attempt is as follows:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(p(3) = (\frac{8136743}{8136744})\cdot(\frac{8136743}{8136744})\cdot(\frac{1}{8136744})\)&lt;/span&gt;, equaling &lt;span class=&#34;math inline&#34;&gt;\(p = 0.0000001228993\)&lt;/span&gt;. In other words there’s a ~0.0000123% chance we’ll win the lottery &lt;em&gt;exactly&lt;/em&gt; on the third try.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Generalizing, the probability distribution of the number of Bernoulli trials needed to get one success on the &lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt; th trial is: &lt;span class=&#34;math inline&#34;&gt;\(P(X = k) = (1 - p)^{(k-1)} \cdot p\)&lt;/span&gt;.&lt;/strong&gt; We can break this up according to our previous example:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(P\)&lt;/span&gt; stands for the probability of getting our value &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; on the &lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt; attempt. Meaning, we want to win the lottery only on the third attempt.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;So our first two attempts should be a failure, thus a probability of &lt;span class=&#34;math inline&#34;&gt;\(q = 1 - \frac{1}{8136744}\)&lt;/span&gt; multiplied by two (two rounds of failures), written as &lt;span class=&#34;math inline&#34;&gt;\((\frac{8136743}{8136744})^{3 - 1}\)&lt;/span&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Lastly, &lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt; stands for the probability of succeeding, &lt;span class=&#34;math inline&#34;&gt;\(\frac{1}{8136744}\)&lt;/span&gt; occurring exactly on the &lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt; attempt.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The probability we just discussed is also known as the probability mass function (PMF) of the geometric distribution. PMF is a function that gives the probability that a random discrete variable is exactly equal to some value. In our above example, the probability that we’ll win exactly on the third try.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;winning-by-a-given-trial&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Winning &lt;strong&gt;by&lt;/strong&gt; a given trial&lt;/h4&gt;
&lt;p&gt;We don’t necessarily want to win the lottery on on a specific &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; attempt, but explore the probabilities of winning &lt;em&gt;by&lt;/em&gt; the &lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt;th attempt. Reframing our previous question we can ask &lt;strong&gt;“what is the probability of winning the lottery on at least one of the first 3 attempts?”&lt;/strong&gt;, bringing us to the Cumulative distribution function (CDF). In a cumulative distribution we calculate the probability that &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; will take a value less than or equal to &lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt; (in our case representing the number of attempts).&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;How does this question change our calculation?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Let’s assume we’re still talking about 3 attempts. Our new framed question means we want to win the lottery either on the first attempt, the second or the third. In other words, we want to add the probability of success when &lt;span class=&#34;math inline&#34;&gt;\(P(X = 1)\)&lt;/span&gt; + &lt;span class=&#34;math inline&#34;&gt;\(P(X = 2)\)&lt;/span&gt; + &lt;span class=&#34;math inline&#34;&gt;\(P(X = 3)\)&lt;/span&gt;. Given that our probability of failure is &lt;span class=&#34;math inline&#34;&gt;\(q = 1 - p\)&lt;/span&gt;, we can write the argument as follows: &lt;span class=&#34;math inline&#34;&gt;\(P(X \leq 3) = q^0\cdot p + q^1 \cdot p + q^2 \cdot p\)&lt;/span&gt;, inputting our values of &lt;span class=&#34;math inline&#34;&gt;\({(\frac{8136743}{8136744}})^0 \cdot p \cdot({\frac{8136743}{8136744}})^1\cdot p, ...\)&lt;/span&gt;, resulting in the probability of winning in one of the first three attempts &lt;span class=&#34;math inline&#34;&gt;\(P(X \leq 3) = 0.000000368\)&lt;/span&gt;, also written as a 0.0000368% chance.&lt;/p&gt;
&lt;p&gt;But if we want to look at the first 50 attempts? we’ll have to sum each individual PMF?&lt;/p&gt;
&lt;p&gt;Here’s exactly the use of the geometric CDF written as &lt;span class=&#34;math inline&#34;&gt;\(P(X &amp;lt;= x) = 1 - q^x\)&lt;/span&gt;. We raise the probability of losing to the power of attempts to win by and deduct it from 1, resulting in the probability of winning by a given trial.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;winning-on-the-first-x-trials&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Winning on the first X trials&lt;/h3&gt;
&lt;p&gt;We just looked at the probability of winning on the first 3 trials, and now that we learned about the CDF we can calculate the probability of winning on the first &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt; trials, for e.g. on the first 100, 1000 and so on. In addition, another important factor we can take into account exploring the cumulative distribution is the money spent reaching each attempt.&lt;/p&gt;
&lt;p&gt;We’ll start by declaring our values. We know the probability for winning the lottery &lt;em&gt;with each ticket we have&lt;/em&gt; is &lt;span class=&#34;math inline&#34;&gt;\(p = \frac{1}{8,136,744}\)&lt;/span&gt; (remember, we get to choose two sets of numbers in each ticket), so let’s declare that:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;p &amp;lt;- 1/8136744&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Next we know the probability for not winning is &lt;span class=&#34;math inline&#34;&gt;\(q = 1 - p\)&lt;/span&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;q &amp;lt;- 1 - p&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now we can create a data frame to account for some 250,000 attempts. We don’t need each attempt so we’ll simulate data for the first 50,000 and then have points spread out in a 500 interval jump all the way to the 100,000,000 attempt.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;df_prob &amp;lt;- tibble(trial = c(1:50000, seq(50000, 1e8, 500)))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Once we have that we can calculate both the probability of winning up to a specific attempt and the cumulative amount of money spent reaching there (according to a price ticket of NIS 5.8):&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;df_prob &amp;lt;- df_prob %&amp;gt;% 
  mutate(cdf = 1 - (q)^trial,
         money_spent = trial * 5.8)

head(df_prob)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 6 x 3
##   trial         cdf money_spent
##   &amp;lt;dbl&amp;gt;       &amp;lt;dbl&amp;gt;       &amp;lt;dbl&amp;gt;
## 1     1 0.000000123         5.8
## 2     2 0.000000246        11.6
## 3     3 0.000000369        17.4
## 4     4 0.000000492        23.2
## 5     5 0.000000614        29  
## 6     6 0.000000737        34.8&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Looks good!&lt;/p&gt;
&lt;p&gt;We see our top 6 observations with 3 columns we just defined (from left to right): the lottery raffle (trial), the probability of winning at a given trial until that point (cdf) and the money spent by that trial. Our probability of winning at &lt;em&gt;any&lt;/em&gt; trial is constant (&lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt;), so it’ll be redundant to add that in here.&lt;/p&gt;
&lt;p&gt;Now let’s look at specific points along our data frame and see how much money is spent reaching there. More specifically, let’s look at the details of some attempts such as 1; 10; 100; 1000; 2500, 5000, &lt;span class=&#34;math inline&#34;&gt;\(...\)&lt;/span&gt;, 1,000,000, 10,000,000; 50,000,000:&lt;/p&gt;
&lt;style&gt;html {
  font-family: -apple-system, BlinkMacSystemFont, &#39;Segoe UI&#39;, Roboto, Oxygen, Ubuntu, Cantarell, &#39;Helvetica Neue&#39;, &#39;Fira Sans&#39;, &#39;Droid Sans&#39;, Arial, sans-serif;
}

#hvuocxzswv .gt_table {
  display: table;
  border-collapse: collapse;
  margin-left: auto;
  margin-right: auto;
  color: #333333;
  font-size: 16px;
  background-color: #FFFFFF;
  width: auto;
  border-top-style: solid;
  border-top-width: 2px;
  border-top-color: #A8A8A8;
  border-right-style: none;
  border-right-width: 2px;
  border-right-color: #D3D3D3;
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #A8A8A8;
  border-left-style: none;
  border-left-width: 2px;
  border-left-color: #D3D3D3;
}

#hvuocxzswv .gt_heading {
  background-color: #FFFFFF;
  text-align: left;
  border-bottom-color: #FFFFFF;
  border-left-style: none;
  border-left-width: 1px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 1px;
  border-right-color: #D3D3D3;
}

#hvuocxzswv .gt_title {
  color: #333333;
  font-size: 125%;
  font-weight: bold;
  padding-top: 4px;
  padding-bottom: 4px;
  border-bottom-color: #FFFFFF;
  border-bottom-width: 0;
}

#hvuocxzswv .gt_subtitle {
  color: #333333;
  font-size: 85%;
  font-weight: initial;
  padding-top: 0;
  padding-bottom: 4px;
  border-top-color: #FFFFFF;
  border-top-width: 0;
}

#hvuocxzswv .gt_bottom_border {
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
}

#hvuocxzswv .gt_col_headings {
  border-top-style: solid;
  border-top-width: 2px;
  border-top-color: #D3D3D3;
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
  border-left-style: none;
  border-left-width: 1px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 1px;
  border-right-color: #D3D3D3;
}

#hvuocxzswv .gt_col_heading {
  color: #333333;
  background-color: #FFFFFF;
  font-size: 100%;
  font-weight: normal;
  text-transform: inherit;
  border-left-style: none;
  border-left-width: 1px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 1px;
  border-right-color: #D3D3D3;
  vertical-align: bottom;
  padding-top: 5px;
  padding-bottom: 6px;
  padding-left: 5px;
  padding-right: 5px;
  overflow-x: hidden;
}

#hvuocxzswv .gt_column_spanner_outer {
  color: #333333;
  background-color: #FFFFFF;
  font-size: 100%;
  font-weight: normal;
  text-transform: inherit;
  padding-top: 0;
  padding-bottom: 0;
  padding-left: 4px;
  padding-right: 4px;
}

#hvuocxzswv .gt_column_spanner_outer:first-child {
  padding-left: 0;
}

#hvuocxzswv .gt_column_spanner_outer:last-child {
  padding-right: 0;
}

#hvuocxzswv .gt_column_spanner {
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
  vertical-align: bottom;
  padding-top: 5px;
  padding-bottom: 6px;
  overflow-x: hidden;
  display: inline-block;
  width: 100%;
}

#hvuocxzswv .gt_group_heading {
  padding: 8px;
  color: #333333;
  background-color: #FFFFFF;
  font-size: 100%;
  font-weight: initial;
  text-transform: inherit;
  border-top-style: solid;
  border-top-width: 2px;
  border-top-color: #D3D3D3;
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
  border-left-style: none;
  border-left-width: 1px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 1px;
  border-right-color: #D3D3D3;
  vertical-align: middle;
}

#hvuocxzswv .gt_empty_group_heading {
  padding: 0.5px;
  color: #333333;
  background-color: #FFFFFF;
  font-size: 100%;
  font-weight: initial;
  border-top-style: solid;
  border-top-width: 2px;
  border-top-color: #D3D3D3;
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
  vertical-align: middle;
}

#hvuocxzswv .gt_striped {
  background-color: rgba(128, 128, 128, 0.05);
}

#hvuocxzswv .gt_from_md &gt; :first-child {
  margin-top: 0;
}

#hvuocxzswv .gt_from_md &gt; :last-child {
  margin-bottom: 0;
}

#hvuocxzswv .gt_row {
  padding-top: 8px;
  padding-bottom: 8px;
  padding-left: 5px;
  padding-right: 5px;
  margin: 10px;
  border-top-style: solid;
  border-top-width: 1px;
  border-top-color: #D3D3D3;
  border-left-style: none;
  border-left-width: 1px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 1px;
  border-right-color: #D3D3D3;
  vertical-align: middle;
  overflow-x: hidden;
}

#hvuocxzswv .gt_stub {
  color: #333333;
  background-color: #FFFFFF;
  font-size: 100%;
  font-weight: initial;
  text-transform: inherit;
  border-right-style: solid;
  border-right-width: 2px;
  border-right-color: #D3D3D3;
  padding-left: 12px;
}

#hvuocxzswv .gt_summary_row {
  color: #333333;
  background-color: #FFFFFF;
  text-transform: inherit;
  padding-top: 8px;
  padding-bottom: 8px;
  padding-left: 5px;
  padding-right: 5px;
}

#hvuocxzswv .gt_first_summary_row {
  padding-top: 8px;
  padding-bottom: 8px;
  padding-left: 5px;
  padding-right: 5px;
  border-top-style: solid;
  border-top-width: 2px;
  border-top-color: #D3D3D3;
}

#hvuocxzswv .gt_grand_summary_row {
  color: #333333;
  background-color: #FFFFFF;
  text-transform: inherit;
  padding-top: 8px;
  padding-bottom: 8px;
  padding-left: 5px;
  padding-right: 5px;
}

#hvuocxzswv .gt_first_grand_summary_row {
  padding-top: 8px;
  padding-bottom: 8px;
  padding-left: 5px;
  padding-right: 5px;
  border-top-style: double;
  border-top-width: 6px;
  border-top-color: #D3D3D3;
}

#hvuocxzswv .gt_table_body {
  border-top-style: solid;
  border-top-width: 2px;
  border-top-color: #D3D3D3;
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
}

#hvuocxzswv .gt_footnotes {
  color: #333333;
  background-color: #FFFFFF;
  border-bottom-style: none;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
  border-left-style: none;
  border-left-width: 2px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 2px;
  border-right-color: #D3D3D3;
}

#hvuocxzswv .gt_footnote {
  margin: 0px;
  font-size: 90%;
  padding: 4px;
}

#hvuocxzswv .gt_sourcenotes {
  color: #333333;
  background-color: #FFFFFF;
  border-bottom-style: none;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
  border-left-style: none;
  border-left-width: 2px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 2px;
  border-right-color: #D3D3D3;
}

#hvuocxzswv .gt_sourcenote {
  font-size: 90%;
  padding: 4px;
}

#hvuocxzswv .gt_left {
  text-align: left;
}

#hvuocxzswv .gt_center {
  text-align: center;
}

#hvuocxzswv .gt_right {
  text-align: right;
  font-variant-numeric: tabular-nums;
}

#hvuocxzswv .gt_font_normal {
  font-weight: normal;
}

#hvuocxzswv .gt_font_bold {
  font-weight: bold;
}

#hvuocxzswv .gt_font_italic {
  font-style: italic;
}

#hvuocxzswv .gt_super {
  font-size: 65%;
}

#hvuocxzswv .gt_footnote_marks {
  font-style: italic;
  font-size: 65%;
}
&lt;/style&gt;
&lt;div id=&#34;hvuocxzswv&#34; style=&#34;overflow-x:auto;overflow-y:auto;width:auto;height:auto;&#34;&gt;&lt;table class=&#34;gt_table&#34;&gt;
  &lt;thead class=&#34;gt_header&#34;&gt;
    &lt;tr&gt;
      &lt;th colspan=&#34;3&#34; class=&#34;gt_heading gt_title gt_font_normal&#34; style&gt;&lt;b&gt;&lt;span style=&#39;font-family:Roboto Condensed&#39;&gt;Lottery probabilities with the geometric distribution&lt;/span&gt;&lt;/b&gt;&lt;/th&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th colspan=&#34;3&#34; class=&#34;gt_heading gt_subtitle gt_font_normal gt_bottom_border&#34; style&gt;&lt;span style=&#39;font-family:Roboto Condensed&#39;&gt;Lottery probabilities winning by a given attempt, the money spent reaching there and your chances of winning by then&lt;/span&gt;&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;thead class=&#34;gt_col_headings&#34;&gt;
    &lt;tr&gt;
      &lt;th class=&#34;gt_col_heading gt_columns_bottom_border gt_left&#34; rowspan=&#34;1&#34; colspan=&#34;1&#34;&gt;Attempt&lt;/th&gt;
      &lt;th class=&#34;gt_col_heading gt_columns_bottom_border gt_left&#34; rowspan=&#34;1&#34; colspan=&#34;1&#34;&gt;Money spent&lt;sup class=&#34;gt_footnote_marks&#34;&gt;1&lt;/sup&gt;&lt;/th&gt;
      &lt;th class=&#34;gt_col_heading gt_columns_bottom_border gt_left&#34; rowspan=&#34;1&#34; colspan=&#34;1&#34;&gt;% winning by then&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody class=&#34;gt_table_body&#34;&gt;
    &lt;tr&gt;
      &lt;td class=&#34;gt_row gt_left&#34;&gt;1&lt;/td&gt;
      &lt;td class=&#34;gt_row gt_left&#34;&gt;$2&lt;/td&gt;
      &lt;td class=&#34;gt_row gt_left&#34;&gt;0.00001%&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td class=&#34;gt_row gt_left&#34;&gt;10&lt;/td&gt;
      &lt;td class=&#34;gt_row gt_left&#34;&gt;$17&lt;/td&gt;
      &lt;td class=&#34;gt_row gt_left&#34;&gt;0.00012%&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td class=&#34;gt_row gt_left&#34;&gt;100&lt;/td&gt;
      &lt;td class=&#34;gt_row gt_left&#34;&gt;$166&lt;/td&gt;
      &lt;td class=&#34;gt_row gt_left&#34;&gt;0.00123%&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td class=&#34;gt_row gt_left&#34;&gt;500&lt;/td&gt;
      &lt;td class=&#34;gt_row gt_left&#34;&gt;$829&lt;/td&gt;
      &lt;td class=&#34;gt_row gt_left&#34;&gt;0.00614%&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td class=&#34;gt_row gt_left&#34;&gt;1,000&lt;/td&gt;
      &lt;td class=&#34;gt_row gt_left&#34;&gt;$1,657&lt;/td&gt;
      &lt;td class=&#34;gt_row gt_left&#34;&gt;0.01229%&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td class=&#34;gt_row gt_left&#34;&gt;2,500&lt;/td&gt;
      &lt;td class=&#34;gt_row gt_left&#34;&gt;$4,143&lt;/td&gt;
      &lt;td class=&#34;gt_row gt_left&#34;&gt;0.03072%&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td class=&#34;gt_row gt_left&#34;&gt;5,000&lt;/td&gt;
      &lt;td class=&#34;gt_row gt_left&#34;&gt;$8,286&lt;/td&gt;
      &lt;td class=&#34;gt_row gt_left&#34;&gt;0.06143%&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td class=&#34;gt_row gt_left&#34;&gt;10,000&lt;/td&gt;
      &lt;td class=&#34;gt_row gt_left&#34;&gt;$16,571&lt;/td&gt;
      &lt;td class=&#34;gt_row gt_left&#34;&gt;0.12282%&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td class=&#34;gt_row gt_left&#34;&gt;25,000&lt;/td&gt;
      &lt;td class=&#34;gt_row gt_left&#34;&gt;$41,429&lt;/td&gt;
      &lt;td class=&#34;gt_row gt_left&#34;&gt;0.30678%&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td class=&#34;gt_row gt_left&#34;&gt;100,000&lt;/td&gt;
      &lt;td class=&#34;gt_row gt_left&#34;&gt;$165,714&lt;/td&gt;
      &lt;td class=&#34;gt_row gt_left&#34;&gt;1.22147%&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td class=&#34;gt_row gt_left&#34;&gt;250,000&lt;/td&gt;
      &lt;td class=&#34;gt_row gt_left&#34;&gt;$414,286&lt;/td&gt;
      &lt;td class=&#34;gt_row gt_left&#34;&gt;3.02576%&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td class=&#34;gt_row gt_left&#34;&gt;500,000&lt;/td&gt;
      &lt;td class=&#34;gt_row gt_left&#34;&gt;$828,571&lt;/td&gt;
      &lt;td class=&#34;gt_row gt_left&#34;&gt;5.95997%&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td class=&#34;gt_row gt_left&#34;&gt;1,000,000&lt;/td&gt;
      &lt;td class=&#34;gt_row gt_left&#34;&gt;$1,657,143&lt;/td&gt;
      &lt;td class=&#34;gt_row gt_left&#34;&gt;11.56473%&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td class=&#34;gt_row gt_left&#34;&gt;10,000,000&lt;/td&gt;
      &lt;td class=&#34;gt_row gt_left&#34;&gt;$16,571,429&lt;/td&gt;
      &lt;td class=&#34;gt_row gt_left&#34;&gt;70.74129%&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td class=&#34;gt_row gt_left&#34;&gt;50,000,000&lt;/td&gt;
      &lt;td class=&#34;gt_row gt_left&#34;&gt;$82,857,143&lt;/td&gt;
      &lt;td class=&#34;gt_row gt_left&#34;&gt;99.78557%&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
  
  &lt;tfoot&gt;
    &lt;tr class=&#34;gt_footnotes&#34;&gt;
      &lt;td colspan=&#34;3&#34;&gt;
        &lt;p class=&#34;gt_footnote&#34;&gt;
          &lt;sup class=&#34;gt_footnote_marks&#34;&gt;
            &lt;em&gt;1&lt;/em&gt;
          &lt;/sup&gt;
           
          Money spent (rounded up) corresponds to the cumulative number of attempts played
          &lt;br /&gt;
        &lt;/p&gt;
      &lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tfoot&gt;
&lt;/table&gt;&lt;/div&gt;
&lt;p&gt;In the above table I printed specific observations along the lottery’s cumulative geometric distribution. In our left column we have the trial number, next the approximate amount of money spent up to that trial and lastly the percent of winning by that given trial. Notice that I converted the New Israeli Shekels to dollars ($1 dollar = ~ NIS 3.5).&lt;/p&gt;
&lt;p&gt;If we played 100 consecutive games with the same number, we would spend 166 dollars by that point and have only a 0.00123% chance of winning. We only pass the 1% (!) chance of winning after buying more than 100,000 tickets, spending a total of $165,714 dollars.&lt;br /&gt;
&lt;strong&gt;To pass the 10% chance of winning you’d have to play 1,000,000 games and spend ~1,600,000 dollars! Reminder: the default prize is only some $1,412,000!&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;dumber.jpg&#34; width=&#34;246&#34; height=&#34;80%&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;average-number-of-attempts&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Average number of attempts&lt;/h3&gt;
&lt;p&gt;An interesting feature of the geometric distribution is that we can calculate the expected number of attempts and variance of the distribution. The expected number of attempts in the discussed geometric cumulative distribution is &lt;span class=&#34;math inline&#34;&gt;\(E(X) = \frac{1}{p}\)&lt;/span&gt; with a variance of &lt;span class=&#34;math inline&#34;&gt;\(var(X) = \frac{1-p}{p^2}\)&lt;/span&gt;. &lt;span class=&#34;math inline&#34;&gt;\(E(X)\)&lt;/span&gt; is basically the expected value for the number of independent trials needed to get the first success (Think of it as an average, only theoretically). So in our lottery example the expected number of attempts to reach a success is &lt;span class=&#34;math inline&#34;&gt;\(E(X) = \frac{1}{\frac{1}{8136744}}\)&lt;/span&gt;, resulting in 8,136,744 attempts.&lt;/p&gt;
&lt;p&gt;R has built in functions for working with the geometric distribution such as &lt;code&gt;pgeom&lt;/code&gt;, &lt;code&gt;rgeom&lt;/code&gt;, &lt;code&gt;qgeom&lt;/code&gt; and &lt;code&gt;dgeom&lt;/code&gt; which you can explore more &lt;a href=&#34;https://www.statology.org/dgeom-pgeom-qgeom-rgeom-r/&#34;&gt;here&lt;/a&gt;. For the purpose of exploring the mean we can use the &lt;code&gt;rgeom&lt;/code&gt; function which generates a value representing the number of failures before a success occurred. For example, let’s see how many failures we’re required to reach one success:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;rgeom(n = 1,p = p)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 30687199&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In the above example &lt;code&gt;rgeom&lt;/code&gt; takes the number of rounds (n = 1) and the probability of winning (p = p). The outputted value indicates the number of failures before our success.&lt;/p&gt;
&lt;p&gt;Using this we can calculate the average number of attempts from 2,000,000 games:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mean(rgeom(2e6, p))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 8129065&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Pretty close to our expected value!&lt;/p&gt;
&lt;p&gt;So what does the &lt;span class=&#34;math inline&#34;&gt;\(E(X)\)&lt;/span&gt; mean in terms of the lottery? &lt;strong&gt;You’d have to play approximately 8,136,744 games to win the lottery, spending a total of NIS 47,193,115 (~$13,483,747) to win approximately NIS 5M (approximately 1.42M dollars)!&lt;/strong&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;conclusion&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Conclusion&lt;/h3&gt;
&lt;p&gt;In this post we were able to uncover and better understand some of the uncertainty that covers a lottery game. Using the geometric distribution we explored the probability of winning the lottery at a specific event, and winning it in the form of a cumulative distribution - Chances of winning up to a given trial.&lt;/p&gt;
&lt;p&gt;Unfortunately, the numbers aren’t in our favor. You’d find yourself spending a great deal of money before actually winning the lottery. I’m definitely not going to tell you what to do with your money, but I hope this blog post helped you understand a little better the chances of (not) winning the lottery. But hey, apparently a &lt;a href=&#34;https://www.businessinsider.com/powerball-lottery-playing-same-numbers-odds-of-winning-2018-11&#34;&gt;New Yorker won the lottery after participating each week for 25 years&lt;/a&gt; so you never know.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;further-reading-exploring&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Further reading \ exploring&lt;/h3&gt;
&lt;p&gt;Two resources I found extremely valuable in learning more about the geometric distribution:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;The &lt;a href=&#34;https://en.wikipedia.org/wiki/Geometric_distribution&#34;&gt;Geometric distribution Wikipedia’s page&lt;/a&gt;. I’m constantly amazed at the vast amount and well articulated statistical pages they have.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Continuing on that, I found the resource that the Wikipedia page relies on extremely helpful: “A modern introduction to probability and statistics : understanding why and how”.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;If you’re more of a video kind of person, I highly recommend a video by &lt;a href=&#34;https://www.youtube.com/channel/UCEWpbFLzoYGPfuWUMFPSaoA&#34;&gt;The Organic Chemistry Tutor&lt;/a&gt; about the &lt;a href=&#34;https://www.youtube.com/watch?v=d5iAWPnrH6w&amp;amp;t=1s&#34;&gt;Geometric distribution&lt;/a&gt;. I think he does a superb job in explaining different various statistical analysis and always enjoys his videos.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;notes&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Notes&lt;/h3&gt;
&lt;/div&gt;
&lt;div class=&#34;footnotes&#34;&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li id=&#34;fn1&#34;&gt;&lt;p&gt;An amusing anecdote is that the Pais organization offers information about ‘Hot’ numbers and the &lt;a href=&#34;https://www.pais.co.il/lotto/statistics.aspx&#34;&gt;frequency of appearance for each number&lt;/a&gt;. Considering that the lottery is random I wouldn’t rely on such a pattern…&lt;a href=&#34;#fnref1&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn2&#34;&gt;&lt;p&gt;In this blog post I only explore one aspect of the geometric PMF by looking at number of failures before the first success in a set of &lt;span class=&#34;math inline&#34;&gt;\(k \in \{0 , 1, 2, ...\}\)&lt;/span&gt; attempts. To read more about the PMF I recommend starting with the Wikipedia page of the &lt;a href=&#34;https://en.wikipedia.org/wiki/Geometric_distribution&#34;&gt;Geometric distribution&lt;/a&gt;.&lt;a href=&#34;#fnref2&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
      
            <category>probability</category>
      
      
            <category>R</category>
      
    </item>
    
    <item>
      <title>Learning Tfidf with Political Theorists</title>
      <link>https://amitlevinson.com/blog/learning-tfidf-with-political-theorists/</link>
      <pubDate>Sun, 31 May 2020 00:00:00 +0000</pubDate>
      
      <guid>https://amitlevinson.com/blog/learning-tfidf-with-political-theorists/</guid>
      <description>
&lt;script src=&#34;https://amitlevinson.com/rmarkdown-libs/kePrint/kePrint.js&#34;&gt;&lt;/script&gt;


&lt;style&gt;
p.caption {
  font-size: 0.9em;
}
&lt;/style&gt;
&lt;p&gt;Thanks to &lt;a href=&#34;https://almogsi.com/&#34;&gt;Almog Simchon&lt;/a&gt; for insightful comments on a first draft of this post.&lt;/p&gt;
&lt;div id=&#34;introduction&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Introduction&lt;/h3&gt;
&lt;p&gt;Learning &lt;code&gt;R&lt;/code&gt; for the past nine months or so has enabled me to explore new topics that are of interest to me, one of them being text analysis. In this post I’ll explain what is Term-Frequency Inverse Document Frequency (tf-idf) and how it can help us explore important words for a document within a corpus of documents&lt;a href=&#34;#fn1&#34; class=&#34;footnote-ref&#34; id=&#34;fnref1&#34;&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt;. The analysis helps in finding words that are common in a given document but are rare across all other documents.&lt;/p&gt;
&lt;p&gt;Following the explanation we’ll implement the method on four great philosophers’ books: ‘Republic’ (Plato), ‘The Prince’ (Machiavelli), ‘Leviathan’ (Hobbes) and lastly, one of my favorite books - ‘On Liberty’ (Mill) 😍. Lastly, we’ll see how tf-idf compares to a Bag of Words analysis (word count) and how using both can benefit your exploring of text.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;The post is aimed for anyone exploring text-analysis&lt;/strong&gt; and wants to learn about tf-idf. &lt;strong&gt;I will be using &lt;code&gt;R&lt;/code&gt; to analyze our data but won’t be explaining the different functions&lt;/strong&gt;, as this post focuses on the tf-idf analysis. If you wish to see the code, feel free to download or explore the .Rmd source code on my &lt;a href=&#34;https://github.com/AmitLevinson/amitlevinson.com/blob/master/content/post/learning-tfidf-with-political-theorists/index.Rmd&#34;&gt;github repository&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;term-frequency&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Term frequency&lt;/h3&gt;
&lt;p&gt;tf-idf gauges a word’s value according to two parameters: The first parameter is the &lt;strong&gt;term-frequency of a word: How common is a word in a given document&lt;/strong&gt; (Bag of Words analysis); one method to calculate term frequency of a word is just to count the total number of times each words appears. Another method - which we’ll use in the tf-idf - is, after summing the total number of times a word appears, we’ll divide it by the total number of words in that document, &lt;strong&gt;describing term frequency as such:&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[tf = \frac{\textrm{Number of times a word appears in a document}}{\textrm{Total number of words in that document}}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Also written as &lt;span class=&#34;math inline&#34;&gt;\(tf(t,d)\)&lt;/span&gt; where &lt;span class=&#34;math inline&#34;&gt;\(t\)&lt;/span&gt; is the number of times a term appears out of all words in document &lt;span class=&#34;math inline&#34;&gt;\(d\)&lt;/span&gt;. Using the above method we’ll have the &lt;strong&gt;proportion&lt;/strong&gt; of each word in our document, a value ranging from 0 to 1, where common words will have higher values.&lt;/p&gt;
&lt;p&gt;While this gives us a value gauging how common a word is in a document, what happens when we have many words across many documents? How do we find &lt;ins&gt;unique&lt;/ins&gt; words for each document? This brings us to &lt;em&gt;idf&lt;/em&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;inverse-document-frequency&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Inverse document frequency&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Inverse document frequency accounts for the occurrence of a word across all documents, thereby giving a higher value to words appearing in less documents.&lt;/strong&gt; In this case, for each term we will calculate the log ratio&lt;a href=&#34;#fn2&#34; class=&#34;footnote-ref&#34; id=&#34;fnref2&#34;&gt;&lt;sup&gt;2&lt;/sup&gt;&lt;/a&gt; of all documents divided by the number of documents that word appears in. This gives us the following:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ idf = \log {\frac{\textrm{N documents in corpus}}{\textrm{n documents containing the term}}}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Also written as &lt;span class=&#34;math inline&#34;&gt;\(idf = \log{\frac{N}{n(t)}}\)&lt;/span&gt; Where &lt;span class=&#34;math inline&#34;&gt;\(N\)&lt;/span&gt; is the total number of documents in our corpus and &lt;span class=&#34;math inline&#34;&gt;\(n(t)\)&lt;/span&gt; is the number of documents the word appears within our corpus of documents.&lt;/p&gt;
&lt;p&gt;To those unfamiliar, a logarithmic transformation helps in reducing wide-ranged numbers to smaller scopes. In this case, if we have 7 documents, and our term appears in all 7 documents, we’ll have following idf value: &lt;span class=&#34;math inline&#34;&gt;\(log_e(\frac{7}{7}) = 0\)&lt;/span&gt;. What if we have a term that appears in only 1 document out of all 7 documents? We’ll have the following: &lt;span class=&#34;math inline&#34;&gt;\(log_e(\frac{7}{1}) = 1.945\)&lt;/span&gt;. Even if a word appears in only 1 document out of 100, a logarithmic transformation will reduce its high value to mitigate bias when we multiply it with its &lt;span class=&#34;math inline&#34;&gt;\(tf\)&lt;/span&gt; value.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;So what do we understand from the idf?&lt;/strong&gt; Since our numerate always remains the same (N documents in corpus), the &lt;em&gt;idf&lt;/em&gt; of a word is contingent upon how common it is &lt;em&gt;across&lt;/em&gt; documents. Words that appear in a small number of documents will have a higher &lt;em&gt;idf&lt;/em&gt;, while words that are common across documents will have a lower &lt;em&gt;idf&lt;/em&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;term-frequency-inverse-document-frequency-tfidf&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Term-Frequency Inverse Document Frequency (tfidf)&lt;/h3&gt;
&lt;p&gt;Once we have the term frequency and inverse document frequency for each word we can calculate the tf-idf by multiplying the two: &lt;span class=&#34;math inline&#34;&gt;\(tf(t,d) \cdot idf(t,D)\)&lt;/span&gt; where &lt;span class=&#34;math inline&#34;&gt;\(D\)&lt;/span&gt; is our corpus of documents.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;To summarize our explanation:&lt;/strong&gt; The two paramteres used to calculate the tf-idf provide each word with a value for its importance to that document in that corpus of text. Ideally We take &lt;strong&gt;words that are &lt;u&gt;common within a document&lt;/u&gt; and that are &lt;u&gt;rare across documents&lt;/u&gt;&lt;/strong&gt;. I write ideally because as we’ll see soon, we might have words that are extremely common in one document but are filtered out because they’re evident in all documents (can happen in a small corpus of documents). This also highlights the question as to what is &lt;em&gt;important&lt;/em&gt;; I define important as contributing to understanding a document in comparison to all other documents.&lt;/p&gt;
&lt;div class=&#34;figure&#34; style=&#34;text-align: center&#34;&gt;&lt;span id=&#34;fig:unnamed-chunk-2&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;https://amitlevinson.com/post/learning-tfidf-with-political-theorists/index_files/figure-html/unnamed-chunk-2-1.png&#34; alt=&#34;Using tf-idf we can calculate how common a word is within a document and how rare is it across documents&#34; width=&#34;85%&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 1: Using tf-idf we can calculate how common a word is within a document and how rare is it across documents
&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;Now that we have some background as to how tf-idf works, let’s dive in to our case study.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;tf-idf-on-political-theorists.&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;TF-IDF on political theorists.&lt;/h3&gt;
&lt;p&gt;I’m a big fan of political theory. I have a small collection at home and always like to read and learn more about it. Except for Mill, we read Plato, Machiavelli and Hobbes in our BA first semester course in political theory. While some of the theorists overlap to some degree, over-all they discuss different topics. tf-idf will help us distinguish important words specific to each book, in a comparison across all books.&lt;/p&gt;
&lt;p&gt;Before we conduct our tf-idf we’d like to explore our text a bit. The following exploratory analysis is inspired from Julia Silge’s blog post &lt;a href=&#34;https://juliasilge.com/blog/term-frequency-tf-idf/&#34;&gt;‘Term Frequency and tf-idf Using Tidy Data Principles’&lt;/a&gt;, a fantastic read.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;data-collection-analysis&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Data collection &amp;amp; Analysis&lt;/h3&gt;
&lt;p&gt;The package we’ll use to gather the data is the &lt;code&gt;{gutenbergr}&lt;/code&gt; package. It enables us to access the &lt;a href=&#34;https://www.gutenberg.org/&#34;&gt;Project Gutenberg&lt;/a&gt; free books, a library of over 60,000 free books. As many other amazing things in &lt;code&gt;R&lt;/code&gt; someone, in this case David Robinson, created a package for it. All we need to do is download them to our computer.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;Mill &amp;lt;- gutenberg_download(34901)
Hobbes &amp;lt;- gutenberg_download(3207)
Machiavelli &amp;lt;- gutenberg_download(1232)
Plato &amp;lt;- gutenberg_download(150)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Several of the books contain sections at the beginning or at the end that aren’t relevant for our analysis. For example long introductions from contemporary scholars; another whole different book at the end, etc. These can confound our analysis and therefore we’ll exclude them. In order to conduct our analysis we also need all the books we collected in one object.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Once we are able to clean the books, this is what our text looks like:&lt;/strong&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;remove_text &amp;lt;- function(book, low_id, top_id = max(rowid), author = deparse(substitute(book))){
  book %&amp;gt;%
  mutate(author = as.factor(author)) %&amp;gt;% 
  rowid_to_column() %&amp;gt;% 
  filter(rowid &amp;gt;= {{low_id}}, rowid &amp;lt;= {{top_id}}) %&amp;gt;% 
  select(author, text, -c(rowid, gutenberg_id))}

books &amp;lt;- rbind(
  remove_text(Mill, 454),
  remove_text(Hobbes, 360, 22317),
  remove_text(Machiavelli, 464, 3790),
  remove_text(Plato, 606))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 45,490 x 2
##    author text                                                                  
##    &amp;lt;fct&amp;gt;  &amp;lt;chr&amp;gt;                                                                 
##  1 Mill   &amp;quot;&amp;quot;                                                                    
##  2 Mill   &amp;quot;&amp;quot;                                                                    
##  3 Mill   &amp;quot;CHAPTER I.&amp;quot;                                                          
##  4 Mill   &amp;quot;&amp;quot;                                                                    
##  5 Mill   &amp;quot;INTRODUCTORY.&amp;quot;                                                       
##  6 Mill   &amp;quot;&amp;quot;                                                                    
##  7 Mill   &amp;quot;&amp;quot;                                                                    
##  8 Mill   &amp;quot;The subject of this Essay is not the so-called Liberty of the Will, ~
##  9 Mill   &amp;quot;unfortunately opposed to the misnamed doctrine of Philosophical&amp;quot;     
## 10 Mill   &amp;quot;Necessity; but Civil, or Social Liberty: the nature and limits of th~
## # ... with 45,480 more rows&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Each row is some text with chapters separated by headings and a column referencing who is the author. Our data frame consists of ~45,000 rows with the filtered text from our four books. Tf-idf can also be done on any n-grams we choose (number of consequent words). We could calculate the tf-idf for each bigram of words (two-words), trigram, etc. I find a unigram an appropriate approach both for tf-idf and especially now when we want to learn more about it. &lt;strong&gt;We just saw that our text is in the form of sentences, so let’s break it into single words.&lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 12 x 4
## # Groups:   author [4]
##    author      word      n sum_words
##    &amp;lt;fct&amp;gt;       &amp;lt;chr&amp;gt; &amp;lt;int&amp;gt;     &amp;lt;int&amp;gt;
##  1 Hobbes      the   14536    207849
##  2 Hobbes      of    10523    207849
##  3 Hobbes      and    7113    207849
##  4 Plato       the    7054    118639
##  5 Plato       and    5746    118639
##  6 Plato       of     4640    118639
##  7 Mill        the    3019     48006
##  8 Mill        of     2461     48006
##  9 Machiavelli the    2006     34821
## 10 Mill        to     1765     48006
## 11 Machiavelli to     1468     34821
## 12 Machiavelli and    1333     34821&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We see that stop-words dominant the frequency of occurrences. That makes sense as they are commonly used, but they’re not usually helpful for learning about a text, specifically here. &lt;strong&gt;We’ll start by exploring how the word frequencies occur within a text:&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://amitlevinson.com/post/learning-tfidf-with-political-theorists/index_files/figure-html/unnamed-chunk-8-1.png&#34; width=&#34;672&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The plot above shows the frequency of terms across documents. We see some words that appear frequently (higher proportion = right side of the x-axis) and many words that are rarer (low proportion). Actually, I had to limit the x-axis or otherwise it would distort the plot with words that are extremely common.&lt;/p&gt;
&lt;p&gt;To help find useful words with the highest tf-idf from each book, we’ll remove stop words before we extract the words with a high tf-idf value:&lt;/p&gt;
&lt;table class=&#34;table&#34; style=&#34;width: auto !important; margin-left: auto; margin-right: auto;&#34;&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:center;&#34;&gt;
Author
&lt;/th&gt;
&lt;th style=&#34;text-align:center;&#34;&gt;
Word
&lt;/th&gt;
&lt;th style=&#34;text-align:center;&#34;&gt;
n
&lt;/th&gt;
&lt;th style=&#34;text-align:center;&#34;&gt;
Sum words
&lt;/th&gt;
&lt;th style=&#34;text-align:center;&#34;&gt;
Term Frequency
&lt;/th&gt;
&lt;th style=&#34;text-align:center;&#34;&gt;
IDF
&lt;/th&gt;
&lt;th style=&#34;text-align:center;&#34;&gt;
TF-IDF
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
Mill
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
opinion
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
150
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
48006
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.0094132
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.0000000
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.0000000
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
Hobbes
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
god
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
1047
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
207849
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.0149024
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.0000000
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.0000000
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
Machiavelli
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
prince
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
185
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
34821
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.0172704
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.2876821
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.0049684
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
Plato
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
true
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
485
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
118639
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.0152953
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.0000000
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.0000000
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;tfoot&gt;
&lt;tr&gt;
&lt;td style=&#34;padding: 0; border: 0;&#34; colspan=&#34;100%&#34;&gt;
&lt;sup&gt;&lt;/sup&gt; Random sample of words and their corresponding tf-idf values
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tfoot&gt;
&lt;/table&gt;
&lt;p&gt;Above we have our tf-idf for a given word from each document. I removed stop-words and calculated the tf-idf for each word in each book. For Hobbes the word ‘God’ appears 1047 times, thus has a &lt;span class=&#34;math inline&#34;&gt;\(tf\)&lt;/span&gt; of &lt;span class=&#34;math inline&#34;&gt;\(\frac {1047} {207849}\)&lt;/span&gt; and an idf of 0 (since it appears in all documents), so it’ll have a tf-idf of 0.&lt;/p&gt;
&lt;p&gt;With Machiavelli the word prince appears 185 times, with a &lt;span class=&#34;math inline&#34;&gt;\(tf\)&lt;/span&gt; of &lt;span class=&#34;math inline&#34;&gt;\(\frac {185} {34821}\)&lt;/span&gt;, resulting in a proportion of 0.0173. The word prince has an idf of 0.288 &lt;span class=&#34;math inline&#34;&gt;\((log_e(\frac 4 {3}))\)&lt;/span&gt;, as there are 4 documents and it appears in 3 of them, so a total tf-idf value of &lt;span class=&#34;math inline&#34;&gt;\(0.0173 \cdot 0.288\)&lt;/span&gt; = &lt;span class=&#34;math inline&#34;&gt;\(0.00497\)&lt;/span&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;tf-idf-plot&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Tf-idf plot&lt;/h3&gt;
&lt;p&gt;As we wrap up our tf-idf analysis, &lt;strong&gt;We don’t want to see all words and their tf-idf, but only words with the highest tf-idf value&lt;/strong&gt; for each author, indicating the importance of a word to a given document. We can look at these words by plotting the top 10 highest valued tf-idf words for each author:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt; ggplot(data = books_for_plot, aes(x = word, y = tf_idf, fill = author))+
  geom_col(show.legend = FALSE)+
  labs(x = NULL, y = &amp;quot;tf-idf&amp;quot;)+
  coord_flip()+
  scale_x_reordered()+
  facet_wrap(~ author, scales = &amp;quot;free_y&amp;quot;, ncol = 2)+
  labs(title = &amp;quot;&amp;lt;b&amp;gt;Term Frequency Inverse Document Frequency&amp;lt;/b&amp;gt; - Political theorists&amp;quot;,
       subtitle = &amp;quot;tf-idf for The Leviathan (Hobbes), On Liberty (Mill), The Prince (Machiavelli)\nand Republic (Plato)&amp;quot;)+
  scale_fill_manual(values = plot_colors)+
  theme_post+
  theme(plot.title = element_markdown())&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://amitlevinson.com/post/learning-tfidf-with-political-theorists/index_files/figure-html/unnamed-chunk-11-1.png&#34; width=&#34;672&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Lovely!&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Let’s review each book and see what we can learn from our tf-idf analysis. My memory of these books is kind of rusty but I’ll try my best:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Hobbes:&lt;/strong&gt; Hobbes in his book describes the &lt;em&gt;natural&lt;/em&gt; state of human beings and how they can leave it by revoking many of their right to the &lt;em&gt;sovereign&lt;/em&gt; who will facilitate order. In his book he describes the soveragin (note the ‘a’) as needed to be strict, rigorous and &lt;em&gt;hath&lt;/em&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Machiavelli:&lt;/strong&gt; Machiavelli provides a leader with a guide on how to rule his country. He prefaces his book with an introduction letter to the &lt;em&gt;Duke&lt;/em&gt;, the recipient of his work. Machiavelli throughout the book conveys his message with examples of many &lt;em&gt;princes&lt;/em&gt;, &lt;em&gt;Alexander&lt;/em&gt; the great, the &lt;em&gt;Orsini&lt;/em&gt; brothers and more. Several of his examples include mentioning of Italy (where he resides), specifically &lt;em&gt;Venetians&lt;/em&gt; and &lt;em&gt;Milan&lt;/em&gt;.&lt;br /&gt;
&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Mill:&lt;/strong&gt; Mill in his book ‘On Liberty’ describes the importance of freedom and liberty for individuals. He does so by describing the relation between people and their &lt;em&gt;society&lt;/em&gt; and other relations with the &lt;em&gt;social&lt;/em&gt;. He highlights in his discussion on liberty a &lt;em&gt;person’s&lt;/em&gt; belonging; these can be &lt;em&gt;Feelings&lt;/em&gt; or basically anything &lt;em&gt;personal&lt;/em&gt;. Protecting the personal is important for the &lt;em&gt;development&lt;/em&gt; of both society and that of the individual.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Plato:&lt;/strong&gt; Plato’s book consists of 10 chapters and it is by far the longest compared to the others. The book is written in the form of a dialogue with &lt;em&gt;replies&lt;/em&gt; between Socrate and his discussants. Along Socrate’s journey to finding out what is the meaning of justice he talks to many people, among them &lt;em&gt;Glaucon&lt;/em&gt;, &lt;em&gt;Thrasymachus&lt;/em&gt; and &lt;em&gt;Adeimantus&lt;/em&gt;. In one section Socrates describes a just society with distinct &lt;em&gt;classes&lt;/em&gt; such as the &lt;em&gt;guardians&lt;/em&gt;. The classes should receive appropriate education, for e.g. &lt;em&gt;gymnastics&lt;/em&gt; for the guardians.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;With the above analysis we were able to explore uniqueness of words for each book across all books. &lt;strong&gt;Some words provided us with great insights while others didn’t necessarily help us despite their uniqeness&lt;/strong&gt;, for example, the names of discussants with Socrate. Tf-idf gauges them as important (as to how I defined importance here) to distinguish between Plato’s book and the others, but I’m sure they’re not the first words that come to mind when someone talks about the Republic.&lt;/p&gt;
&lt;p&gt;The analysis also shows this methodology’s value addition is not in just applying tf-idf - or any other statistical analysis – rather its power lies in its explanatory abilities. In other words, &lt;strong&gt;tf-idf provides us with a value indicating the importance of a word to a given document within a corpus, it is our job to take that extra step interpreting and contextualizing the output.&lt;/strong&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;comparing-to-bag-of-words-bog&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Comparing to Bag Of Words (BOG)&lt;/h3&gt;
&lt;p&gt;A common text analysis is a word count I discussed earlier, also known as Bag of Words (BoW). This is an easy to understand method that can be done easily when exploring text. However, relying only on a bag of words method to draw insights can limit its usefulness if other analytic methods are not also included. The BoW relies only on the frequency of a word, so if a word is common across all documents, it might show up in all of them and not contribute to finding &lt;em&gt;unique words&lt;/em&gt; for each document.&lt;/p&gt;
&lt;p&gt;Now that we have our books we can also explore the raw occurrence of each word to compare it to our above tf-idf analysis:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(data = bow_books, aes(x = reorder(word_with_color,n), y = n, fill = author))+
  geom_col(show.legend = FALSE)+
  labs(x = NULL, y = &amp;quot;Word Frequency&amp;quot;)+
  coord_flip()+
  scale_x_reordered()+
  facet_wrap(~ author, scales = &amp;quot;free&amp;quot;, ncol = 2)+
  labs(title = &amp;quot;&amp;lt;b&amp;gt;Term Frequency&amp;lt;/b&amp;gt; - Political theorists&amp;quot;)+
  scale_fill_manual(values = plot_colors)+
  theme_post+
  theme(axis.text.y = element_markdown(),
        plot.title = element_markdown(),
        strip.text = element_text(color = &amp;quot;grey50&amp;quot;))&lt;/code&gt;&lt;/pre&gt;
&lt;div class=&#34;figure&#34; style=&#34;text-align: center&#34;&gt;&lt;span id=&#34;fig:unnamed-chunk-13&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;https://amitlevinson.com/post/learning-tfidf-with-political-theorists/index_files/figure-html/unnamed-chunk-13-1.png&#34; alt=&#34;Term frequency plot with words that are common across documents in bold&#34; width=&#34;672&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 2: Term frequency plot with words that are common across documents in bold
&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;&lt;strong&gt;The above plot amplifies, in my opinion, tf-idf’s contribution in finding unique words for each document.&lt;/strong&gt; While many of the words are similar to those we found in the previous tf-idf analysis, we also draw words that are common across documents. For example, we see the frequency of ‘Time’, ‘People’ and ‘Nature’ twice in different books and words such as ‘True’ and ‘Truth’ with similar meanings do so too (however this could have happened in tf-idf too).&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;However, the Bag of Words also provided new words we didn’t see earlier.&lt;/strong&gt; Here we can learn on new words like Power in Hobbes, Opinions in Mill and more. With the bag of words we get words that are common without controlling for other texts, while the tf-idf searches for words that are common within but are rare across.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;closing-remarks&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Closing remarks&lt;/h3&gt;
&lt;p&gt;In this post we learned the term frequency inverse document frequency (tf-idf) analysis and implemented it on four great political theorists. We finished by exploring tfidf in comparison to a bag of words analysis and showed the benefits of each. This also emphasizes how we define &lt;em&gt;important&lt;/em&gt;: Important to a document by itself or important to a document compared to other documents.
The definition of ‘important’ here also highlights tf-idf heuristic quantifying approach (&lt;a href=&#34;https://en.wikipedia.org/wiki/Tf%E2%80%93idf&#34;&gt;specifically the idf&lt;/a&gt;) and thus should be used with caution. If you are aware of theoretical development of it I’d be glad to read more about it.&lt;/p&gt;
&lt;p&gt;By now you should be equipped to give tf-idf a try yourself on a corpus of documents you find appropriate.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;where-to-next&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Where to next&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Further reading about text analysis - If you want to read more on text mining with R, I highly recommend the Julia Silge &amp;amp; David Robinson’s &lt;a href=&#34;https://www.tidytextmining.com/&#34;&gt;text mining with R book&lt;/a&gt;and/or exploring the &lt;a href=&#34;https://quanteda.io/&#34;&gt;&lt;code&gt;{quanteda}&lt;/code&gt;&lt;/a&gt; package.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Text datasets - As to finding text data, you can try the &lt;code&gt;{gutenbergr}&lt;/code&gt; package that gives access to thousands of books, a &lt;a href=&#34;https://github.com/rfordatascience/tidytuesday&#34;&gt;#TidyTuesday&lt;/a&gt; data set or collect tweets from Twitter using the &lt;code&gt;{rtweet}&lt;/code&gt; package.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Other posts of mine - If you’re interested in other posts of mine where I explore some text you can read my &lt;a href=&#34;https://amitlevinson.com/2020/04/20/israeli-elections-on-twitter/&#34;&gt;Israeli elections Twitter tweets analysis&lt;/a&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;That’s it for now. Feel free to contact me for any and all comments!&lt;/p&gt;
&lt;div id=&#34;notes&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Notes&lt;/h4&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;footnotes&#34;&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li id=&#34;fn1&#34;&gt;&lt;p&gt;A single document can be a book, chapter, paragraph or sentence, it all depends on your research and what you define as an ‘entity’ within a corpus of text.&lt;a href=&#34;#fnref1&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn2&#34;&gt;&lt;p&gt;&lt;strong&gt;What’s log ratio?&lt;/strong&gt; In general, and for the purpose of the tf-idf, a logarithm transformation (in short &lt;span class=&#34;math inline&#34;&gt;\(log\)&lt;/span&gt;) helps in reducing wide ranged numbers to smaller scopes. Assuming we have the following &lt;span class=&#34;math inline&#34;&gt;\(\log _{2}(16) = x\)&lt;/span&gt;, we ask ourselves (and calculate) 2 in the power of what (x) will give us 16. so in this case 2^3 will give us 16, which is basically written as &lt;span class=&#34;math inline&#34;&gt;\(\log _{2}(16) = 3\)&lt;/span&gt;. In order to generalize it, &lt;span class=&#34;math inline&#34;&gt;\(\log _{b}(x) = y\)&lt;/span&gt;, means b is the base we will raise to the power of y to reach x. Therefore written oppositely as &lt;span class=&#34;math inline&#34;&gt;\(b^y = x\)&lt;/span&gt;. The common uses of log are &lt;span class=&#34;math inline&#34;&gt;\(\log_2\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(\log_{10}\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(log_e\)&lt;/span&gt;, also written as plain log.&lt;a href=&#34;#fnref2&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
      
            <category>tidytext</category>
      
      
            <category>R</category>
      
    </item>
    
    <item>
      <title>Israeli elections on Twitter</title>
      <link>https://amitlevinson.com/blog/israeli-elections-on-twitter/</link>
      <pubDate>Mon, 20 Apr 2020 00:00:00 +0000</pubDate>
      
      <guid>https://amitlevinson.com/blog/israeli-elections-on-twitter/</guid>
      <description>


&lt;div id=&#34;introduction&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Introduction&lt;/h3&gt;
&lt;p&gt;Israel had its 3rd election within 12 months on March 2, 2020. This is because our Knesset - Hebrew term for house of representatives - wasn’t able to form or hold a government after each of the previous elections. As I won’t get into the politics of why they didn’t succeed in forming one (get it? politics 😉), I do want to take the opportunity and analyze some tweets posted in the time before and after the elections.&lt;br /&gt;
When we think of a data aggregating tweets, many questions arise - who, what, when, where and more about our data. Namely, with the collected data I want to answer the following questions:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;What was the frequency of tweets associated with the word ‘elections’?&lt;/li&gt;
&lt;li&gt;Who tweeted the most?&lt;/li&gt;
&lt;li&gt;What was the most common #Hashtag tweeted?&lt;/li&gt;
&lt;li&gt;Which tweet was most liked and which was retweeted the most?&lt;/li&gt;
&lt;li&gt;What were the most common words and bigrams (two words) in tweets?&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;div id=&#34;gathering-the-data&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Gathering the data &lt;i class=&#34;fab fa-twitter&#34;&gt;&lt;/i&gt;&lt;/h3&gt;
&lt;p&gt;Twitter’s API allows scraping &lt;strong&gt;6-9 days back for free&lt;/strong&gt;. Therefore, I scraped the data already on March 7, 2020 and saved it for later use.&lt;/p&gt;
&lt;p&gt;Let’s start with the packages we’ll use:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(rtweet)
library(tidyverse)
library(tidytext)
library(igraph)
library(hrbrthemes)
library(ggraph)
library(extrafont)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;I could use a consistent plot theme throughout the post but I’ll probably be editing each one a bit, while also some are not our regular graphs. With that said, There are some tweaks that will be consistent acorss several of the plots. Therefore, let’s create a theme function as a supplement to all other theme arguments I’ll use that will save a few lines of code:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mini_theme &amp;lt;- function(family = &amp;quot;Roboto Condensed&amp;quot;, tsize = 16) {
  theme_classic() +
  theme(text = element_text(family = family),
        axis.ticks = element_blank(),
        axis.line = element_blank(),
        plot.title = element_text(size = tsize))}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Next we’ll gather the tweets we need:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;elections_raw &amp;lt;- search_tweets(&amp;quot;בחירות&amp;quot;, n = 18000, retryonratelimit = TRUE)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To gather the tweets we can use the &lt;a href=&#34;https://rtweet.info/&#34;&gt;{rtweet}&lt;/a&gt; package which is amazing for collecting Twitter data. As I mentioned earlier, I already scraped the data a few days after the elections but left the command here to show what we did and how easy it is to do it. I searched only one term, ‘elections’ in Hebrew, and rtweet gathered all tweets containing that word.&lt;/p&gt;
&lt;p&gt;What did our search yield? Let’s have a look:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;dim(elections)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 16560    90&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;16,560 rows and 90 columns! As we can see, the &lt;code&gt;{rtweet}&lt;/code&gt; package brings back a lot of information!&lt;/p&gt;
&lt;div id=&#34;some-caveats&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Some Caveats:&lt;/h4&gt;
&lt;p&gt;Before we begin, I will say this post doesn’t aim to be representative of the discussions that were held during the election period. As a matter of fact, nor does it aim to be representative of the twitter discussions surrounding the elections. this is due to two main reasons:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;Twitter isn’t common in Israel at all. I’m not sure what’s the usage rate but it’s definitely not representative of the Israeli population.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;I searched for only one word - elections (in Hebrew) - which yielded some 16560 tweets. This is definitely not a large enough pool of tweets to claim for representation.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;With that said, the data gathered provides an opportunity to look at some Twitter data from the election period and motivate others to use the &lt;code&gt;{rtweet}&lt;/code&gt; package, so why not give it a go.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;tweet-frequency&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Tweet frequency&lt;/h3&gt;
&lt;p&gt;First, let’s see how the tweets distribute across the time span we searched for. we can create a quick time plot using the &lt;code&gt;ts_plot()&lt;/code&gt; argument from the &lt;code&gt;{rtweet}&lt;/code&gt; package:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;elections %&amp;gt;% 
  ts_plot(&amp;quot;2 hours&amp;quot;)+
  geom_line(size = 1, color = &amp;quot;black&amp;quot;)+
  mini_theme()+
  scale_x_datetime(date_breaks = &amp;quot;1 day&amp;quot;,date_labels = &amp;quot;%d %b&amp;quot;)+
  labs(x= NULL, y = NULL,
       title = &amp;quot;Tweet frequency throughout the Israeli elections week&amp;quot;,
       subtitle = &amp;quot;Tweets aggregated by two-hour interval. Only tweets containing the word &amp;#39;elections&amp;#39;\nin Hebrew were gathered&amp;quot;)+
  geom_text(aes(x = as.POSIXct(&amp;quot;2020-03-02 23:00:00&amp;quot;), y = 435, label = &amp;quot;10 PM:\nPolls close&amp;quot;),
            hjust = 0, size = 3, family = &amp;quot;Roboto Condensed&amp;quot;)+
  geom_vline(xintercept = as.POSIXct(&amp;quot;2020-03-02 22:00&amp;quot;),linetype = &amp;quot;dashed&amp;quot;, size = 0.5, color = &amp;quot;black&amp;quot;, alpha = 5/10)+
  theme(plot.subtitle = element_text(color = &amp;quot;gray70&amp;quot;))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://amitlevinson.com/post/elections-twitter/index_files/figure-html/unnamed-chunk-6-1.png&#34; width=&#34;672&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Interesting - we see the number of tweets during the closing time is equivalent to that of midday on March 4th. Most of the votes were counted by the end of March 3rd, so I can’t really put my finger on what this jump represents. After all, I collected tweets containing our word so it could have been that many people tweeted that specific term in that time slot. Anyway, I wasn’t able to find anything interesting that happened on the news that day but feel free to explore and offer suggestions.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;users-with-most-tweets&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Users with most tweets&lt;/h3&gt;
&lt;p&gt;Next, let’s look at who tweeted the most:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;elections %&amp;gt;% 
  count(screen_name, sort = T) %&amp;gt;% 
  slice(1:15) %&amp;gt;% 
  mutate(screen_name = reorder(screen_name,n)) %&amp;gt;% 
  ggplot(aes(x= screen_name, y= n))+
  geom_col(fill = &amp;quot;gray70&amp;quot;)+
  coord_flip()+
  scale_y_continuous(breaks = seq(0,180, 30), labels = seq(0,180,30))+
  labs(x = &amp;quot;Screen name&amp;quot;, y = &amp;quot;Number of tweets&amp;quot;, title = &amp;quot;Top 15 users tweeting the word &amp;#39;elections&amp;#39;&amp;quot;)+
  mini_theme()+
  theme(text = element_text(family = &amp;quot;Calibri&amp;quot;),
        axis.text = element_text(size = 12),
        axis.title.y = element_blank())&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://amitlevinson.com/post/elections-twitter/index_files/figure-html/unnamed-chunk-7-1.png&#34; width=&#34;768&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;We see that many news companies tweeted a lot using the word ‘elections’: ‘newisrael13’, ‘kann_news’, ‘MaarivOnline’, ‘RotterNews’, ‘bahazit_news’, ‘RotterNet’. I personnaly don’t recognize the rest, but on the other hand I use Twitter mostly to follow &lt;code&gt;R&lt;/code&gt; and academic related tweets, not necessarily Israeli politics.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;common-hashtags&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Common Hashtags&lt;/h3&gt;
&lt;p&gt;When using the &lt;code&gt;{rtweet}&lt;/code&gt; package to gather twitter data, one of the variables collected is the hashtags used in tweets. Although it doesn’t require too many lines of code to extract hashtags out of text, I think this is an amazing feature that shows the effort and details &lt;a href=&#34;https://mikewk.com/&#34;&gt;Michael W. Kearney&lt;/a&gt; and contributors put into the package.&lt;/p&gt;
&lt;p&gt;According to &lt;a href=&#34;https://en.wikipedia.org/wiki/Hashtag&#34;&gt;Wikipedia&lt;/a&gt;, a ‘Hashtag’ “is a type of metadata tag used on social networks such as Twitter and other microblogging services.”, that basically tags the message with a specific theme. This helps to see trends and themes in a macro level.&lt;/p&gt;
&lt;p&gt;OK then, let’s see what we have:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;hashtags &amp;lt;- elections %&amp;gt;% 
  select(hashtags) %&amp;gt;% 
  unlist() %&amp;gt;% 
  as.tibble() %&amp;gt;% 
  mutate(value = tolower(value)) %&amp;gt;% 
  count(value, name = &amp;quot;Count&amp;quot;, sort = T) %&amp;gt;%
  mutate(value = reorder(value, Count),
         iscorona = ifelse(value == &amp;quot;קורונה&amp;quot; | value == &amp;quot;coronavirus&amp;quot;, &amp;quot;y&amp;quot;, &amp;quot;n&amp;quot;)) %&amp;gt;% 
  filter(!is.na(value)) %&amp;gt;% 
  slice(1:20)

ggplot(data = hashtags, aes(x = Count, y = value, fill = iscorona))+
  geom_col(show.legend = FALSE)+
  scale_fill_manual(values = c(y = &amp;quot;#1DA1F2&amp;quot;, n = &amp;quot;gray70&amp;quot;))+
  labs(y = NULL, x = &amp;quot;Number of Tweets&amp;quot;, title = &amp;quot;Top 20 Hashtags addressing the Israeli elections&amp;quot;)+
  mini_theme()+
  theme(text = element_text(family = &amp;quot;Calibri&amp;quot;),
        axis.text = element_text(size = 12))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://amitlevinson.com/post/elections-twitter/index_files/figure-html/unnamed-chunk-9-1.png&#34; width=&#34;672&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The tweets include pretty much what we expect - hashtags about the elections - with the two leading ones being ‘elections’ and ‘elections2020’. We also see a peculiar hashtag ‘right_following_right_people’, and others such as ‘Netanyahu’ (the Prime minister at the time), ‘Israel’ and others.&lt;br /&gt;
I highlighted in blue an interesting hashtag at the time - &lt;font color=&#34;#1DA1F2&#34;&gt;&lt;strong&gt;Corona&lt;/strong&gt;&lt;/font&gt; (in hebrew) and &lt;font color=&#34;#1DA1F2&#34;&gt;&lt;strong&gt;coronavirus&lt;/strong&gt;&lt;/font&gt;. The elections were held on March 2, 2020, a little bit after the first cases reached Israel. Little did we know how it will affect us (I’m finalzing this post on April 18, 2020, and only now we’re starting to get back to routine. Slowly)&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;most-liked-and-retweeted&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Most liked and retweeted&lt;/h3&gt;
&lt;p&gt;Let’s have a look at which tweet was &lt;strong&gt;most liked&lt;/strong&gt;. Twitter doesn’t define it as ‘likes’ but as ‘favorite’, or at least in the data that is collected through the &lt;code&gt;{rtweet}&lt;/code&gt; package. Since I will want to gather the most of something - both favorite and later retweeted - I’ll create a function that will minimize re-writing the code.&lt;br /&gt;
&lt;br&gt;
The function takes in a variable, reorders our dataset according to the variable we declared, extracts the first row and then pulls (extracts) the status id of that tweet. Lastly, the &lt;code&gt;blogdown::shortcode&lt;/code&gt; enables to embed tweets, youtube videos and more into a blogdown post such as this, so we end the function by inserting our status id into that. For those just getting into functions notice that within the &lt;code&gt;arrange&lt;/code&gt; argument we insert our variable in two curly brackets {{}}. This is a powerful feature of &lt;code&gt;{rlang}&lt;/code&gt; when you want to manipulate a variable in a dataframe within a function. You can read more about that &lt;a href=&#34;https://www.tidyverse.org/blog/2019/06/rlang-0-4-0/&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;get_most &amp;lt;- function(var){
elections %&amp;gt;% 
  arrange(desc({{var}})) %&amp;gt;% 
  .[1,] %&amp;gt;% 
  pull(status_id) %&amp;gt;% 
  blogdown::shortcode(&amp;#39;tweet&amp;#39;,.)
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now Let’s see which tweet was &lt;strong&gt;most liked&lt;/strong&gt; during that week:&lt;/p&gt;
&lt;center&gt;
&lt;blockquote class=&#34;twitter-tweet&#34;&gt;&lt;p lang=&#34;iw&#34; dir=&#34;rtl&#34;&gt;יותר מהכל, אני שמח שלא יהיו עוד בחירות בשביל המשפחה שלי שסבלה בגבורה שנה ורבע. רעות עברי וענר 😍&lt;/p&gt;&amp;mdash; עמית סגל Amit Segal (@amit_segal) &lt;a href=&#34;https://twitter.com/amit_segal/status/1234584864415997952?ref_src=twsrc%5Etfw&#34;&gt;March 2, 2020&lt;/a&gt;&lt;/blockquote&gt;
&lt;script async src=&#34;https://platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;

&lt;/center&gt;
&lt;p&gt;The tweet is by ‘Amit Segal’ - an Israeli news reporter - and it says (my translation):&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;“More than anything, I’m glad there won’t be another elections for my family that suffered in honors a year and a quarter. Reut, Ivri and Aner 😍”&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Ha, interestingly he wrote it before the end of the elections, hopefully he’s right!&lt;/p&gt;
&lt;p&gt;Now let’s look at the &lt;strong&gt;most re-tweeted&lt;/strong&gt; tweet:&lt;/p&gt;
&lt;center&gt;
&lt;blockquote class=&#34;twitter-tweet&#34;&gt;&lt;p lang=&#34;iw&#34; dir=&#34;rtl&#34;&gt;אם ההקלטה של יועצו של גנץ מבושלת ושקרית (כדברי גנץ עכשיו), אז למה גנץ פיטר אותו?&lt;br&gt;&lt;br&gt;יועצו של גנץ פוטר בגלל שאמר את האמת שכולם יודעים: גנץ לא יכול להיות ראש ממשלה. אנחנו כן. עוד 2 מנדטים לליכוד ואנחנו מוציאים את המדינה מהפלונטר, מונעים עוד בחירות ומקימים ממשלה&lt;/p&gt;&amp;mdash; Benjamin Netanyahu (@netanyahu) &lt;a href=&#34;https://twitter.com/netanyahu/status/1233342393740603394?ref_src=twsrc%5Etfw&#34;&gt;February 28, 2020&lt;/a&gt;&lt;/blockquote&gt;
&lt;script async src=&#34;https://platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;

&lt;/center&gt;
&lt;p&gt;The tweet is by Benjamin Netanyahu, at the time the prime minister of Israel, who writes:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;“If the recording of Gantz’s advisor is orcherstrated and fabricated (according to Gantz’s words just now), why did Gantz fire him? Gantz’s advisor was fired because he said the truth everyone knows: Gantz can’t be a prime minister. We can. 2 more mandates to the Likkud and we are taking the country out of the plonter, preventing another election and form a government”&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;This came after the exposure of a secret recording of Gantz in a closed meeting, A few days before election day.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;wordcloud-and-bigrams&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Wordcloud and bigrams&lt;/h2&gt;
&lt;p&gt;Let’s have a look at two more text-related analyses:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;A word-cloud&lt;br /&gt;
&lt;/li&gt;
&lt;li&gt;Bigrams (two-words) from our text&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;We could try out more algorthims but I’ll save them for a different post (feel free to try on your own).&lt;/p&gt;
&lt;div id=&#34;wordcloud&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Wordcloud&lt;/h3&gt;
&lt;p&gt;In order to tackle the wordcloud, I’ll break up all the tweets into &lt;strong&gt;single words&lt;/strong&gt;, filter any Hebrew stop words (file found online) and all English words. The decision to filter English words is mainly because I’m interested in the Hebrew sentences, but also because most the common English words used in our data are those of Twitter user names cited when replying to a tweet:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;he_stopwords &amp;lt;- read_tsv(&amp;quot;https://raw.githubusercontent.com/gidim/HebrewStopWords/master/heb_stopwords.txt&amp;quot;, col_names = &amp;quot;word&amp;quot;)

election_token &amp;lt;- elections %&amp;gt;% 
  unnest_tokens(word, text) %&amp;gt;% 
  select(word) %&amp;gt;%
  anti_join(he_stopwords) %&amp;gt;% 
  count(word, sort = T) %&amp;gt;%
  filter(!grepl(&amp;quot;([a-z]+|בחירות)&amp;quot;, word), n&amp;gt;= 150)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now we can create a wordcloud of words appearing more than 150 times using &lt;code&gt;{wordcloud2}&lt;/code&gt; package&lt;a href=&#34;#fn1&#34; class=&#34;footnote-ref&#34; id=&#34;fnref1&#34;&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;wordcloud2::wordcloud2(election_token, color = &amp;quot;#1DA1F2&amp;quot;, shape = &amp;quot;circle&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;div class=&#34;figure&#34; style=&#34;text-align: center&#34;&gt;&lt;span id=&#34;fig:unnamed-chunk-13&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;wc.png&#34; alt=&#34;Wordcloud excludes Hebrew stop words and the word &#39;elections&#39;&#34; width=&#34;550&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 1: Wordcloud excludes Hebrew stop words and the word ‘elections’
&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;What we can see is many of the words we’d expect: Political candidates, government, fourth (in the context of fourth elections), partis’ names and more. I’ll provide a more thorough discussion following our bigram plot below, as I believe it addresses many of the same words.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;common-bigrams&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Common Bigrams&lt;/h3&gt;
&lt;p&gt;Like we did before, we can break up our text data into &lt;strong&gt;two word&lt;/strong&gt; observations, also known as bigrams. In order to account for all combinations, we break up the sentence to fit all possible options. For example, assume we have the following sentence:&lt;/p&gt;
&lt;p&gt;“Danny went to vote yesterday”&lt;/p&gt;
&lt;p&gt;Using the &lt;code&gt;unnest_tokens&lt;/code&gt; we’ll break the sentence into the following bigrams:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Danny went&lt;br /&gt;
&lt;/li&gt;
&lt;li&gt;went to&lt;br /&gt;
&lt;/li&gt;
&lt;li&gt;to vote&lt;br /&gt;
&lt;/li&gt;
&lt;li&gt;vote yesterday&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Which gives us all possible options. We will also include two columns consisting of the bigram broken up into single words. This will help in filtering out bigrams containing Hebrew stop words or English words. I’ll not run through the following code but instead will point you to &lt;a href=&#34;http://varianceexplained.org/&#34;&gt;David Ronbinson&lt;/a&gt; &amp;amp; &lt;a href=&#34;https://juliasilge.com/&#34;&gt;Julia Silge&lt;/a&gt; &lt;a href=&#34;https://www.tidytextmining.com/&#34;&gt;‘Text Mining with R’ Book&lt;/a&gt; for further reading.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;elec_bigram &amp;lt;- elections %&amp;gt;%
  select(text) %&amp;gt;% 
  unnest_tokens(bigram, text, token = &amp;quot;ngrams&amp;quot;, n = 2) %&amp;gt;%
  separate(bigram, into = c(&amp;quot;word1&amp;quot;, &amp;quot;word2&amp;quot;), sep = &amp;quot; &amp;quot;, remove = FALSE) %&amp;gt;% 
  filter(!word1 %in% he_stopwords$word,
         !word2 %in% he_stopwords$word,
         !grepl(&amp;quot;([a-z]+|בחירות)&amp;quot;, bigram)) %&amp;gt;% 
  count(word1, word2, sort = T) %&amp;gt;% 
  slice(1:45) %&amp;gt;%
  graph_from_data_frame()

p_arrow &amp;lt;- arrow(type = &amp;quot;closed&amp;quot;, length = unit(.1, &amp;quot;inches&amp;quot;))

ggraph(elec_bigram, layout = &amp;quot;fr&amp;quot;)+
  geom_edge_link(aes(edge_alpha = n), arrow = p_arrow, end_cap = circle(.04, &amp;quot;inches&amp;quot;), show.legend = FALSE)+
  geom_node_point(color = &amp;quot;lightblue&amp;quot;, size = 3)+
  geom_node_text(aes(label = name), vjust = 1, hjust = 1, family = &amp;quot;Calibri&amp;quot;)+
  theme_void()+
  labs(title = &amp;quot;Twitter text bigram&amp;quot;)+
  theme(text = element_text(family = &amp;quot;Calibri&amp;quot;),
        plot.title = element_text(hjust = 0.5 , face = &amp;quot;bold&amp;quot;, size = 18))&lt;/code&gt;&lt;/pre&gt;
&lt;div class=&#34;figure&#34; style=&#34;text-align: center&#34;&gt;&lt;span id=&#34;fig:unnamed-chunk-15&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;https://amitlevinson.com/post/elections-twitter/index_files/figure-html/unnamed-chunk-15-1.png&#34; alt=&#34;Word bigram excludes Hebrew stop words and the word &#39;elections&#39;&#34; width=&#34;672&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 2: Word bigram excludes Hebrew stop words and the word ‘elections’
&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;How should we read this graph?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;First off, We only plotted the 45 most common bigrams (out of 100,000+). Every word is connected to another word with an arrow pointing to a given direction. The direction to which the arrow points is the way to read that bigram. In addition, bolder lines represent a higher frequency of that bigram throughout all our text.&lt;br /&gt;
For example, on the bottom of our graph we see the number ‘2’ connected to the words ‘mandates’ and ‘campagin’. The direction of the arrow signals that we should read the bigram as ‘2 mandates’ and ‘2 campagins’.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;What does this all mean?&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;We have discussions regarding the &lt;strong&gt;number of chairs a govenrment will have (62/61/60/58)&lt;/strong&gt; connected to mentions of the number of election campaigns (2/3) we had, discussions of a united and/or minimal government and the forming of one in general.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;We see &lt;strong&gt;mentions of individuals&lt;/strong&gt; such as “Benjamin Netanyahu”, “Amit Segal” (Both we discussed earlier), “Natan Eshel”, &lt;strong&gt;but no mention of the main candidate running against Netanyahu - “Benny Gantz”&lt;/strong&gt;. That’s actually kind of odd, but more on that in a minute.&lt;br /&gt;
&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;We also see mentions of political parties such as “Meretz”, “Gesher” and “Labor” who ran together this time around, “Otzma Yehudit”, “United Torah Judaism”, and the “Joint List”. &lt;strong&gt;There’s no mention of the two leading parties - “Kahol Lavan” &amp;amp; “The Likkud”.&lt;/strong&gt;, despite the mentioning of the latter’s leader.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Mentions of Netanyahu’s indicment and the personal law associated him.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Mentions I’d categorize as ‘other’ such as “Terrorist supporters”, “Will of the people”, “Fake news”, &#34;Go vote’, etc.
&lt;br&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Actaully, this turned out more interesting than I thought. Several questions arose while looking at it: Several words are missing such as the main parties names (Likkud &amp;amp; Kahol-Lavan), The leading oponent running against Benjamin Netanyahu - Benny Gantz - and other questions such as with whom are specific terms associated. Before we close up I’ll look at one question that troubles me - &lt;strong&gt;Why doesn’t Gantz appear in our list&lt;/strong&gt; 😱?&lt;/p&gt;
&lt;div id=&#34;benny-gantzs-disappearance&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Benny Gantz’s disappearance&lt;/h4&gt;
&lt;p&gt;In order to see why Benny Gantz doesn’t appear in our bigram plot I’ll do the following: I’ll break the text into bigrams and filter to &lt;strong&gt;have only the bigrams containing the word Gantz&lt;/strong&gt;. Once we have that we can see why he doesn’t appear in our bigram plot despite appearing in our wordcloud.&lt;br /&gt;
Before I run the analysis and give you the answer think for a moment - What was the process of coming up with the bigram? If I chose only the 50 most frequent bigrams, why would a word that appears many times in our text not appear in our bigram list? Alternatively, did we filter anything along the way? Maybe even give the previous chunk another glance before I answer it.&lt;br /&gt;
&lt;br&gt;
Let’s have a look:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;gantz &amp;lt;-elections %&amp;gt;%
  select(text) %&amp;gt;% 
  unnest_tokens(bigram, text, token = &amp;quot;ngrams&amp;quot;, n = 2) %&amp;gt;%
  separate(bigram, into = c(&amp;quot;word1&amp;quot;, &amp;quot;word2&amp;quot;), sep = &amp;quot; &amp;quot;, remove = FALSE) %&amp;gt;% 
  filter(word1 %in% &amp;quot;גנץ&amp;quot; |
         word2 %in% &amp;quot;גנץ&amp;quot;,
         !grepl(&amp;quot;([a-z]+|בחירות)&amp;quot;, bigram))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The code is similar to what we did earlier only this time we left &lt;strong&gt;bigrams that match the word we want&lt;/strong&gt; - bigrams containing the word Gantz. Now that we have our list of bigrams, let’s look at the count of bigrams containing the word גנץ (‘Gantz’):&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;gantz %&amp;gt;% 
  count(bigram, sort = T)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 978 x 2
##    bigram         n
##    &amp;lt;chr&amp;gt;      &amp;lt;int&amp;gt;
##  1 של גנץ       160
##  2 בני גנץ      138
##  3 גנץ לא        90
##  4 על גנץ        70
##  5 את גנץ        69
##  6 עם גנץ        61
##  7 אם גנץ        41
##  8 גנץ היה       25
##  9 גנץ או        19
## 10 גנץ ליברמן    19
## # ... with 968 more rows&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;AHA!&lt;/strong&gt; Now I see what happened. The first bigram is a stop word and the word Gantz (‘Of Gantz’). The second bigram should have been included as it is Gantz’s full name - Benny Gantz, which appears 138 times.&lt;br /&gt;
So, why has it been filtered? This is a great question which we can answer if we look at our stop words we initially used. Let’s see if it has the word בני (‘benny’ in Hebrew):&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;he_stopwords %&amp;gt;% 
  filter(word == &amp;quot;בני&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 1 x 1
##   word 
##   &amp;lt;chr&amp;gt;
## 1 בני&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Yes it does. At the time of writing this blog post it leaves me in a dilemma - Should I change the stop words file I used to a different one or maybe create my own? Or should I continue as is? I think leaving it will teach me (and hopefully whoever read this far) a valuable lesson of always checking your stop words. In a different context the specific bigram wouldn’t have got me thinking, but here it didn’t make sense that our leading candidate was filtered, thus my inquire into what happened. In hebrew the word Benny also means ‘my son’, which I wouldn’t describe as a stop word but whoever made the dataset I guess did.&lt;/p&gt;
&lt;p&gt;If you wish to give it a try yourself, you can find the data in the form of an &lt;code&gt;.rds&lt;/code&gt; or smaller &lt;code&gt;.csv&lt;/code&gt; (excludes list columns) in my &lt;a href=&#34;https://github.com/AmitLevinson/amitlevinson.com/blob/master/content/post/elections-twitter&#34;&gt;github repository&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Well then, that’s all for now folks! &lt;strong&gt;And remember, make sure to validate your stop words dataset!&lt;/strong&gt;
&lt;br&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;footnotes&#34;&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li id=&#34;fn1&#34;&gt;&lt;p&gt;The function &lt;code&gt;wordcloud2&lt;/code&gt; we wrote wasn’t actually run because it renders an html object which distorts the post. Instead I used the webshot of our rendered html file, read more about that &lt;a href=&#34;https://www.r-graph-gallery.com/196-the-wordcloud2-library.html&#34;&gt;here&lt;/a&gt;.&lt;a href=&#34;#fnref1&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
      
            <category>rtweet</category>
      
            <category>tidytext</category>
      
      
            <category>R</category>
      
    </item>
    
    <item>
      <title>Making updating exam grades easy with R</title>
      <link>https://amitlevinson.com/blog/update-exam-grades-easy-with-r/</link>
      <pubDate>Sat, 15 Feb 2020 00:00:00 +0000</pubDate>
      
      <guid>https://amitlevinson.com/blog/update-exam-grades-easy-with-r/</guid>
      <description>
&lt;script src=&#34;https://amitlevinson.com/rmarkdown-libs/kePrint/kePrint.js&#34;&gt;&lt;/script&gt;


&lt;p&gt;&lt;strong&gt;TL;DR&lt;/strong&gt; Instead of each time searching for an id in the xlsx template the university provides we make our own xlsx and merge between the two. I then run through two options of either saving the new data frame as an &lt;code&gt;.xlsx&lt;/code&gt; using the &lt;code&gt;{xlsx}&lt;/code&gt; package, and I show another option where I extract the new column I need using &lt;code&gt;write_clip&lt;/code&gt; from the &lt;code&gt;{clipr}&lt;/code&gt; package.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;“Progress isn’t made by early risers. It’s made by lazy men trying to find easier ways to do something.” &lt;/br&gt; ― Robert Heinlein&lt;/p&gt;
&lt;/blockquote&gt;
&lt;div id=&#34;whats-the-story&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;What’s the story?&lt;/h3&gt;
&lt;p&gt;The other day I had to update students’ exams into a blank excel file. Every course exam each student gets an exam id. Their id is comprised from a number / number, for example, 26/1; 1/1; 42/15 and so forth. In our course of up to 70 students the left number goes all the way to the number of students in the exam class, and the right number goes up to 15 or 20 and starts again from 1.&lt;br /&gt;
This would make it easy to insert the grade for each id into the excel file that is already organized. However, since this is a new system and I was waiting to get access to download the excel I decided to open a new spreadsheet instead. Also, writing the id instead of looking it up in the excel file each time can save, in my opinion, a little time of searching.&lt;br /&gt;
So we have our spreadsheet which is not sorted, and we have the university’s spreadsheet which is sorted - how are we going to sync between them, considering our id column we wrote is recognized as a &lt;code&gt;character&lt;/code&gt; class? I know, let’s turn to &lt;code&gt;R&lt;/code&gt;&lt;a href=&#34;#fn1&#34; class=&#34;footnote-ref&#34; id=&#34;fnref1&#34;&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;looking-at-our-data&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Looking at our data&lt;/h3&gt;
&lt;p&gt;Let’s start off with loading our packages:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyverse)
# For reading xlsx files
library(readxl)
# To nicely display the tables in the following paragraph
library(kableExtra)
library(knitr)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now let’s read both files: Our spreadsheet with just the id and grade of each student we wrote in, and the other spreadsheet with the students’ id and a numerical vector to sort by that the university provides.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;messy &amp;lt;- read_excel(&amp;quot;messy_grades.xlsx&amp;quot;)
clean &amp;lt;- read_excel(&amp;quot;clean.xlsx&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This gives us the following tables where on the left we have our &lt;strong&gt;messy&lt;/strong&gt; table we wrote and on the right our &lt;strong&gt;clean&lt;/strong&gt; table we want to merge to:&lt;/p&gt;
&lt;table class=&#34;table&#34; style=&#34;width: auto !important; float: left; margin-right: 10px;&#34;&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:center;&#34;&gt;
id
&lt;/th&gt;
&lt;th style=&#34;text-align:center;&#34;&gt;
grade
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
67/13
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
94
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
56/2
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
90
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
68/14
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
84
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
63/9
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
100
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
55/1
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
89
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
62/8
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
97
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;table class=&#34;table&#34; style=&#34;width: auto !important; margin-right: 0; margin-left: auto&#34;&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:center;&#34;&gt;
id
&lt;/th&gt;
&lt;th style=&#34;text-align:center;&#34;&gt;
participated
&lt;/th&gt;
&lt;th style=&#34;text-align:center;&#34;&gt;
number_for_sorting
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
1/1
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
V
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
1
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
2/2
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
V
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
2
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
3/3
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
V
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
3
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
4/4
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
NA
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
4
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
5/5
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
NA
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
5
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
6/6
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
V
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
6
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;&lt;br&gt;
So we now have several options:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Join between the two tables, save the clean table as a new xlsx and upload it to the University’s exam system.&lt;/li&gt;
&lt;li&gt;Join between the two tables, clip the column with the organized grades and paste it into the university’s sorted excel file.&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;div id=&#34;option-1---merge-and-write-to-a-new-excel-file&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Option 1 - Merge and write to a new excel file&lt;/h3&gt;
&lt;p&gt;So the first option will be to merge the two tables into the clean one and save that as a new excel file using the &lt;code&gt;{xlsx}&lt;/code&gt; package:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;joined_tables &amp;lt;- messy %&amp;gt;% 
    right_join(clean)

xlsx::write.xlsx(joined_tables, &amp;quot;010210078-29012020C.xlsx&amp;quot;, showNA = FALSE)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Below is a screen shot of our new table:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://amitlevinson.com/img/ta_efficient/xl.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;However, going with this approach I encountered that the new .xlsx file is saved with a new column of id numbers that we see in the screenshot. We can just delete that column and have our file all ready to go.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;option-2---clip-the-sorted-column-into-the-excel-file&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Option 2 - Clip the sorted column into the excel file&lt;/h3&gt;
&lt;p&gt;This time around I’ll write a function for what we’ll be doing: I want to join the tables but this time around I want to clip the column I need and then manually paste it in the original template excel file:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;clip_grades &amp;lt;- function(messy, clean){
  messy %&amp;gt;% 
    right_join(clean) %&amp;gt;% 
    pull(grade) %&amp;gt;% 
    clipr::write_clip()
}

clip_grades(messy, clean)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;which gives us the following:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://amitlevinson.com/img/ta_efficient/clipgif.gif&#34; /&gt;&lt;/p&gt;
&lt;p&gt;That’s it!&lt;/p&gt;
&lt;p&gt;Well, more or less. We need to delete the ‘NA’ that are copied from the function. Unfortunately I wasn’t able to delete them from within &lt;code&gt;R&lt;/code&gt;, so I manually delete them.&lt;/p&gt;
&lt;p&gt;As to which option is better, I think the first option is more efficient as we only need to delete the id column. However, using the &lt;code&gt;{xlsx}&lt;/code&gt; package is dependent on &lt;code&gt;{rJava}&lt;/code&gt;and having java installed on the computer from what I encountered. Option two can be a little messy and possibly yield mistakes if we copy and paste the new grades and then manually delete the &lt;code&gt;NA&lt;/code&gt; - your call.&lt;/p&gt;
&lt;div id=&#34;so-what-did-i-learn-here&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;So what did I learn here?&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;How to read and write an excel file.&lt;/li&gt;
&lt;li&gt;Using the &lt;code&gt;write_clip&lt;/code&gt; function which is amazingly easy.&lt;/li&gt;
&lt;li&gt;How to make updating exams easier 💪&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;/br&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;footnotes&#34;&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li id=&#34;fn1&#34;&gt;&lt;p&gt;For confidentiality and other reasons I only left columns with information that can’t be linked to students (I also changed the grades altogether for this demonstration).&lt;a href=&#34;#fnref1&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
      
            <category>xlsx</category>
      
            <category>clipr</category>
      
      
            <category>R</category>
      
    </item>
    
    <item>
      <title>Visualizing Eliud Kichoge&#39;s new marathon record</title>
      <link>https://amitlevinson.com/blog/eliud-kichoge/</link>
      <pubDate>Fri, 07 Feb 2020 00:00:00 +0000</pubDate>
      
      <guid>https://amitlevinson.com/blog/eliud-kichoge/</guid>
      <description>


&lt;div class=&#34;figure&#34; style=&#34;text-align: center&#34;&gt;&lt;span id=&#34;fig:unnamed-chunk-1&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;new_record.jpg&#34; alt=&#34;_Eliud Kipchoge breaks the two-hour marathon barrier. Photo from the &#39;New York Times&#39;. Leonhard Foeger/Reuters _&#34; width=&#34;512&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 1: &lt;em&gt;Eliud Kipchoge breaks the two-hour marathon barrier. Photo from the ‘New York Times’. Leonhard Foeger/Reuters &lt;/em&gt;
&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;table-of-contents&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Table of Contents&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#background&#34;&gt;Background&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#retrieving-data-from-wikipedia&#34;&gt;Retrieving data from wikipedia&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#warngling-the-data&#34;&gt;Warngling the Data&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#plot&#34;&gt;Plot&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#ggimage&#34;&gt;ggimage&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#plot-aesthetics&#34;&gt;Plot Aesthetics&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#final-annotation&#34;&gt;Final annotation&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#appendix&#34;&gt;Appendix&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;background&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Background&lt;/h2&gt;
&lt;p&gt;On saturday October 12, 2019, Eliud Kipchoge broke (unofficially) the two-hour marathon barrier 🏆&lt;br /&gt;
I saw &lt;a href=&#34;https://twitter.com/neilfws/status/1182958246753009665&#34;&gt;Neil Saunders’ Twitter post&lt;/a&gt; visualizing the new record and wanted to try and reproduce it with runners instead of points. In this post I’ll walk through how I obtained the data from a Wikipedia page with &lt;code&gt;{rvest}&lt;/code&gt;, wrangled and tidied it and eventually plotted it using &lt;code&gt;{ggimage}&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;When I initially created the plot I mistakenly took the &lt;a href=&#34;https://en.wikipedia.org/wiki/Marathon_year_rankings&#34;&gt;Marathon year rankings&lt;/a&gt; from the Wikipedia webpage. That page showcases The yearly rankings and not the world records in general. In addition, I also changed the method of obtaining the data from first creating the plot to now. When I first did it I copied and pasted the table from Wikipedia into a &lt;code&gt;.csv&lt;/code&gt; file and worked with that. For that specific time point, where my experience with &lt;code&gt;R&lt;/code&gt; was extremly novice, I think it was adequate. This time around I gave scraping Wikipedia’s webpage a try which also renders a reproducible example.&lt;/p&gt;
&lt;p&gt;Let’s start with loading the packages we’ll need:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyverse)
library(rvest)
library(janitor)
library(lubridate)
library(ggimage)
library(hrbrthemes)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We’ll use &lt;code&gt;{tidyverse}&lt;/code&gt; for tidy manipulation and plotting, &lt;code&gt;{janitor}&lt;/code&gt; for cleaning the column names, &lt;code&gt;{lubridate}&lt;/code&gt; for working with dates, &lt;code&gt;{ggimage}&lt;/code&gt; for a plot with images and &lt;code&gt;{hrbrthemes}&lt;/code&gt; for a nice quick aesthetic theme.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;retrieving-data-from-wikipedia&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Retrieving data from wikipedia&lt;/h2&gt;
&lt;p&gt;In order to view the new record in comparison to other world records, We’ll turn to Wikipedia and see what we can find there.&lt;/p&gt;
&lt;div class=&#34;figure&#34; style=&#34;text-align: center&#34;&gt;&lt;span id=&#34;fig:wiki-ss&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;wikipage.png&#34; alt=&#34;_Wikipedia page of marathon world records_&#34; width=&#34;807&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 2: &lt;em&gt;Wikipedia page of marathon world records&lt;/em&gt;
&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;Here we can see that the webpage contains information about marathon records, where in the screenshot we see the men section. We only want the table with men’s records, so let’s get that:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#The Wikipage we&amp;#39;ll need
wiki_url &amp;lt;- &amp;quot;https://en.wikipedia.org/wiki/Marathon_world_record_progression&amp;quot;

runners_wiki &amp;lt;- wiki_url %&amp;gt;% 
  read_html() %&amp;gt;% 
  html_nodes(xpath=&amp;#39;//*[@id=&amp;quot;mw-content-text&amp;quot;]/div/table[1]&amp;#39;) %&amp;gt;% 
  html_table(fill = TRUE) %&amp;gt;% 
  as.data.frame()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Using the &lt;code&gt;{rvest}&lt;/code&gt; package we are able to scrape the Wikipedia page for the table we wanted. Frankly, this is the first time I used &lt;code&gt;rvest&lt;/code&gt;, but I found a good example from &lt;a href=&#34;https://r-tastic.co.uk/post/exploring-london-crime-with-r-heat-maps/&#34;&gt;Kasia Kulma’s blog post&lt;/a&gt; exploring London crime with R heat maps. I used the &lt;em&gt;SelctorGadget&lt;/em&gt; which identified the page’s content as “mw-content-text”. Using that id we looked for the tables (/div/table), specifically the first table &lt;code&gt;[1]&lt;/code&gt; of men world records we saw earlier in figure &lt;a href=&#34;#fig:wiki-ss&#34;&gt;2&lt;/a&gt;. Once we have the table we turn it into a dataframe for us to use.&lt;/p&gt;
&lt;p&gt;Alternatively, you can also use the following method to extract a table by extracting all tables from the Wikipedia page and choosing the first one:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;runners_wiki_alternative &amp;lt;- wiki_url %&amp;gt;% 
  read_html() %&amp;gt;%
  html_table(fill = TRUE) %&amp;gt;%
  .[[1]]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This option extracts all table from the html page using &lt;code&gt;html_table()&lt;/code&gt;. Using this on the whole page parses the html tables into data frames nesting within a list object. Like before, &lt;code&gt;{rvest}&lt;/code&gt; makes it easy for us and if the tables have inconsistent number of values it requires (or demands?) us to fill them. Once we have the tables in a list object we can extract the one we need using &lt;code&gt;.[[1]]&lt;/code&gt;. The &lt;code&gt;.&lt;/code&gt; acts as a place holder for the previous object passed, here a list of tables we scraped. The &lt;code&gt;[[1]]&lt;/code&gt; following that calls for the first object within the list, but in the form of its core class - data.frame. If we’d use one square bracket &lt;code&gt;[1]&lt;/code&gt; it would return an object with the original class from which it was drawn, in this case a list which is not good for us here since we want it as a dataframe to continue our data preparation.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;warngling-the-data&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Warngling the Data&lt;/h2&gt;
&lt;p&gt;Let’s look at our table to see what we have and what we’ll need to do:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;head(runners_wiki, n = 3)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##        Time          Name   Nationality              Date
## 1 2:55:18.4  Johnny Hayes United States     July 24, 1908
## 2 2:52:45.4 Robert Fowler United States   January 1, 1909
## 3 2:46:52.8   James Clark United States February 12, 1909
##                    Event.Place   Source
## 1       London, United Kingdom IAAF[53]
## 2  Yonkers,[nb 5]United States IAAF[53]
## 3 New York City, United States IAAF[53]
##                                                                                                                                                                                            Notes
## 1 Time was officially recorded as 2:55:18 2/5.[54]Italian Dorando Pietri finished in 2:54:46.4, but was disqualified for receiving assistance from race officials near the finish.[55] Note.[56]
## 2                                                                                                                                                                                      Note.[56]
## 3                                                                                                                                                                                      Note.[56]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;A little messey but that’s OK. What we’ll need to visualize Eliud Kipchoge’s record is the Name, Time and Date of all runners. We’ll start with cleaning our data:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;runners_clean &amp;lt;- runners_wiki %&amp;gt;% 
  clean_names() %&amp;gt;% 
  select(1,2,4)
str(runners_clean)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## &amp;#39;data.frame&amp;#39;:    50 obs. of  3 variables:
##  $ time: chr  &amp;quot;2:55:18.4&amp;quot; &amp;quot;2:52:45.4&amp;quot; &amp;quot;2:46:52.8&amp;quot; &amp;quot;2:46:04.6&amp;quot; ...
##  $ name: chr  &amp;quot;Johnny Hayes&amp;quot; &amp;quot;Robert Fowler&amp;quot; &amp;quot;James Clark&amp;quot; &amp;quot;Albert Raines&amp;quot; ...
##  $ date: chr  &amp;quot;July 24, 1908&amp;quot; &amp;quot;January 1, 1909&amp;quot; &amp;quot;February 12, 1909&amp;quot; &amp;quot;May 8, 1909&amp;quot; ...&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The &lt;code&gt;clean_names&lt;/code&gt; function cleans the column names making them easier to use. I then picked the columns we’ll need using &lt;code&gt;select&lt;/code&gt;. Lastly, we want to look at the variables structure to see if we they need any manipulations. Yes, it seems both the time and date are not recognized appropriately (In this case they’re characters) - let’s fix that:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;runners_mutate &amp;lt;- runners_clean %&amp;gt;% 
  add_row(time = &amp;quot;1:59:40&amp;quot;, name = &amp;quot;Eliud Kipchoge&amp;quot;, date = &amp;quot;November 12, 2019&amp;quot;) %&amp;gt;% 
  mutate(run_period_raw = hms(time),
         run_duration = as.numeric(run_period_raw, &amp;quot;minutes&amp;quot;),
         run_year = year(mdy(date))) %&amp;gt;% 
  select(c(-date,-time))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning: 1 failed to parse.&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let’s see what we did here. First I add Eliud Kipchoge’s new unofficial record as an observation into our dataframe. I then turned to the &lt;code&gt;{lubridate}&lt;/code&gt; package where I used the &lt;code&gt;hms&lt;/code&gt; function to mutate the time variable we had into a new variable called ‘r_period_raw’. Although this cleans the variable, &lt;code&gt;hms&lt;/code&gt; transforms it into a &lt;code&gt;period&lt;/code&gt; object which I found a little difficult to use when we want to plot. What we need is to turn it into a numeric class which we did in our new variable ‘run_duration’. This will help us in plotting but I retained the period class variable as it makes it easier to read in this case.&lt;/p&gt;
&lt;p&gt;I then turned the date column into a Month-Day-Year variable using the &lt;code&gt;mdy&lt;/code&gt; function, which eventually I only extracted the year using &lt;code&gt;year&lt;/code&gt;. Lastly I discarded the old columns we don’t need anymore. We also recieved a warning sign that one observation didn’t parse. This was because the value in the cell didn’t match the pattern of the &lt;code&gt;hms&lt;/code&gt; fuction. The original pattern looked like this: May 26, 1909[nb 6]. All we want is the specific year which we’ll probably anyway filter later so it’s no big deal, but let’s go ahead and manually add it if we decide to use it later:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;runners_mutate[5,4] &amp;lt;- 1909&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This brings us the following:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(runners_mutate,aes(x = run_year, y = run_duration))+
  geom_point()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://amitlevinson.com/post/eliud-kichoge/index_files/figure-html/unnamed-chunk-9-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Great, that’s a good start. Now we want to make it a little less crowded so we can easily insert an image of runners instead of points and not have it cluttered. In order to do that we’ll look at each several years and lastly at 2019, the current record. First, let’s look at the years we have:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;runners_mutate %&amp;gt;% 
  pull(run_year)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  [1] 1908 1909 1909 1909 1909 1909 1913 1913 1914 1920 1925 1929 1935 1935 1935
## [16] 1935 1947 1952 1953 1953 1954 1956 1958 1960 1963 1963 1963 1964 1964 1965
## [31] 1967 1969 1970 1974 1978 1980 1981 1984 1985 1988 1998 1999 2002 2003 2007
## [46] 2008 2011 2013 2014 2018 2019&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Using the &lt;code&gt;pull&lt;/code&gt; function we were able to extract the column we wanted, much similar to using the &lt;code&gt;runners_mutate$column_name&lt;/code&gt; approach. ‘Unfortunately’, we can’t filter exactly by round intervals (for example every exact 10 years) so we’ll create a vector with specific years to filter by. Although it might sound trivial, make sure you’re assigning years that are observed in your data set, otherwise it’ll filter only by the years you do have and not those you don’t.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;year_sub &amp;lt;- c(1908, 1920, 1929, 1947, seq(1960,1980,10), 1999, 2011, 2019)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here we created a vector with values for every 15+- years. Now we can filter our new dataframe according to the years we want:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;runners_mutate &amp;lt;- runners_mutate %&amp;gt;% 
 filter(run_year %in% year_sub)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Using the &lt;code&gt;filter&lt;/code&gt; function with &lt;code&gt;%in%&lt;/code&gt; we discard anything from the &lt;code&gt;run_year&lt;/code&gt; column that’s not in the &lt;code&gt;year_sub&lt;/code&gt; vector. I find &lt;code&gt;%in%&lt;/code&gt; facsinating and extremely helpful when you want to look/filter several parameters. Basically, you can read it as “Keep all rows in ‘run_year’ that match values in ‘year_sub’”.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;plot&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Plot&lt;/h2&gt;
&lt;div id=&#34;ggimage&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;ggimage&lt;/h3&gt;
&lt;p&gt;In order for us to plot a runner icon instead of points we need to load the images into our data frame as values for each observation. To do that we use the &lt;code&gt;{ggimage}&lt;/code&gt; package which we’ll also use for the plot.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;runners_mutate &amp;lt;- runners_mutate %&amp;gt;% 
  mutate(run_image = &amp;quot;run.png&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;And now let’s look at our new plot:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;g &amp;lt;- ggplot(runners_mutate, aes(x = run_year, y = run_duration))+
  geom_image(aes(image = run_image), size = 0.05)+
  theme_ipsum_rc()
g&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://amitlevinson.com/post/eliud-kichoge/index_files/figure-html/unnamed-chunk-14-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Not bad. I like the icons although the whole graph might be a bit misleading if readers perceive that these are the only records there are. However, this is a tutorial and we’ll also add that note into our plot momentarily. You can adjust the size and other parameters of the images we plot, here for example I chose to adjust the size from its default. I also added &lt;code&gt;theme_ipsum_rc&lt;/code&gt; from the &lt;code&gt;{hrbrthemes}&lt;/code&gt; package for a quick aesthetic theme I like.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;plot-aesthetics&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Plot Aesthetics&lt;/h3&gt;
&lt;p&gt;So the plot so far looks nice, but we want it to be aesthetic and also to easily understand the progress of records across years. In order to do that, let’s turn to adjust both the y and x axis, and following that add some information to understand what we’re looking at:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;g1 &amp;lt;-  g +
    scale_x_continuous(name = &amp;quot;Year&amp;quot;,
                        limits = c(1900,2025),
                        breaks = seq(1900,2020,10),
                        labels = c(&amp;quot;1900&amp;quot;, paste0(&amp;quot;&amp;#39;&amp;quot;, seq(10,90,10)),&amp;quot;2000&amp;quot;,
&amp;quot;&amp;#39;10&amp;quot;,&amp;quot;&amp;#39;19&amp;quot;))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here we added some nice x labels in a format that’s both concise and informative. I remember taking this from &lt;a href=&#34;https://github.com/LiamDBailey/TidyTuesday/blob/master/R/17_09_2019.R&#34;&gt;Liam Bailey’s&lt;/a&gt; &lt;a href=&#34;https://github.com/rfordatascience/tidytuesday&#34;&gt;#Tidytuesday&lt;/a&gt; plot a while back when i first made this visualiztion. What we did was teak the &lt;code&gt;scale_x_continuous&lt;/code&gt; by assigning a &lt;code&gt;name&lt;/code&gt; to the axis, expanding its &lt;code&gt;limits&lt;/code&gt;, added specific &lt;code&gt;breaks&lt;/code&gt; and then a &lt;code&gt;label&lt;/code&gt; for each break using &lt;code&gt;paste0&lt;/code&gt;. Note that you must have the same number of labels and breaks for the plot to render so it’s important to have the sequences identical in length; otherwise it’ll return an error. With the &lt;code&gt;paste0&lt;/code&gt; we can add any value or observation and then ‘stick’ to it whatever else we want. Using that we are able to create years in the format of ’10 and so on. It is also possible to use the &lt;code&gt;{glue}&lt;/code&gt; package which I heard is very intuitive, maybe next post I’ll give that a try.&lt;/p&gt;
&lt;p&gt;Next, let’s change our y duration axis:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;g2 &amp;lt;- 
  g1 + scale_y_time(name = &amp;quot;Time (hours)&amp;quot;,
                  limits = c(100,180),
                  breaks = seq(100,180,10),
                  labels = c(&amp;quot;1:40&amp;quot;,&amp;quot;1:50&amp;quot;, &amp;quot;2:00&amp;quot;, &amp;quot;2:10&amp;quot;,&amp;quot;2:20&amp;quot;,&amp;quot;2:30&amp;quot;,&amp;quot;2:40&amp;quot;, &amp;quot;2:50&amp;quot;, &amp;quot;3:00&amp;quot;))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If you recall, we previously mutated the column we read from Wikipedia into a period class and a duration of minutes. using the &lt;code&gt;scale_*_time&lt;/code&gt; (either x or y instead of *) we can work with an &lt;code&gt;hms&lt;/code&gt; object. What we did is add a &lt;code&gt;name&lt;/code&gt;, expand a little the &lt;code&gt;limits&lt;/code&gt;, add &lt;code&gt;breaks&lt;/code&gt; and &lt;code&gt;labels&lt;/code&gt; same as before. This time around we used our breaks as minutes, so every 60 minutes represents an hour. I initially used hours as the numeric value, but then it makes it harder to break every 10 minutes (that’ll mean breaks every 0.166…). For the labels I was having some problems automating it so I comprimised on manually inputting it; I guess sometimes you just have to choose your battles between automating and manualy inserting.&lt;/p&gt;
&lt;p&gt;Let’s finish up by adding a title, subtitle and integrating last aesthetics to our plot.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;g3 &amp;lt;- g2 +
  labs(title = &amp;quot;How does Eliud Kipchoge marathon score compare to previous yearly records?&amp;quot;,
       subtitle = &amp;quot;Points are world records for every 10-15 years. \nEliud Kipchoge is the first to break the two-hour barrier (unofficially), Great job!&amp;quot;)+
    theme(
      panel.grid.minor = element_blank(),
      panel.grid.major = element_line(colour = &amp;quot;gray75&amp;quot;, size = 0.1, linetype = &amp;quot;dashed&amp;quot;),
      plot.title = element_text(size = 14),
      plot.subtitle = element_text(size = 10)
      )&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;After adding some labs I tweaked a bit the gridlines using &lt;code&gt;panel.grid&lt;/code&gt; minor or major. You can play around with them to see which minimilize your plot in the perfect way. I chose to leave the major grid lines since I find it easier to read the values with them. Although we defined a theme earlier on we can still tweak it by adding another &lt;code&gt;theme&lt;/code&gt; argument to the previous one as we just did.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;final-annotation&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Final annotation&lt;/h3&gt;
&lt;p&gt;Lastly, we want the new record to be evident and stand out in a first glance. Here I was somewhat debating between using a regular &lt;code&gt;geom_point&lt;/code&gt; instead of the &lt;code&gt;geom_image&lt;/code&gt; because then we could easily use a vertical line to highlight the 2:00 hour threshold. Since a line in this case will cut right through the icon, let’s use an arrow annotation instead.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;g4 &amp;lt;- g3 +
  geom_curve(aes(x = 2018, y = 120, xend = 2015, yend = 113),
             colour = &amp;quot;black&amp;quot;, size = 0.9, curvature = 0.5,
             arrow = arrow(length = unit(2,&amp;quot;mm&amp;quot;), type = &amp;quot;closed&amp;quot;))+
  annotate(&amp;quot;text&amp;quot;, x=2010, y= 105, 
           label = &amp;quot;Eliud Kipchoge\n12.10.2019\n1:59:40&amp;quot;,
           color = &amp;quot;black&amp;quot;, size = 3, hjust = 0)

g4&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://amitlevinson.com/post/eliud-kichoge/index_files/figure-html/unnamed-chunk-18-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;And voila!&lt;/p&gt;
&lt;p&gt;In our final touches we added both an arrow and text to explain what we’re seeing. I decided to go with a &lt;code&gt;geom_curve&lt;/code&gt; arrow where we can set the start and end of the arrow along with the kind of curve we want. We then set the curve to be &lt;code&gt;arrow&lt;/code&gt; and adjust its length. You can also use a closed head arrow, for more on that read on &lt;code&gt;?arrow&lt;/code&gt; as part of the &lt;code&gt;geom_curve&lt;/code&gt; or &lt;code&gt;geom_segment&lt;/code&gt; you can use here.&lt;/p&gt;
&lt;p&gt;That’s it, seems like were good to go. &lt;strong&gt;Great job for Elihud Kipchoge&lt;/strong&gt; 👏&lt;br /&gt;
&lt;/br&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;appendix&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Appendix&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;When I initially created this visulization I was just starting with &lt;code&gt;R&lt;/code&gt;. I first created 11 slots, added 1921, the sequence of 1930-2010 and then a 2019 (reminder: When I first created this viz I took a different dataset altogether). Little did I know how to properly use the &lt;code&gt;c()&lt;/code&gt; function that we used in the current post.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;year.sub &amp;lt;- vector (&amp;quot;double&amp;quot;, 11)
year.sub[1] &amp;lt;- 1921
year.sub[2:10] &amp;lt;- seq(1930,2010,10)
year.sub[11] &amp;lt;- 2019&lt;/code&gt;&lt;/pre&gt;
&lt;center&gt;
&lt;a href=&#34;#top&#34;&gt; Back to top&lt;/a&gt;
&lt;/center&gt;
&lt;/div&gt;
</description>
      
            <category>ggimage</category>
      
            <category>rvest</category>
      
      
            <category>R</category>
      
    </item>
    
    <item>
      <title>Mapping bomb shelters in Be&#39;er-Sheva, IL</title>
      <link>https://amitlevinson.com/blog/bomb-shelters/</link>
      <pubDate>Tue, 14 Jan 2020 00:00:00 +0000</pubDate>
      
      <guid>https://amitlevinson.com/blog/bomb-shelters/</guid>
      <description>
&lt;script src=&#34;https://amitlevinson.com/rmarkdown-libs/htmlwidgets/htmlwidgets.js&#34;&gt;&lt;/script&gt;
&lt;script src=&#34;https://amitlevinson.com/rmarkdown-libs/jquery/jquery.min.js&#34;&gt;&lt;/script&gt;
&lt;link href=&#34;https://amitlevinson.com/rmarkdown-libs/leaflet/leaflet.css&#34; rel=&#34;stylesheet&#34; /&gt;
&lt;script src=&#34;https://amitlevinson.com/rmarkdown-libs/leaflet/leaflet.js&#34;&gt;&lt;/script&gt;
&lt;link href=&#34;https://amitlevinson.com/rmarkdown-libs/leafletfix/leafletfix.css&#34; rel=&#34;stylesheet&#34; /&gt;
&lt;script src=&#34;https://amitlevinson.com/rmarkdown-libs/Proj4Leaflet/proj4-compressed.js&#34;&gt;&lt;/script&gt;
&lt;script src=&#34;https://amitlevinson.com/rmarkdown-libs/Proj4Leaflet/proj4leaflet.js&#34;&gt;&lt;/script&gt;
&lt;link href=&#34;https://amitlevinson.com/rmarkdown-libs/rstudio_leaflet/rstudio_leaflet.css&#34; rel=&#34;stylesheet&#34; /&gt;
&lt;script src=&#34;https://amitlevinson.com/rmarkdown-libs/leaflet-binding/leaflet.js&#34;&gt;&lt;/script&gt;


&lt;div id=&#34;update-from-march-21-2020&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;&lt;a id=&#34;update&#34;&gt;&lt;/a&gt;&lt;strong&gt;Update from March 21, 2020&lt;/strong&gt;&lt;/h4&gt;
&lt;p&gt;I’ve been wanting to return to this post and make this map more interactive. As a matter of fact it was easier than I thought, I just never got around to doing it. I won’t be going through the code for the leaflet map below but will leave it for whoever would like to review it:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(leaflet)
library(magrittr)
readr::read_csv(&amp;quot;shelters.csv&amp;quot;) %&amp;gt;% 
leaflet() %&amp;gt;% 
  addTiles() %&amp;gt;% 
  setView(34.7913, 31.25181,zoom = 13) %&amp;gt;% 
  addCircles(radius = 4, color = &amp;quot;red&amp;quot;, fill = TRUE)&lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;htmlwidget-1&#34; style=&#34;width:672px;height:480px;&#34; class=&#34;leaflet html-widget&#34;&gt;&lt;/div&gt;
&lt;script type=&#34;application/json&#34; data-for=&#34;htmlwidget-1&#34;&gt;{&#34;x&#34;:{&#34;options&#34;:{&#34;crs&#34;:{&#34;crsClass&#34;:&#34;L.CRS.EPSG3857&#34;,&#34;code&#34;:null,&#34;proj4def&#34;:null,&#34;projectedBounds&#34;:null,&#34;options&#34;:{}}},&#34;calls&#34;:[{&#34;method&#34;:&#34;addTiles&#34;,&#34;args&#34;:[&#34;//{s}.tile.openstreetmap.org/{z}/{x}/{y}.png&#34;,null,null,{&#34;minZoom&#34;:0,&#34;maxZoom&#34;:18,&#34;tileSize&#34;:256,&#34;subdomains&#34;:&#34;abc&#34;,&#34;errorTileUrl&#34;:&#34;&#34;,&#34;tms&#34;:false,&#34;noWrap&#34;:false,&#34;zoomOffset&#34;:0,&#34;zoomReverse&#34;:false,&#34;opacity&#34;:1,&#34;zIndex&#34;:1,&#34;detectRetina&#34;:false,&#34;attribution&#34;:&#34;&amp;copy; &lt;a href=\&#34;http://openstreetmap.org\&#34;&gt;OpenStreetMap&lt;\/a&gt; contributors, &lt;a href=\&#34;http://creativecommons.org/licenses/by-sa/2.0/\&#34;&gt;CC-BY-SA&lt;\/a&gt;&#34;}]},{&#34;method&#34;:&#34;addCircles&#34;,&#34;args&#34;:[[31.259018768,31.2597950910001,31.2592493760001,31.2573049280001,31.2574797520001,31.258424086,31.254246294,31.255173057,31.2556456460001,31.253666206,31.2537330280001,31.2569812850001,31.2570752350001,31.257608025,31.2575462630001,31.2700855640001,31.2702905,31.2695012190001,31.2658482960001,31.259725431,31.2646111440001,31.266921333,31.2604320470001,31.2596058720001,31.262762868,31.263789048,31.261942934,31.2638102080001,31.2632644710001,31.263504445,31.262432614,31.2708580180001,31.2711996570001,31.270725645,31.2712747300001,31.2718630300001,31.2720855330001,31.2726126790001,31.272427384,31.2704051920001,31.266736774,31.2628119170001,31.2455927190001,31.248994151,31.250188747,31.2459192310001,31.24676858,31.2479923820001,31.2478412890001,31.249363784,31.250175772,31.248725547,31.248610904,31.251846362,31.2518736100001,31.250946067,31.253505949,31.257884104,31.2540270010001,31.2559118880001,31.2710156910001,31.271205497,31.270356722,31.269982842,31.2698227120001,31.269304209,31.269990116,31.2687801060001,31.2682527530001,31.2683451450001,31.270695158,31.27323147,31.2733343200001,31.2737922810001,31.2728741080001,31.274249592,31.2746572610001,31.272394175,31.2739432400001,31.2744760080001,31.2717045570001,31.2729638220001,31.2757156430001,31.2741278410001,31.2746324350001,31.274596902,31.271494137,31.2718457610001,31.275141078,31.275589424,31.2759297440001,31.2649303200001,31.270103943,31.2659409890001,31.2669887380001,31.2697833300001,31.2704315440001,31.266575474,31.266131746,31.221500807,31.224164194,31.223587228,31.2258336940001,31.222024801,31.2197570490001,31.221235918,31.224912993,31.224000586,31.2261810250001,31.223021734,31.2238066,31.2209769780001,31.222977471,31.2321786260001,31.2324369340001,31.2431229220001,31.235916617,31.238806793,31.237065128,31.248259068,31.251491909,31.2504392380001,31.2504933070001,31.2518083160001,31.246153377,31.250630739,31.25563615,31.256696517,31.254957205,31.257290099,31.25460144,31.256567933,31.255031301,31.2575173870001,31.2579245330001,31.2579056490001,31.2226565670001,31.224564345,31.2426843960001,31.240248009,31.2399610090001,31.267945217,31.2638298580001,31.270236595,31.269875347,31.265144816,31.2690648330001,31.2681699770001,31.269377084,31.265713315,31.270953035,31.2722040720001,31.2645033750001,31.271228369,31.2715811,31.256697924,31.257101161,31.269666226,31.275281239,31.274241079,31.269109099,31.272405753,31.2598086400001,31.250707328,31.251678113,31.2525321090001,31.2486613240001,31.2554510690001,31.2490194850001,31.2670087990001,31.2682888350001,31.269309503,31.250801417,31.268097267,31.2679672850001,31.267170004,31.261017135,31.2604871560001,31.261171405,31.260286697,31.260856463,31.258957008,31.2669088850001,31.264312395,31.2644250700001,31.262675503,31.262628384,31.266354108,31.2378694280001,31.237196182,31.2522976180001,31.2551276230001,31.2555366730001,31.271009238,31.2697859260001,31.248898803,31.250194783,31.250113594,31.248703956,31.2482595930001,31.247643195,31.252913957,31.2562439010001,31.250285107,31.2561600820001,31.2510416860001,31.249713179,31.258150351,31.2590159170001,31.2581197600001,31.25558649,31.2571552970001,31.254918092,31.2545320300001,31.2465726200001,31.2520725430001,31.261599754,31.2721663470001,31.270971611,31.2616581070001,31.264973377,31.2658491130001,31.2688886780001,31.2683040970001,31.2707125310001,31.271428288,31.254769068,31.257070663,31.2465921090001,31.2692059390001,31.2443915640001,31.2314304170001,31.269208157,31.269363072,31.270308577,31.2595743270001,31.2700607460001,31.2564875580001,31.2479156200001,31.2481547410001,31.254812955,31.266566502,31.2504635080001,31.2541823770001,31.2683791260001,31.271307544,31.2482951640001,31.2737507580001,31.272590789,31.2570404000001,31.252699315,31.2517972700001,31.2663416,31.2709878490001,31.2660293330001,31.25408293,31.2701366480001,31.2402113960001,31.2646567780001,31.2463304390001,31.2347788370001,31.252428283],[34.808214546,34.8078915740001,34.809368438,34.8093634950001,34.810975436,34.810334568,34.805622382,34.8026935250001,34.804473768,34.8095251010001,34.8075520780001,34.808555186,34.7651206140001,34.76364862,34.7628454490001,34.778674504,34.7777073990001,34.7762277100001,34.770138844,34.7869285290001,34.795184944,34.7997725760001,34.7940810010001,34.7916208840001,34.7906790590001,34.79068958,34.7927366010001,34.79231459,34.7927492380001,34.792743929,34.7905235110001,34.7883433920001,34.7867639080001,34.785602972,34.7852056130001,34.787070295,34.787588817,34.7939116200001,34.7924905290001,34.7941080740001,34.8014500780001,34.792161772,34.7956425140001,34.7970971840001,34.7970376860001,34.7936621440001,34.7914949660001,34.784890746,34.787583614,34.7873553950001,34.7857147330001,34.779422326,34.7796193240001,34.7841264850001,34.7828629210001,34.7779282610001,34.782604796,34.7832538050001,34.7900850610001,34.7963698560001,34.806238138,34.8089210210001,34.809585251,34.8086036290001,34.809357278,34.8091260510001,34.8067740120001,34.8089092710001,34.8086805990001,34.8079129270001,34.804323229,34.800439962,34.8088667390001,34.8085067230001,34.809244467,34.808135475,34.8077908650001,34.8096215740001,34.8069308010001,34.806502575,34.8037255140001,34.802776814,34.802732577,34.8024465140001,34.8025668970001,34.804125362,34.802364975,34.8055181280001,34.8074389120001,34.8070149960001,34.8064910510001,34.7604294980001,34.7639117040001,34.765621094,34.7664207510001,34.7608955090001,34.7630732060001,34.7598450790001,34.7611068100001,34.775540274,34.7789006310001,34.7774103090001,34.778600274,34.7726666250001,34.773724196,34.7718155210001,34.775669044,34.780235401,34.780269586,34.7763314490001,34.775308716,34.7743583120001,34.7736159990001,34.7923748970001,34.78033953,34.779738781,34.784380764,34.7847258240001,34.7817552400001,34.792498504,34.7868126540001,34.789983894,34.789128825,34.787953332,34.7942136410001,34.7945532960001,34.7903546790001,34.7959888820001,34.7941725480001,34.795403445,34.7891443610001,34.793907173,34.7965042000001,34.7892947980001,34.781547866,34.783656121,34.7751580730001,34.780917947,34.781822312,34.781617833,34.783344335,34.7623511370001,34.762642929,34.771785949,34.7709577200001,34.7636153930001,34.7619093160001,34.7644633370001,34.764872847,34.7577812160001,34.770979358,34.765673103,34.7643799360001,34.767547478,34.769529817,34.773305672,34.773415368,34.805293723,34.805267585,34.800891017,34.808306987,34.806721075,34.8088518340001,34.8019847370001,34.8065743970001,34.810013871,34.8081264090001,34.7780347280001,34.782620235,34.7707858410001,34.7723190740001,34.774894422,34.77918176,34.7948698500001,34.7970907800001,34.795007293,34.79702511,34.7965872190001,34.795338615,34.7935443280001,34.7929193080001,34.7929353970001,34.7939598030001,34.795247798,34.797073217,34.7944644400001,34.7974265890001,34.792997642,34.785492021,34.7889980750001,34.804945893,34.8100962180001,34.808716111,34.783396279,34.7850399330001,34.795998975,34.805139436,34.8028419560001,34.8061534390001,34.804344707,34.807264053,34.81146289,34.806185188,34.806567597,34.81037658,34.80369548,34.8085987040001,34.811233825,34.809843013,34.8094435680001,34.8080172910001,34.810334235,34.8070907060001,34.8035195030001,34.8086055260001,34.802861137,34.7937478430001,34.7956186760001,34.793586141,34.7919188430001,34.79992937,34.801187988,34.8012474810001,34.792345267,34.7969623870001,34.794766757,34.8086733960001,34.80687017,34.776587685,34.79294107,34.791070099,34.7935582580001,34.7954634880001,34.7948531970001,34.7868227160001,34.7857320460001,34.795653003,34.791395838,34.7796560830001,34.781008178,34.77898084,34.794873022,34.780569927,34.8023777060001,34.7968104340001,34.7835412000001,34.7892121180001,34.807038339,34.8080192800001,34.8085495820001,34.779316936,34.8025542940001,34.7698468990001,34.8072728060001,34.8008629990001,34.8116456560001,34.7842904080001,34.7950379800001,34.7961240610001,34.8083349930001,34.7825503890001,34.808149154],4,null,null,{&#34;interactive&#34;:true,&#34;className&#34;:&#34;&#34;,&#34;stroke&#34;:true,&#34;color&#34;:&#34;red&#34;,&#34;weight&#34;:5,&#34;opacity&#34;:0.5,&#34;fill&#34;:true,&#34;fillColor&#34;:&#34;red&#34;,&#34;fillOpacity&#34;:0.2},null,null,null,{&#34;interactive&#34;:false,&#34;permanent&#34;:false,&#34;direction&#34;:&#34;auto&#34;,&#34;opacity&#34;:1,&#34;offset&#34;:[0,0],&#34;textsize&#34;:&#34;10px&#34;,&#34;textOnly&#34;:false,&#34;className&#34;:&#34;&#34;,&#34;sticky&#34;:true},null,null]}],&#34;setView&#34;:[[31.25181,34.7913],13,[]],&#34;limits&#34;:{&#34;lat&#34;:[31.2197570490001,31.2759297440001],&#34;lng&#34;:[34.7577812160001,34.8116456560001]}},&#34;evals&#34;:[],&#34;jsHooks&#34;:[]}&lt;/script&gt;
&lt;/div&gt;
&lt;div id=&#34;original-post&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;&lt;strong&gt;Original post&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;I’ve been wanting to learn how to use maps in R for a while before creating the map in this post. Seeing dataframes with longitude and latitude coordinates on various occasions on &lt;a href=&#34;https://github.com/rfordatascience/tidytuesday&#34;&gt;#Tidytuesday&lt;/a&gt; encouraged me to do so.&lt;br /&gt;
A day before this visualizaiton, I discovered our municupality’s open access data &lt;a href=&#34;https://www.beer-sheva.muni.il/OpenData/Pages/default.aspx&#34;&gt;website&lt;/a&gt;. In this website you can find various datasets like street light coordinates, bomb shelters spread out in the city and more. A day after discovering it Israel, the country I live in, was fired missiles at. I decided to take the opportunity and map some of the shelters around my house. You know, just in case.&lt;/p&gt;
&lt;p&gt;Let’s begin with the packages (📦) we’ll need:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#for data manipulation
library(tidyverse)
#for a nice map
library(ggmap)
#for reading and working with .geojson file
library(geojsonio)
library(sp)
#for integrating a nice font
library(extrafont)&lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;loading-and-tidying-the-data&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Loading and tidying the data&lt;/h3&gt;
&lt;p&gt;I initially tried using the &lt;code&gt;.csv&lt;/code&gt; file they have on their webiste but I was having too much trouble with the Hebrew so I decided to try and work with the &lt;code&gt;geojsonio&lt;/code&gt; package. I had no idea how to work with a &lt;code&gt;.geojson&lt;/code&gt; file or frankly how to work with maps in general. To my save, i found this incredible blog by &lt;a href=&#34;https://randomjohn.github.io/r-geojson-srt/&#34;&gt;John Johnson&lt;/a&gt; to help me transform a ‘geomjson’ file to a dataframe you can work with.&lt;/p&gt;
&lt;p&gt;Let’s begin:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#read the .geojson file
my_geojson &amp;lt;- &amp;quot;shelters.geojson&amp;quot;
#convert the .geojson file to an sp object
data_json &amp;lt;- geojson_read(my_geojson, what = &amp;quot;sp&amp;quot;)
#now we can convert it to a nice data frame
shelters &amp;lt;- as.data.frame(data_json)
#last tidying of the column names
names(shelters)[6:7] &amp;lt;- c(&amp;quot;long&amp;quot;, &amp;quot;lat&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;What we did was load the geojson file, read it as an ‘sp’ object and then turn it into to a dataframe. I changed the names so that it’ll be easier to read the columns. we could also use &lt;code&gt;dplyr::rename&lt;/code&gt; but I liked the base R function Johnson used in his blog so I’ll stick with that.&lt;br /&gt;
let’s look at the top 3 observations of our new data frame:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;##       long      lat        name X.U.05E7..U.05D5..U.05D3._.U.05E1.
## 1 34.80821 31.25902  &amp;lt;U+05D2&amp;gt;/2                                 17
## 2 34.80789 31.25980  &amp;lt;U+05D2&amp;gt;/1                                 17
## 3 34.80937 31.25925 &amp;lt;U+05D2&amp;gt;/25                                 17
##                elc group_ F_.U.05E6..U.05D5..U.05D5..U.05EA.
## 1 &amp;lt;U+05D9&amp;gt;&amp;lt;U+05E9&amp;gt;      0                                   
## 2 &amp;lt;U+05D9&amp;gt;&amp;lt;U+05E9&amp;gt;      0                                   
## 3 &amp;lt;U+05D9&amp;gt;&amp;lt;U+05E9&amp;gt;      0&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Ugh, well the names weren’t read well into &lt;code&gt;R&lt;/code&gt;. While this isn’t a big issue to resolve, I don’t find it necessary for the final piece. the names of the shelters are anyway in Hebrew and only represnt a letter and some sort of number (for e.g, A/23, only in Hebrew). Therefore we’ll leave it as is since what I’m interested in is the longitude and latitude coordinations and for that we don’t need the character column.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;retrieving-the-map&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Retrieving the map&lt;/h3&gt;
&lt;p&gt;So we have our data frame with long and lat points, let’s get our map. I want a map that can be readable in terms of streets and roads, therefore I’ll give the &lt;code&gt;ggmap&lt;/code&gt; package a try&lt;a href=&#34;#fn1&#34; class=&#34;footnote-ref&#34; id=&#34;fnref1&#34;&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt;. Google requires you to register in order to recieve an API key to pull maps to plot. Unfortunately I won’t cover how to regiser in this blog post but I’m sure you can find plenty of tutorials addressing it online.&lt;br /&gt;
Let’s get Be’er-Sheva’s map:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;b7_map &amp;lt;- get_map(location = c(34.7913 , 31.25181), 
              zoom = 13, scale = 2, maptype = &amp;quot;roadmap&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;What we did here was use the &lt;code&gt;get_map&lt;/code&gt; function to pull the map according to the long and lat coordinates I gave it of Be’er-Sheva. You should first pass the longitude and then the latitude in the &lt;code&gt;location&lt;/code&gt; argument. In addition you can change other features such as the zoom level, the maptype and more as we saw here (See &lt;code&gt;?get_map&lt;/code&gt; for more info).&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;plot&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Plot&lt;/h3&gt;
&lt;p&gt;Now that we have our data set ready and the map as an object we can go on to plot it. ggmap extends ggplot features so we can run the data frame smoothly into the &lt;code&gt;ggmap&lt;/code&gt; function:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggmap(b7_map)+
  geom_point(shelters, mapping = aes(long,lat),
            color = &amp;quot;red&amp;quot;, size = 0.3, shape = 15)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://amitlevinson.com/post/bomb-shelters/index_files/figure-html/unnamed-chunk-6-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;What we did was pass the b7_map as an object into the &lt;code&gt;ggmap&lt;/code&gt; function and add a geom, in this case &lt;code&gt;geom_point&lt;/code&gt; representing our shelter coordinates. However, this map doesn’t really help me in a time of need since it doesn’t show &lt;em&gt;my address&lt;/em&gt; clearly.&lt;/p&gt;
&lt;p&gt;Let’s try zooming in so that we can see what we’re looking at:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#retreiving a new map with a greater `zoom`
b7_map_zoom &amp;lt;- get_map(location = c(34.7913 , 31.25181), 
                    zoom = 16, scale = 2, maptype = &amp;quot;roadmap&amp;quot;)

p &amp;lt;- ggmap(b7_map_zoom)+
geom_point(shelters, mapping = aes(long,lat), color = &amp;quot;red&amp;quot;,
           size = 3, shape = 15)
p&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://amitlevinson.com/post/bomb-shelters/index_files/figure-html/unnamed-chunk-7-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Much nicer and clearer. Using the zoom option in &lt;code&gt;get_map&lt;/code&gt; enables to center more on where I want. Great, this shows me some bomb shetlers I have around me in a time of need. Let’s add some fine tuning for our theme:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;p+  
#for the title, caption and removing the X and Y axis
  labs (title = &amp;quot;Neighborhood B, Beer-Sheva, Israel, bomb shelters&amp;quot;,
        x = NULL, y = NULL,
        caption = &amp;quot;data: www.beer-sheva.muni.il | @Amit_Levinson&amp;quot;)+
  theme_minimal()+
  theme(text = element_text(family = &amp;quot;Microsoft Tai Le&amp;quot;),
    #Changing the position of the title
    plot.title = element_text(hjust = 0.5, size = 20, face = &amp;quot;bold&amp;quot;),
    axis.text = element_blank(),
    plot.caption = element_text(size = 9, face = &amp;quot;italic&amp;quot;, hjust = 0),
    panel.border = element_rect(color = &amp;quot;black&amp;quot;, size=2, fill = NA)
  )&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://amitlevinson.com/post/bomb-shelters/index_files/figure-html/unnamed-chunk-8-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Perfect, I can now save the plot and distribute it if someone needs it.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggsave(&amp;quot;shelters_b_eng.png&amp;quot;, width = 8, height = 8)&lt;/code&gt;&lt;/pre&gt;
&lt;center&gt;
&lt;blockquote class=&#34;twitter-tweet&#34;&gt;&lt;p lang=&#34;en&#34; dir=&#34;ltr&#34;&gt;1.  I&amp;#39;ve been wanting to learn to plot maps in &lt;a href=&#34;https://twitter.com/hashtag/rstats?src=hash&amp;amp;ref_src=twsrc%5Etfw&#34;&gt;#rstats&lt;/a&gt;&lt;br&gt;2. Yesterday I encountered open data our municipality publishes&lt;br&gt;3. Today missiles are fired towards Israel in response to assassination of a top terrorist.&lt;br&gt;&lt;br&gt;1+2+3: Plotting bomb shelter locations near where I live&lt;a href=&#34;https://twitter.com/hashtag/ggmap?src=hash&amp;amp;ref_src=twsrc%5Etfw&#34;&gt;#ggmap&lt;/a&gt; &lt;a href=&#34;https://t.co/4Irz0ZKuZr&#34;&gt;pic.twitter.com/4Irz0ZKuZr&lt;/a&gt;&lt;/p&gt;&amp;mdash; Amit Levinson (@Amit_Levinson) &lt;a href=&#34;https://twitter.com/Amit_Levinson/status/1194274713759039488?ref_src=twsrc%5Etfw&#34;&gt;November 12, 2019&lt;/a&gt;&lt;/blockquote&gt;
&lt;script async src=&#34;https://platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;

&lt;/center&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;footnotes&#34;&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li id=&#34;fn1&#34;&gt;&lt;p&gt;For a first map I decided to go with a static one, but an interactive one can defenitely be a 2.0 version of this blog (as you saw with the &lt;a href=&#34;#update&#34;&gt;March 21st update&lt;/a&gt;. Hopefully we won’t need it.&lt;a href=&#34;#fnref1&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
      
            <category>ggmap</category>
      
            <category>rvest</category>
      
      
            <category>R</category>
      
    </item>
    
  </channel>
</rss>
