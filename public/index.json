[{"authors":["admin"],"categories":null,"content":"Hi, I\u0026rsquo;m Amit Levinson, I\u0026rsquo;m an MA student for Sociology \u0026amp; Anthropology at Ben-Gurion University of the Negev. I currently live with my wife in Be\u0026rsquo;er Sheva, southern Israel. My interests include my thesis topic, privacy and trust in the online cannabis market, other fields such as political participation, statistics and R. I\u0026rsquo;m currently working as a research assistant for Dr. Jennifer Oser from the Department of Politics and Government researching online and offline political participation.\nI opened this website so that I can share what I learn throughout my journey in R. Make sure to visit R-bloggers \u0026amp; RWeekly to explore many more tutorials. Enjoy!\n","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"/authors/admin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/admin/","section":"authors","summary":"Hi, I\u0026rsquo;m Amit Levinson, I\u0026rsquo;m an MA student for Sociology \u0026amp; Anthropology at Ben-Gurion University of the Negev. I currently live with my wife in Be\u0026rsquo;er Sheva, southern Israel. My interests include my thesis topic, privacy and trust in the online cannabis market, other fields such as political participation, statistics and R. I\u0026rsquo;m currently working as a research assistant for Dr. Jennifer Oser from the Department of Politics and Government researching online and offline political participation.","tags":null,"title":"Amit Levinson","type":"authors"},{"authors":[],"categories":[],"content":"\r\r‚ÄúI have a great idea to get rich. All we need is a lot of money.‚Äù  ‚Äï A meme on the internet\n\rA while ago I was reading about Bernoulli trials and decided that I wanted to explore them further. I was wondering what would be an interesting case study for such a topic, and then it hit me (üí°): Why not explore lottery probabilities? Little did I know that this topic would lead me down the geometric distribution road and help me better grasp the uncertainty in lotteries.\nThe rules\rWe‚Äôll be exploring the probability of winning the Israeli lottery held by the Pais organization, a well-known lottery enterprise in Israel. Pais holds their lotteries twice a week with the regular prize of 5 million New Israeli Shekels (NIS), equivalent to $1,420,000. If there‚Äôs no winner for a given week, the prize accumulates to the following lottery. For the sake of the post I‚Äôll only discuss winning a prize, and not focus on the effect of prize increase on when to participate.\nWhen filling out a lottery form you choose 6 numbers in the range of 1‚Äì37 and a ‚Äòstrong‚Äô number in the range of 1‚Äì71. In order to win first place you have to get guess correctly both the 6 number set and the strong number. Luckily, the order of the 6 numbers doesn‚Äôt matter therefore if you wrote \\(6,12...n\\) or \\(12,6,...n\\) your good on both (Also known as combinations, more on that in a minute).\nAnother ‚Äòluck‚Äô in our favor is that in each lottery ticket we have the option to fill out two sets of numbers, therefore doubling our odds of winning. I‚Äôm assuming we‚Äôre on the same page and you won‚Äôt use both of your attempts to guess the same sets of numbers, so that somewhat increases our odds of winning. Speaking of odds, let‚Äôs have a look at them.\n\rThe odds\rTo understand the lottery probabilities, we need to calculate the probability of guessing a combination of 6 numbers out of 37 options along with one strong number out of 7 possible numbers. In order to do this, we can turn to combinations:\n\rIn mathematics, a combination is a selection of items from a collection, such that (unlike permutations) the order of selection does not matter.  ‚Äï Wikipedia\n\rThat‚Äôs exactly what we need. We want to calculate the probability of randomly guessing six numbers regardless of their order; If the order was important we‚Äôd want to look at permutations. In addition, each number is drawn without replacement, and therefore there can‚Äôt be repetition of the same number.\nThe formula to calculate combinations is as follows: \\(C(n,k) = \\frac{n!}{k!(n-k)!}\\) Where \\(n\\) is the number of options to choose from and \\(k\\) is the number of choices we make.\nInputting our numbers we get \\(C(37,6) = \\frac{37!}{6!(37-6)!}\\), and now we just calculate away. However, don‚Äôt forget we also need to guess another number out of 7 possible numbers (the strong one), so we‚Äôll multiply our outcome by \\(\\frac1 7\\), yielding a probability of \\(p = \\frac{1}{16273488}\\). ‚ÄòLuckily‚Äô we choose two sets of numbers in a given ticket, so we multiply the probability by 2.\nTherefore, the probability of winning the lottery is \\(p = \\frac{1}{8136744}\\).\nWow! that‚Äôs a very low probability. How low? Let‚Äôs try and visualize it.\nSometimes when we receive a probability it‚Äôs hard to grasp the odds and numbers thrown at us. Therefore, I‚Äôll try to visualize it for us. Imagine there‚Äôs a pool filled with 8,136,744 balls. One of those balls is red and choosing that exact red ball blindly will win you the lottery:\nlibrary(ggplot2)\rlibrary(dplyr)\rlibrary(scattermore)\rlibrary(extrafont)\rlibrary(ggtext)\rset.seed(123)\rdf_viz \u0026lt;- data.frame(x = rnorm(8136743, mean = 1000, sd = 1000),y = rnorm(8136743))\rpoint_highlight \u0026lt;- data.frame(x = -1811.674, y = -2.268505588)\rlot_p \u0026lt;- ggplot()+\rgeom_scattermost(df_viz, pointsize = 0.1, pixels = c(2000,2000))+\rgeom_point(data = point_highlight, aes(x = x, y = y), size = 0.3, color = \u0026quot;red\u0026quot;)+\rannotate(geom = \u0026quot;curve\u0026quot;, x = -2750, xend = -1860, y = -3.10, yend = -2.29,\rcurvature = -.2, color = \u0026quot;grey25\u0026quot;, size = 0.75, arrow = arrow(length = unit(1.5, \u0026quot;mm\u0026quot;)))+\rannotate(\u0026quot;text\u0026quot; ,x = -2750, y = -3.30, label = \u0026quot;Winner\u0026quot;, family = \u0026quot;Roboto Condensed\u0026quot;, size = 3)+\rlabs(title = \u0026quot;Winning the Israeli lottery\u0026quot;, subtitle = \u0026quot;To win, imagine trying to randomly choose a \u0026lt;b\u0026gt;\u0026lt;span style=\u0026#39;color:red\u0026#39;\u0026gt;specific ball\u0026lt;/span\u0026gt;\u0026lt;/b\u0026gt; out of 8,136,744 balls\u0026quot;)+\rtheme_void()+\rtheme(text = element_text(family = \u0026quot;Roboto Condensed\u0026quot;),\rplot.title.position = \u0026quot;plot\u0026quot;,\rplot.title = element_text(size = 16, face= \u0026quot;bold\u0026quot;),\rplot.subtitle = element_markdown(family = \u0026quot;Roboto Condensed\u0026quot;,size = 12))\rlot_p\rNot easy is it?\nNow that we know the probability of winning at each attempt, let‚Äôs see how it manifests across multiple attempts.\n\rMultiple attempts - Geometric distribution\rA Bernoulli trial is a random experiment with exactly two outcomes - such as success\\failure, heads\\tails - in which the probability for each outcome is the same every time (Wikipedia). This sets the ground for discussing an outcome of a lottery in which you either win or lose.\nBut we want to learn more about the distribution of attempts, and this brings us to the geometric distribution. A Geometric distribution enables us to calculate the probability distribution of a number of failures before the first success2.\nBefore we begin, we must meet several conditions to use the geometric distribution:\n‚úîÔ∏è Each trial is independent from one another - succeeding in one trial doesn‚Äôt affect the next trial. We know this is true since winning in one lottery won‚Äôt affect your chances of winning the next round.\n‚úîÔ∏è Every trial has an outcome of a success or failure. This assumption is true in our case where each lottery you participate in you either win or lose.\n‚úîÔ∏è The probability of success \\(p\\) is the same every trial - This is also true given the lottery probabilities are consistent across each game.\nNow that we got the technicalities out of the way we can start exploring some of the uncertainty surrounding the lottery.\nWinning at a given trial\rWe can denote the probability of winning as \\(p\\), which in the case of a lottery game is equal to \\(p = \\frac{1}{8136744}\\). What if we wanted to know the probability of winning the lottery on the third try? That means we need two failures and then a success. If the probability of success - guessing the correct numbers - is \\(p = \\frac{1}{8136744}\\), so the probability of a failure is \\(q = 1 - p\\), in this case \\(q = \\frac{8136743}{8136744}\\). In order to win the lottery on the third try, this means getting two failures and then a success, resulting in a total of \\(k = 3\\) attempts. Thus, the probability of winning on the third attempt is as follows:\n\\(p(3) = (\\frac{8136743}{8136744})\\cdot(\\frac{8136743}{8136744})\\cdot(\\frac{1}{8136744})\\), equaling \\(p = 0.0000001228993\\). In other words there‚Äôs a ~0.0000123% chance we‚Äôll win the lottery exactly on the third try.\nGeneralizing, the probability distribution of the number of Bernoulli trials needed to get one success on the \\(k\\) th trial is: \\(P(X = k) = (1 - p)^{(k-1)} \\cdot p\\). We can break this up according to our previous example:\n\r\\(P\\) stands for the probability of getting our value \\(X\\) on the \\(k\\) attempt. Meaning, we want to win the lottery only on the third attempt.\n\rSo our first two attempts should be a failure, thus a probability of \\(q = 1 - \\frac{1}{8136744}\\) multiplied by two (two rounds of failures), written as \\((\\frac{8136743}{8136744})^{3 - 1}\\).\n\rLastly, \\(p\\) stands for the probability of succeeding, \\(\\frac{1}{8136744}\\) occurring exactly on the \\(k\\) attempt.\n\r\rThe probability we just discussed is also known as the probability mass function (PMF) of the geometric distribution. PMF is a function that gives the probability that a random discrete variable is exactly equal to some value. In our above example, the probability that we‚Äôll win exactly on the third try.\n\rWinning by a given trial\rWe don‚Äôt necessarily want to win the lottery on on a specific \\(X\\) attempt, but explore the probabilities of winning by the \\(k\\)th attempt. Reframing our previous question we can ask ‚Äúwhat is the probability of winning the lottery on the first 3 attempts?‚Äù, bringing us to the Cumulative distribution function (CDF). In a cumulative distribution we calculate the probability that \\(X\\) will take a value less than or equal to \\(k\\) (in our case representing the number of attempts).\nHow does this question change our calculation?\nLet‚Äôs assume we‚Äôre still talking about 3 attempts. Our new framed question means we want to win the lottery either on the first attempt, the second or the third. In other words, we want to add the probability of success when \\(P(X = 1)\\) + \\(P(X = 2)\\) + \\(P(X = 3)\\). Given that our probability of failure is \\(q = 1 - p\\), we can write the argument as follows: \\(P(X \\leq 3) = q^0\\cdot p + q^1 \\cdot p + q^2 \\cdot p\\), inputting our values of \\({(\\frac{8136743}{8136744}})^0 \\cdot p \\cdot({\\frac{8136743}{8136744}})^1\\cdot p, ...\\), resulting in the probability of winning in one of the first three attempts \\(P(X \\leq 3) = 0.000000368\\), also written as a 0.0000368% chance.\nBut if we want to look at the first 50 attempts? we‚Äôll have to sum each individual PMF?\nHere‚Äôs exactly the use of the geometric CDF written as \\(P(X \u0026lt;= x) = 1 - q^x\\). We power the probability of loosing by the threshold of attempts to win by and deduct it from 1, resulting in the probability of winning by a given trial.\n\r\rWinning on the first X trials\rWe just looked at the probability of winning on the first 3 trials, and now that we learned about the CDF we can calculate the probability of winning on the first \\(x\\) trials, for e.g.¬†on the first 100, 1000 and so on. In addition, another important factor we can take into account exploring the cumulative distribution is the money spent reaching each attempt.\nWe‚Äôll start by declaring our values. We know the probability for winning the lottery with each ticket we have is \\(p = \\frac{1}{8,136,744}\\) (remember, we get to choose two sets of numbers in each ticket), so let‚Äôs declare that:\np \u0026lt;- 1/8136744\rNext we know the probability for not winning is \\(q = 1 - p\\):\nq \u0026lt;- 1 - p\rNow we can create a data frame to account for some 250,000 attempts. We don‚Äôt need each attempt so we‚Äôll simulate data for the first 50,000 and then have points spread out in a 500 interval jump all the way to the 100,000,000 attempt.\ndf_prob \u0026lt;- tibble(trial = c(1:50000, seq(50000, 1e8, 500)))\rOnce we have that we can calculate both the probability of winning up to a specific attempt and the cumulative amount of money spent reaching there:\ndf_prob \u0026lt;- df_prob %\u0026gt;% mutate(cdf = 1 - (1 - p)^trial,\rmoney_spent = trial * 5.8)\rhead(df_prob)\r## # A tibble: 6 x 3\r## trial cdf money_spent\r## \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt;\r## 1 1 0.000000123 5.8\r## 2 2 0.000000246 11.6\r## 3 3 0.000000369 17.4\r## 4 4 0.000000492 23.2\r## 5 5 0.000000614 29 ## 6 6 0.000000737 34.8\rLooks good!\nWe see our top 6 observations with 3 columns we just defined (from left to right): the lottery raffle (trial), the probability of winning at a given trial until that point (cdf) and the money spent by that trial. Our probability of winning at any trial is constant (\\(p\\)), so it‚Äôll be redundant to add that in here.\nNow let‚Äôs look at specific points along our data frame and see how much money is spent reaching there. More specifically, let‚Äôs look at the details of some attempts such as 1; 10; 100; 1000; 2500, 5000 \\(...\\) 1,000,000, 10,000,000; 50,000,000:\nhtml {\rfont-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, Cantarell, 'Helvetica Neue', 'Fira Sans', 'Droid Sans', Arial, sans-serif;\r}\r#hvuocxzswv .gt_table {\rdisplay: table;\rborder-collapse: collapse;\rmargin-left: auto;\rmargin-right: auto;\rcolor: #333333;\rfont-size: 16px;\rbackground-color: #FFFFFF;\rwidth: auto;\rborder-top-style: solid;\rborder-top-width: 2px;\rborder-top-color: #A8A8A8;\rborder-right-style: none;\rborder-right-width: 2px;\rborder-right-color: #D3D3D3;\rborder-bottom-style: solid;\rborder-bottom-width: 2px;\rborder-bottom-color: #A8A8A8;\rborder-left-style: none;\rborder-left-width: 2px;\rborder-left-color: #D3D3D3;\r}\r#hvuocxzswv .gt_heading {\rbackground-color: #FFFFFF;\rtext-align: left;\rborder-bottom-color: #FFFFFF;\rborder-left-style: none;\rborder-left-width: 1px;\rborder-left-color: #D3D3D3;\rborder-right-style: none;\rborder-right-width: 1px;\rborder-right-color: #D3D3D3;\r}\r#hvuocxzswv .gt_title {\rcolor: #333333;\rfont-size: 125%;\rfont-weight: bold;\rpadding-top: 4px;\rpadding-bottom: 4px;\rborder-bottom-color: #FFFFFF;\rborder-bottom-width: 0;\r}\r#hvuocxzswv .gt_subtitle {\rcolor: #333333;\rfont-size: 85%;\rfont-weight: initial;\rpadding-top: 0;\rpadding-bottom: 4px;\rborder-top-color: #FFFFFF;\rborder-top-width: 0;\r}\r#hvuocxzswv .gt_bottom_border {\rborder-bottom-style: solid;\rborder-bottom-width: 2px;\rborder-bottom-color: #D3D3D3;\r}\r#hvuocxzswv .gt_col_headings {\rborder-top-style: solid;\rborder-top-width: 2px;\rborder-top-color: #D3D3D3;\rborder-bottom-style: solid;\rborder-bottom-width: 2px;\rborder-bottom-color: #D3D3D3;\rborder-left-style: none;\rborder-left-width: 1px;\rborder-left-color: #D3D3D3;\rborder-right-style: none;\rborder-right-width: 1px;\rborder-right-color: #D3D3D3;\r}\r#hvuocxzswv .gt_col_heading {\rcolor: #333333;\rbackground-color: #FFFFFF;\rfont-size: 100%;\rfont-weight: normal;\rtext-transform: inherit;\rborder-left-style: none;\rborder-left-width: 1px;\rborder-left-color: #D3D3D3;\rborder-right-style: none;\rborder-right-width: 1px;\rborder-right-color: #D3D3D3;\rvertical-align: bottom;\rpadding-top: 5px;\rpadding-bottom: 6px;\rpadding-left: 5px;\rpadding-right: 5px;\roverflow-x: hidden;\r}\r#hvuocxzswv .gt_column_spanner_outer {\rcolor: #333333;\rbackground-color: #FFFFFF;\rfont-size: 100%;\rfont-weight: normal;\rtext-transform: inherit;\rpadding-top: 0;\rpadding-bottom: 0;\rpadding-left: 4px;\rpadding-right: 4px;\r}\r#hvuocxzswv .gt_column_spanner_outer:first-child {\rpadding-left: 0;\r}\r#hvuocxzswv .gt_column_spanner_outer:last-child {\rpadding-right: 0;\r}\r#hvuocxzswv .gt_column_spanner {\rborder-bottom-style: solid;\rborder-bottom-width: 2px;\rborder-bottom-color: #D3D3D3;\rvertical-align: bottom;\rpadding-top: 5px;\rpadding-bottom: 6px;\roverflow-x: hidden;\rdisplay: inline-block;\rwidth: 100%;\r}\r#hvuocxzswv .gt_group_heading {\rpadding: 8px;\rcolor: #333333;\rbackground-color: #FFFFFF;\rfont-size: 100%;\rfont-weight: initial;\rtext-transform: inherit;\rborder-top-style: solid;\rborder-top-width: 2px;\rborder-top-color: #D3D3D3;\rborder-bottom-style: solid;\rborder-bottom-width: 2px;\rborder-bottom-color: #D3D3D3;\rborder-left-style: none;\rborder-left-width: 1px;\rborder-left-color: #D3D3D3;\rborder-right-style: none;\rborder-right-width: 1px;\rborder-right-color: #D3D3D3;\rvertical-align: middle;\r}\r#hvuocxzswv .gt_empty_group_heading {\rpadding: 0.5px;\rcolor: #333333;\rbackground-color: #FFFFFF;\rfont-size: 100%;\rfont-weight: initial;\rborder-top-style: solid;\rborder-top-width: 2px;\rborder-top-color: #D3D3D3;\rborder-bottom-style: solid;\rborder-bottom-width: 2px;\rborder-bottom-color: #D3D3D3;\rvertical-align: middle;\r}\r#hvuocxzswv .gt_striped {\rbackground-color: rgba(128, 128, 128, 0.05);\r}\r#hvuocxzswv .gt_from_md  :first-child {\rmargin-top: 0;\r}\r#hvuocxzswv .gt_from_md  :last-child {\rmargin-bottom: 0;\r}\r#hvuocxzswv .gt_row {\rpadding-top: 8px;\rpadding-bottom: 8px;\rpadding-left: 5px;\rpadding-right: 5px;\rmargin: 10px;\rborder-top-style: solid;\rborder-top-width: 1px;\rborder-top-color: #D3D3D3;\rborder-left-style: none;\rborder-left-width: 1px;\rborder-left-color: #D3D3D3;\rborder-right-style: none;\rborder-right-width: 1px;\rborder-right-color: #D3D3D3;\rvertical-align: middle;\roverflow-x: hidden;\r}\r#hvuocxzswv .gt_stub {\rcolor: #333333;\rbackground-color: #FFFFFF;\rfont-size: 100%;\rfont-weight: initial;\rtext-transform: inherit;\rborder-right-style: solid;\rborder-right-width: 2px;\rborder-right-color: #D3D3D3;\rpadding-left: 12px;\r}\r#hvuocxzswv .gt_summary_row {\rcolor: #333333;\rbackground-color: #FFFFFF;\rtext-transform: inherit;\rpadding-top: 8px;\rpadding-bottom: 8px;\rpadding-left: 5px;\rpadding-right: 5px;\r}\r#hvuocxzswv .gt_first_summary_row {\rpadding-top: 8px;\rpadding-bottom: 8px;\rpadding-left: 5px;\rpadding-right: 5px;\rborder-top-style: solid;\rborder-top-width: 2px;\rborder-top-color: #D3D3D3;\r}\r#hvuocxzswv .gt_grand_summary_row {\rcolor: #333333;\rbackground-color: #FFFFFF;\rtext-transform: inherit;\rpadding-top: 8px;\rpadding-bottom: 8px;\rpadding-left: 5px;\rpadding-right: 5px;\r}\r#hvuocxzswv .gt_first_grand_summary_row {\rpadding-top: 8px;\rpadding-bottom: 8px;\rpadding-left: 5px;\rpadding-right: 5px;\rborder-top-style: double;\rborder-top-width: 6px;\rborder-top-color: #D3D3D3;\r}\r#hvuocxzswv .gt_table_body {\rborder-top-style: solid;\rborder-top-width: 2px;\rborder-top-color: #D3D3D3;\rborder-bottom-style: solid;\rborder-bottom-width: 2px;\rborder-bottom-color: #D3D3D3;\r}\r#hvuocxzswv .gt_footnotes {\rcolor: #333333;\rbackground-color: #FFFFFF;\rborder-bottom-style: none;\rborder-bottom-width: 2px;\rborder-bottom-color: #D3D3D3;\rborder-left-style: none;\rborder-left-width: 2px;\rborder-left-color: #D3D3D3;\rborder-right-style: none;\rborder-right-width: 2px;\rborder-right-color: #D3D3D3;\r}\r#hvuocxzswv .gt_footnote {\rmargin: 0px;\rfont-size: 90%;\rpadding: 4px;\r}\r#hvuocxzswv .gt_sourcenotes {\rcolor: #333333;\rbackground-color: #FFFFFF;\rborder-bottom-style: none;\rborder-bottom-width: 2px;\rborder-bottom-color: #D3D3D3;\rborder-left-style: none;\rborder-left-width: 2px;\rborder-left-color: #D3D3D3;\rborder-right-style: none;\rborder-right-width: 2px;\rborder-right-color: #D3D3D3;\r}\r#hvuocxzswv .gt_sourcenote {\rfont-size: 90%;\rpadding: 4px;\r}\r#hvuocxzswv .gt_left {\rtext-align: left;\r}\r#hvuocxzswv .gt_center {\rtext-align: center;\r}\r#hvuocxzswv .gt_right {\rtext-align: right;\rfont-variant-numeric: tabular-nums;\r}\r#hvuocxzswv .gt_font_normal {\rfont-weight: normal;\r}\r#hvuocxzswv .gt_font_bold {\rfont-weight: bold;\r}\r#hvuocxzswv .gt_font_italic {\rfont-style: italic;\r}\r#hvuocxzswv .gt_super {\rfont-size: 65%;\r}\r#hvuocxzswv .gt_footnote_marks {\rfont-style: italic;\rfont-size: 65%;\r}\r\r\rLottery probabilities with the geometric distribution\r\r\rLottery probabilities winning by a given attempt, the money spent reaching there and your chances of winning by then\r\r\r\rAttempt\rMoney spent1\r% winning by then\r\r\r\r1\r$2\r0.00001%\r\r\r10\r$17\r0.00012%\r\r\r100\r$166\r0.00123%\r\r\r500\r$829\r0.00614%\r\r\r1,000\r$1,657\r0.01229%\r\r\r2,500\r$4,143\r0.03072%\r\r\r5,000\r$8,286\r0.06143%\r\r\r10,000\r$16,571\r0.12282%\r\r\r25,000\r$41,429\r0.30678%\r\r\r100,000\r$165,714\r1.22147%\r\r\r250,000\r$414,286\r3.02576%\r\r\r500,000\r$828,571\r5.95997%\r\r\r1,000,000\r$1,657,143\r11.56473%\r\r\r10,000,000\r$16,571,429\r70.74129%\r\r\r50,000,000\r$82,857,143\r99.78557%\r\r\r\r1\r\rMoney spent corresponds to the cumulative number of attempts played\r\r\r\r\rIn the above table I printed specific observations along the lottery‚Äôs cumulative geometric distribution. In our left column we have the trial number, next the approximate amount of money spent up to that trial and lastly the percent of winning by that given trial. Notice that I converted the New Israeli Shekels to dollars ($1 dollar = ~ NIS 3.5).\nIf we played 100 consecutive games with the same number, we would spend 166 dollars by that point and have only a 0.00123% chance of winning. We only pass the 1% (!) chance of winning after buying more than 100,000 tickets, spending a total of $165,714 dollars.\nTo pass the 10% chance of winning you‚Äôd have to play 1,000,000 games and spend ~1,600,000 dollars! Remember, the default prize is only some $1,412,000!\n\rAverage number of attempts\rAn interesting feature of the geometric distribution is that we can calculate the mean and variance of the distribution. The mean in the discussed cumulative distribution is \\(E(X) = \\frac{1}{p}\\) and a variance of \\(var(X) = \\frac{1-p}{p^2}\\). The mean is basically the expected value for the number of independent trials needed to get the first success. So in our lottery example the expected number of attempts to reach a success is \\(E(X) = \\frac{1}{\\frac{1}{8136744}}\\), resulting in 8,136,744 attempts.\nR has built in functions for working with the geometric distribution such as pgeom, rgeom, qgeom and dgeom which you can explore more here. For the purpose of exploring the mean we can use the rgeom function which generates a value representing the number of failures before a success occurred. For example, let‚Äôs see how many failures we‚Äôre required to reach one success:\nrgeom(n = 1,p = p)\r## [1] 30687199\rIn the above example rgeom takes the number of rounds (n = 1) and the probability of winning (p = p). The outputted value indicates the number of failures before our success.\nUsing this we can calculate the average number of attempts from 2,000,000 games:\nmean(rgeom(2e6, p))\r## [1] 8129065\rPretty close to our expected value!\nSo what does the \\(E(X)\\) mean in terms of the lottery? On average, you‚Äôd have to play 8,136,744 games to win the lottery, spending a total of NIS 47,193,115 (~$13,483,747) to win approximately NIS 5M (1.42M dollars)!\n\rConclusion\rIn this post we were able to uncover and better understand some of the uncertainty that covers a lottery game. Using the geometric distribution we explored the probability of winning the lottery at a specific event, and winning it in the form of a cumulative distribution - Chances of winning up to a given trial.\nUnfortunately, the numbers aren‚Äôt in our favor. You‚Äôd find yourself spending a great deal of money before actually winning the lottery. I‚Äôm definitely not going to tell you what to do with your money, but I hope this blog post helped you grasp a little better the chances of (not) winning the lottery.\n\rFurther reading \\ exploring\rTwo resources I found extremely valuable in learning more about the geometric distribution:\n\rThe Geometric distribution Wikipedia‚Äôs page. I‚Äôm constantly amazed at the vast amount and well articulated statistical pages they have.\n\rContinuing on that, I found the resource that the Wikipedia page relies on extremely helpful: ‚ÄúA modern introduction to probability and statistics : understanding why and how‚Äù.\n\rIf you‚Äôre more of a video kind of person, I highly recommend a video by The Organic Chemistry Tutor about the Geometric distribution. I think he does a superb job in explaining different various statistical analysis and always enjoys his videos.\n\r\r\rNotes\r\r\rAn amusing anecdote is that the Pais organization offers information about ‚ÄòHot‚Äô numbers and the frequency of appearance for each number. Considering that the lottery is random I wouldn‚Äôt rely on such a pattern‚Ä¶‚Ü©Ô∏é\n\rIn this blog post I only explore one aspect of the geometric PMF by looking at number of failures before the first success in a set of \\(k \\in \\{0 , 1, 2, ...\\}\\) attempts. To read more about the PMF I recommend starting with the Wikipedia page of the Geometric distribution.‚Ü©Ô∏é\n\r\r\r","date":1594166400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1594166400,"objectID":"70cc7d5c64ae8b17f831ed2416c8e23a","permalink":"/post/uncertainty-in-the-israeli-lottery/","publishdate":"2020-07-08T00:00:00Z","relpermalink":"/post/uncertainty-in-the-israeli-lottery/","section":"post","summary":"Exploring the probability of winning the lottery at and by a given trial with the Geometric distribution","tags":["probability"],"title":"Defining uncertainty in the Israeli lottery","type":"post"},{"authors":null,"categories":["R"],"content":"\r\r\rp.caption {\rfont-size: 0.9em;\r}\r\rThanks to Almog Simchon for insightful comments on a first draft of this post.\nIntroduction\rLearning R for the past nine months or so has enabled me to explore new topics that are of interest to me, one of them being text analysis. In this post I‚Äôll explain what is Term-Frequency Inverse Document Frequency (tf-idf) and how it can help us explore important words for a document within a corpus of documents1. The analysis helps in finding words that are common in a given document but are rare across all other documents.\nFollowing the explanation we‚Äôll implement the method on four great philosophers‚Äô books: ‚ÄòRepublic‚Äô (Plato), ‚ÄòThe Prince‚Äô (Machiavelli), ‚ÄòLeviathan‚Äô (Hobbes) and lastly, one of my favorite books - ‚ÄòOn Liberty‚Äô (Mill) üòç. Lastly, we‚Äôll see how tf-idf compares to a Bag of Words analysis (word count) and how using both can benefit your exploring of text.\nThe post is aimed for anyone exploring text-analysis and wants to learn about tf-idf. I will be using R to analyze our data but won‚Äôt be explaining the different functions, as this post focuses on the tf-idf analysis. If you wish to see the code, feel free to download or explore the .Rmd source code on my github repository.\n\rTerm frequency\rtf-idf gauges a word‚Äôs value according to two parameters: The first parameter is the term-frequency of a word: How common is a word in a given document (Bag of Words analysis); one method to calculate term frequency of a word is just to count the total number of times each words appears. Another method - which we‚Äôll use in the tf-idf - is, after summing the total number of times a word appears, we‚Äôll divide it by the total number of words in that document, describing term frequency as such:\n\\[tf = \\frac{\\textrm{Number of times a word appears in a document}}{\\textrm{Total number of words in that document}}\\]\nAlso written as \\(tf(t,d)\\) where \\(t\\) is the number of times a term appears out of all words in document \\(d\\). Using the above method we‚Äôll have the proportion of each word in our document, a value ranging from 0 to 1, where common words will have higher values.\nWhile this gives us a value gauging how common a word is in a document, what happens when we have many words across many documents? How do we find unique words for each document? This brings us to idf.\n\rInverse document frequency\rInverse document frequency accounts for the occurrence of a word across all documents, thereby giving a higher value to words appearing in less documents. In this case, for each term we will calculate the log ratio2 of all documents divided by the number of documents that word appears in. This gives us the following:\n\\[ idf = \\log {\\frac{\\textrm{N documents in corpus}}{\\textrm{n documents containing the term}}}\\]\nAlso written as \\(idf = \\log{\\frac{N}{n(t)}}\\) Where \\(N\\) is the total number of documents in our corpus and \\(n(t)\\) is the number of documents the word appears within our corpus of documents.\nTo those unfamiliar, a logarithmic transformation helps in reducing wide-ranged numbers to smaller scopes. In this case, if we have 7 documents, and our term appears in all 7 documents, we‚Äôll have following idf value: \\(log_e(\\frac{7}{7}) = 0\\). What if we have a term that appears in only 1 document out of all 7 documents? We‚Äôll have the following: \\(log_e(\\frac{7}{1}) = 1.945\\). Even if a word appears in only 1 document out of 100, a logarithmic transformation will reduce its high value to mitigate bias when we multiply it with its \\(tf\\) value.\nSo what do we understand from the idf? Since our numerate always remains the same (N documents in corpus), the idf of a word is contingent upon how common it is across documents. Words that appear in a small number of documents will have a higher idf, while words that are common across documents will have a lower idf.\n\rTerm-Frequency Inverse Document Frequency (tfidf)\rOnce we have the term frequency and inverse document frequency for each word we can calculate the tf-idf by multiplying the two: \\(tf(t,d) \\cdot idf(t,D)\\) where \\(D\\) is our corpus of documents.\nTo summarize our explanation: The two paramteres used to calculate the tf-idf provide each word with a value for its importance to that document in that corpus of text. Ideally We take words that are common within a document and that are rare across documents. I write ideally because as we‚Äôll see soon, we might have words that are extremely common in one document but are filtered out because they‚Äôre evident in all documents (can happen in a small corpus of documents). This also highlights the question as to what is important; I define important as contributing to understanding a document in comparison to all other documents.\n\rFigure 1: Using tf-idf we can calculate how common a word is within a document and how rare is it across documents\r\rNow that we have some background as to how tf-idf works, let‚Äôs dive in to our case study.\n\rTF-IDF on political theorists.\rI‚Äôm a big fan of political theory. I have a small collection at home and always like to read and learn more about it. Except for Mill, we read Plato, Machiavelli and Hobbes in our BA first semester course in political theory. While some of the theorists overlap to some degree, over-all they discuss different topics. tf-idf will help us distinguish important words specific to each book, in a comparison across all books.\nBefore we conduct our tf-idf we‚Äôd like to explore our text a bit. The following exploratory analysis is inspired from Julia Silge‚Äôs blog post ‚ÄòTerm Frequency and tf-idf Using Tidy Data Principles‚Äô, a fantastic read.\n\rData collection \u0026amp; Analysis\rThe package we‚Äôll use to gather the data is the {gutenbergr} package. It enables us to access the Project Gutenberg free books, a library of over 60,000 free books. As many other amazing things in R someone, in this case David Robinson, created a package for it. All we need to do is download them to our computer.\nMill \u0026lt;- gutenberg_download(34901)\rHobbes \u0026lt;- gutenberg_download(3207)\rMachiavelli \u0026lt;- gutenberg_download(1232)\rPlato \u0026lt;- gutenberg_download(150)\rSeveral of the books contain sections at the beginning or at the end that aren‚Äôt relevant for our analysis. For example long introductions from contemporary scholars; another whole different book at the end, etc. These can confound our analysis and therefore we‚Äôll exclude them. In order to conduct our analysis we also need all the books we collected in one object.\nOnce we are able to clean the books, this is what our text looks like:\nremove_text \u0026lt;- function(book, low_id, top_id = max(rowid), author = deparse(substitute(book))){\rbook %\u0026gt;%\rmutate(author = as.factor(author)) %\u0026gt;% rowid_to_column() %\u0026gt;% filter(rowid \u0026gt;= {{low_id}}, rowid \u0026lt;= {{top_id}}) %\u0026gt;% select(author, text, -c(rowid, gutenberg_id))}\rbooks \u0026lt;- rbind(\rremove_text(Mill, 454),\rremove_text(Hobbes, 360, 22317),\rremove_text(Machiavelli, 464, 3790),\rremove_text(Plato, 606))\r## # A tibble: 45,490 x 2\r## author text ## \u0026lt;fct\u0026gt; \u0026lt;chr\u0026gt; ## 1 Mill \u0026quot;\u0026quot; ## 2 Mill \u0026quot;\u0026quot; ## 3 Mill \u0026quot;CHAPTER I.\u0026quot; ## 4 Mill \u0026quot;\u0026quot; ## 5 Mill \u0026quot;INTRODUCTORY.\u0026quot; ## 6 Mill \u0026quot;\u0026quot; ## 7 Mill \u0026quot;\u0026quot; ## 8 Mill \u0026quot;The subject of this Essay is not the so-called Liberty of the Will, ~\r## 9 Mill \u0026quot;unfortunately opposed to the misnamed doctrine of Philosophical\u0026quot; ## 10 Mill \u0026quot;Necessity; but Civil, or Social Liberty: the nature and limits of th~\r## # ... with 45,480 more rows\rEach row is some text with chapters separated by headings and a column referencing who is the author. Our data frame consists of ~45,000 rows with the filtered text from our four books. Tf-idf can also be done on any n-grams we choose (number of consequent words). We could calculate the tf-idf for each bigram of words (two-words), trigram, etc. I find a unigram an appropriate approach both for tf-idf and especially now when we want to learn more about it. We just saw that our text is in the form of sentences, so let‚Äôs break it into single words.\n## # A tibble: 12 x 4\r## # Groups: author [4]\r## author word n sum_words\r## \u0026lt;fct\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;int\u0026gt; \u0026lt;int\u0026gt;\r## 1 Hobbes the 14536 207849\r## 2 Hobbes of 10523 207849\r## 3 Hobbes and 7113 207849\r## 4 Plato the 7054 118639\r## 5 Plato and 5746 118639\r## 6 Plato of 4640 118639\r## 7 Mill the 3019 48006\r## 8 Mill of 2461 48006\r## 9 Machiavelli the 2006 34821\r## 10 Mill to 1765 48006\r## 11 Machiavelli to 1468 34821\r## 12 Machiavelli and 1333 34821\rWe see that stop-words dominant the frequency of occurrences. That makes sense as they are commonly used, but they‚Äôre not usually helpful for learning about a text, specifically here. We‚Äôll start by exploring how the word frequencies occur within a text:\nThe plot above shows the frequency of terms across documents. We see some words that appear frequently (higher proportion = right side of the x-axis) and many words that are rarer (low proportion). Actually, I had to limit the x-axis or otherwise it would distort the plot with words that are extremely common.\nTo help find useful words with the highest tf-idf from each book, we‚Äôll remove stop words before we extract the words with a high tf-idf value:\n\r\rAuthor\r\rWord\r\rn\r\rSum words\r\rTerm Frequency\r\rIDF\r\rTF-IDF\r\r\r\r\r\rMill\r\ropinion\r\r150\r\r48006\r\r0.0094132\r\r0.0000000\r\r0.0000000\r\r\r\rHobbes\r\rgod\r\r1047\r\r207849\r\r0.0149024\r\r0.0000000\r\r0.0000000\r\r\r\rMachiavelli\r\rprince\r\r185\r\r34821\r\r0.0172704\r\r0.2876821\r\r0.0049684\r\r\r\rPlato\r\rtrue\r\r485\r\r118639\r\r0.0152953\r\r0.0000000\r\r0.0000000\r\r\r\r\r\r Random sample of words and their corresponding tf-idf values\r\r\r\r\rAbove we have our tf-idf for a given word from each document. I removed stop-words and calculated the tf-idf for each word in each book. For Hobbes the word ‚ÄòGod‚Äô appears 1047 times, thus has a \\(tf\\) of \\(\\frac {1047} {207849}\\) and an idf of 0 (since it appears in all documents), so it‚Äôll have a tf-idf of 0.\nWith Machiavelli the word prince appears 185 times, with a \\(tf\\) of \\(\\frac {185} {34821}\\), resulting in a proportion of 0.0173. The word prince has an idf of 0.288 \\((log_e(\\frac 4 {3}))\\), as there are 4 documents and it appears in 3 of them, so a total tf-idf value of \\(0.0173 \\cdot 0.288\\) = \\(0.00497\\).\n\rTf-idf plot\rAs we wrap up our tf-idf analysis, We don‚Äôt want to see all words and their tf-idf, but only words with the highest tf-idf value for each author, indicating the importance of a word to a given document. We can look at these words by plotting the top 10 highest valued tf-idf words for each author:\n ggplot(data = books_for_plot, aes(x = word, y = tf_idf, fill = author))+\rgeom_col(show.legend = FALSE)+\rlabs(x = NULL, y = \u0026quot;tf-idf\u0026quot;)+\rcoord_flip()+\rscale_x_reordered()+\rfacet_wrap(~ author, scales = \u0026quot;free_y\u0026quot;, ncol = 2)+\rlabs(title = \u0026quot;\u0026lt;b\u0026gt;Term Frequency Inverse Document Frequency\u0026lt;/b\u0026gt; - Political theorists\u0026quot;,\rsubtitle = \u0026quot;tf-idf for The Leviathan (Hobbes), On Liberty (Mill), The Prince (Machiavelli)\\nand Republic (Plato)\u0026quot;)+\rscale_fill_manual(values = plot_colors)+\rtheme_post+\rtheme(plot.title = element_markdown())\rLovely!\nLet‚Äôs review each book and see what we can learn from our tf-idf analysis. My memory of these books is kind of rusty but I‚Äôll try my best:\n\rHobbes: Hobbes in his book describes the natural state of human beings and how they can leave it by revoking many of their right to the sovereign who will facilitate order. In his book he describes the soveragin (note the ‚Äòa‚Äô) as needed to be strict, rigorous and hath.\n\rMachiavelli: Machiavelli provides a leader with a guide on how to rule his country. He prefaces his book with an introduction letter to the Duke, the recipient of his work. Machiavelli throughout the book conveys his message with examples of many princes, Alexander the great, the Orsini brothers and more. Several of his examples include mentioning of Italy (where he resides), specifically Venetians and Milan.\n\rMill: Mill in his book ‚ÄòOn Liberty‚Äô describes the importance of freedom and liberty for individuals. He does so by describing the relation between people and their society and other relations with the social. He highlights in his discussion on liberty a person‚Äôs belonging; these can be Feelings or basically anything personal. Protecting the personal is important for the development of both society and that of the individual.\n\rPlato: Plato‚Äôs book consists of 10 chapters and it is by far the longest compared to the others. The book is written in the form of a dialogue with replies between Socrate and his discussants. Along Socrate‚Äôs journey to finding out what is the meaning of justice he talks to many people, among them Glaucon, Thrasymachus and Adeimantus. In one section Socrates describes a just society with distinct classes such as the guardians. The classes should receive appropriate education, for e.g.¬†gymnastics for the guardians.\n\r\rWith the above analysis we were able to explore uniqueness of words for each book across all books. Some words provided us with great insights while others didn‚Äôt necessarily help us despite their uniqeness, for example, the names of discussants with Socrate. Tf-idf gauges them as important (as to how I defined importance here) to distinguish between Plato‚Äôs book and the others, but I‚Äôm sure they‚Äôre not the first words that come to mind when someone talks about the Republic.\nThe analysis also shows this methodology‚Äôs value addition is not in just applying tf-idf - or any other statistical analysis ‚Äì rather its power lies in its explanatory abilities. In other words, tf-idf provides us with a value indicating the importance of a word to a given document within a corpus, it is our job to take that extra step interpreting and contextualizing the output.\n\rComparing to Bag Of Words (BOG)\rA common text analysis is a word count I discussed earlier, also known as Bag of Words (BoW). This is an easy to understand method that can be done easily when exploring text. However, relying only on a bag of words method to draw insights can limit its usefulness if other analytic methods are not also included. The BoW relies only on the frequency of a word, so if a word is common across all documents, it might show up in all of them and not contribute to finding unique words for each document.\nNow that we have our books we can also explore the raw occurrence of each word to compare it to our above tf-idf analysis:\nggplot(data = bow_books, aes(x = reorder(word_with_color,n), y = n, fill = author))+\rgeom_col(show.legend = FALSE)+\rlabs(x = NULL, y = \u0026quot;Word Frequency\u0026quot;)+\rcoord_flip()+\rscale_x_reordered()+\rfacet_wrap(~ author, scales = \u0026quot;free\u0026quot;, ncol = 2)+\rlabs(title = \u0026quot;\u0026lt;b\u0026gt;Term Frequency\u0026lt;/b\u0026gt; - Political theorists\u0026quot;)+\rscale_fill_manual(values = plot_colors)+\rtheme_post+\rtheme(axis.text.y = element_markdown(),\rplot.title = element_markdown(),\rstrip.text = element_text(color = \u0026quot;grey50\u0026quot;))\r\rFigure 2: Term frequency plot with words that are common across documents in bold\r\rThe above plot amplifies, in my opinion, tf-idf‚Äôs contribution in finding unique words for each document. While many of the words are similar to those we found in the previous tf-idf analysis, we also draw words that are common across documents. For example, we see the frequency of ‚ÄòTime‚Äô, ‚ÄòPeople‚Äô and ‚ÄòNature‚Äô twice in different books and words such as ‚ÄòTrue‚Äô and ‚ÄòTruth‚Äô with similar meanings do so too (however this could have happened in tf-idf too).\nHowever, the Bag of Words also provided new words we didn‚Äôt see earlier. Here we can learn on new words like Power in Hobbes, Opinions in Mill and more. With the bag of words we get words that are common without controlling for other texts, while the tf-idf searches for words that are common within but are rare across.\n\rClosing remarks\rIn this post we learned the term frequency inverse document frequency (tf-idf) analysis and implemented it on four great political theorists. We finished by exploring tfidf in comparison to a bag of words analysis and showed the benefits of each. This also emphasizes how we define important: Important to a document by itself or important to a document compared to other documents.\rThe definition of ‚Äòimportant‚Äô here also highlights tf-idf heuristic quantifying approach (specifically the idf) and thus should be used with caution. If you are aware of theoretical development of it I‚Äôd be glad to read more about it.\nBy now you should be equipped to give tf-idf a try yourself on a corpus of documents you find appropriate.\n\rWhere to next\r\rFurther reading about text analysis - If you want to read more on text mining with R, I highly recommend the Julia Silge \u0026amp; David Robinson‚Äôs text mining with R bookand/or exploring the {quanteda} package.\n\rText datasets - As to finding text data, you can try the {gutenbergr} package that gives access to thousands of books, a #TidyTuesday data set or collect tweets from Twitter using the {rtweet} package.\n\rOther posts of mine - If you‚Äôre interested in other posts of mine where I explore some text you can read my Israeli elections Twitter tweets analysis.\n\r\rThat‚Äôs it for now. Feel free to contact me for any and all comments!\nNotes\r\r\r\rA single document can be a book, chapter, paragraph or sentence, it all depends on your research and what you define as an ‚Äòentity‚Äô within a corpus of text.‚Ü©\n\rWhat‚Äôs log ratio? In general, and for the purpose of the tf-idf, a logarithm transformation (in short \\(log\\)) helps in reducing wide ranged numbers to smaller scopes. Assuming we have the following \\(\\log _{2}(16) = x\\), we ask ourselves (and calculate) 2 in the power of what (x) will give us 16. so in this case 2^3 will give us 16, which is basically written as \\(\\log _{2}(16) = 3\\). In order to generalize it, \\(\\log _{b}(x) = y\\), means b is the base we will raise to the power of y to reach x. Therefore written oppositely as \\(b^y = x\\). The common uses of log are \\(\\log_2\\), \\(\\log_{10}\\) and \\(log_e\\), also written as plain log.‚Ü©\n\r\r\r","date":1590883200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1590883200,"objectID":"f4238aec4b93fcbc99b765d9c408e3bc","permalink":"/post/learning-tfidf-with-political-theorists/","publishdate":"2020-05-31T00:00:00Z","relpermalink":"/post/learning-tfidf-with-political-theorists/","section":"post","summary":"Learning tf-idf through political theorists.","tags":["tidytext"],"title":"Learning Tfidf with Political Theorists","type":"post"},{"authors":null,"categories":[],"content":"\r\rp.caption {\rfont-size: 0.8em;\r}\r\r\rFigure 1: No more sending html files - Host your interactive graphs online with GitHub pages\r\rBackground\rA few weeks ago I gave my first talk about analyzing and visualizing data in R. I shared with participants - political activists - many of R‚Äôs abilities, one of them creating ‚Äòinteractive visualizations‚Äô which I very much like. I believe interactive graphs provide some edge to a static graph when used appropriately. The first time I created an interactive graph was probably for #TidyTuesday, which left me with the question how to provide others the ability to interact with it?\nTL;DR\rFor those who already have a rendered html file:\nPush the rendered html file to GitHub repository.\n\rGo into your GitHub repository where the html file is located.\n\rOn the top right click Settings  Scroll down to GitHub Pages section  instead of none choose master branch\r\rThat‚Äôs it! You can now find your html widget under username.GitHub.io/repo-name/file.html\n\r\rWhy host it online?\r\rSending html files of your new interactive graph can be cumbersome, especially if it‚Äôs a work in progress with updated versions you keep sending. Enough with those html_4_final_final kind of files - Seriously, not cool.\n\rYou participated on #Tidytuesday and want to share your graph for others to explore themselves. In this case I recommend recording a gif or video where you interact with the graph and also host the file online for others to explore.\n\rBecause once you set up your GitHub page up, it‚Äôs just 2 more lines of code saving it as a file and pushing it to your repository.\n\r\rThere are many great tutorials for hosting slides and static files online that you can find here, here, and here. I found these very useful for hosting slides from the talk I gave and wanted to share that same tutorial aimed at hosting html graphs, technically the same file (html). The format is identical, but I remember when starting off with R I didn‚Äôt know how. I was naive and just pushed my html file to GitHub thinking once it‚Äôs there I can interact with it. Although it‚Äôs pretty much that, we first need to turn our GitHub repository into a GitHub page.\n\n\rSaving an html file\rSaving an interactive graph - such as a {leafly}, {highcharter}, {plotly} or any other interactive object - can be done with various packages. Here I‚Äôll use the {htmlwidgets} package since, in my opinion, its default settings nicely fit the browser page. You can also try out other packages and see what suits you (for e.g.¬†{htmltools}).\nLet‚Äôs take a #TidyTuesday dataset I used to create a {highcharter} map of median GDPR fines. If you‚Äôre interested in how to make such a map, you can find the source code here, or a fantastic blog detaling the process by Kyle Cuilla.\nSo we have our map ready and can interact with it within R as we can see in the gif below:\n\rFigure 2: Our interactive graph is all set in R, What‚Äôs next?\r\rFirst, we want to render it into an html file. Make sure to save your graph into an object, let‚Äôs say for this example our object is called ‚Äòhc_gdpr‚Äô. Before we save it using htmlwidget::saveWidget, we can define how the graph will render on the webpage. If you‚Äôd like to adjust it‚Äôs width and height parameters, you can add them to the html object which is saved as a list, for e.g.:\nhc_gdpro$width \u0026lt;- \u0026quot;1400px\u0026quot;\rhc_gdpro$height \u0026lt;- \u0026quot;700px\u0026quot;\rAnd now we can write our file to our current working directory. The knitrOptions takes our height and width configuration - Play around with it until it‚Äôs satisfactory for you; depends for what I need it but I find the default settings - seems like 100% width and 100% height - adequate for my needs:\nsaveWidget(hc-gdpr, \u0026quot;hc-gdpr.html\u0026quot;, selfcontained = TRUE, knitrOptions = list())\r\rGitHub Pages \rGreat, we now have a new html file containing our graph saved in our working directory. If you open it up it should run smoothly, only notice it‚Äôs doing so on your local computer. If you send the file as is, it will work on someone else‚Äôs computer if even if they don‚Äôt have R. But what if we update our graph such as fix a typo? That‚Äôs where hosting it online comes in.\nWe‚Äôll be hosting the html file on GitHub Pages, so if you‚Äôre not familiar with Git or working with GitHub through R I recommend following happygitwithr tutorial for getting your account set up.\nYou can open a new repository for hosting your html files, but I think using the one where you host your source code is better. For example, if it‚Äôs a #TidyTuesday graph like the example here, I just push it along with my R source code for that week. Make sure to include the html file, hosting only the code file (.R / .Rmd) won‚Äôt render it interactive; of course you can also host the html without the code (but hey, sharing is caring when it comes to open source).\n\rFigure 3: After writing your html object, push it to a GitHub repository\r\rNow that we have our file hosted we can assign our GitHub page: In the repository your html file is hosted, click on Settings on the top right and scroll down until you reach the ‚ÄòGitHub Pages‚Äô section. There you want to enable the GitHub Pages by changing the default ‚ÄòNone‚Äô to master-branch.\n\rFigure 4: Turn your Github repository into a Github page\r\rThat‚Äôs it!\nYou can find your hosted html file at username.github.io/repo-name/file.html. For instance, the example I used for this blog post can be found here.\nOnce you have a GitHub page you can do so much more with it (such as host a personal website), but that‚Äôs for a different post altogether. If you want to learn more about adding a theme to GitHub Pages check out this post by GitHub.\nAnother option to host your html file would be on Rpubs, but I find the GitHub page option more than adequate for my needs.\nEnjoy!\n\r","date":1588291200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1588291200,"objectID":"cab8ef7be328a835a26c96c1d9d5ec73","permalink":"/post/sharing-interactive-charts/","publishdate":"2020-05-01T00:00:00Z","relpermalink":"/post/sharing-interactive-charts/","section":"post","summary":"A walk through for sharing your interactive visualizations on GitHub Pages","tags":[],"title":"Hosting interactive graphs online","type":"post"},{"authors":[],"categories":["R"],"content":"\rIntroduction\rIsrael had its 3rd election within 12 months on March 2, 2020. This is because our Knesset - Hebrew term for house of representatives - wasn‚Äôt able to form or hold a government after each of the previous elections. As I won‚Äôt get into the politics of why they didn‚Äôt succeed in forming one (get it? politics üòâ), I do want to take the opportunity and analyze some tweets posted in the time before and after the elections.\nWhen we think of a data aggregating tweets, many questions arise - who, what, when, where and more about our data. Namely, with the collected data I want to answer the following questions:\nWhat was the frequency of tweets associated with the word ‚Äòelections‚Äô?\rWho tweeted the most?\rWhat was the most common #Hashtag tweeted?\rWhich tweet was most liked and which was retweeted the most?\rWhat were the most common words and bigrams (two words) in tweets?\r\r\rGathering the data \rTwitter‚Äôs API allows scraping 6-9 days back for free. Therefore, I scraped the data already on March 7, 2020 and saved it for later use.\nLet‚Äôs start with the packages we‚Äôll use:\nlibrary(rtweet)\rlibrary(tidyverse)\rlibrary(tidytext)\rlibrary(igraph)\rlibrary(hrbrthemes)\rlibrary(ggraph)\rlibrary(extrafont)\rI could use a consistent plot theme throughout the post but I‚Äôll probably be editing each one a bit, while also some are not our regular graphs. With that said, There are some tweaks that will be consistent acorss several of the plots. Therefore, let‚Äôs create a theme function as a supplement to all other theme arguments I‚Äôll use that will save a few lines of code:\nmini_theme \u0026lt;- function(family = \u0026quot;Roboto Condensed\u0026quot;, tsize = 16) {\rtheme_classic() +\rtheme(text = element_text(family = family),\raxis.ticks = element_blank(),\raxis.line = element_blank(),\rplot.title = element_text(size = tsize))}\rNext we‚Äôll gather the tweets we need:\nelections_raw \u0026lt;- search_tweets(\u0026quot;◊ë◊ó◊ô◊®◊ï◊™\u0026quot;, n = 18000, retryonratelimit = TRUE)\rTo gather the tweets we can use the {rtweet} package which is amazing for collecting Twitter data. As I mentioned earlier, I already scraped the data a few days after the elections but left the command here to show what we did and how easy it is to do it. I searched only one term, ‚Äòelections‚Äô in Hebrew, and rtweet gathered all tweets containing that word.\nWhat did our search yield? Let‚Äôs have a look:\ndim(elections)\r## [1] 16560 90\r16,560 rows and 90 columns! As we can see, the {rtweet} package brings back a lot of information!\nSome Caveats:\rBefore we begin, I will say this post doesn‚Äôt aim to be representative of the discussions that were held during the election period. As a matter of fact, nor does it aim to be representative of the twitter discussions surrounding the elections. this is due to two main reasons:\nTwitter isn‚Äôt common in Israel at all. I‚Äôm not sure what‚Äôs the usage rate but it‚Äôs definitely not representative of the Israeli population.\n\rI searched for only one word - elections (in Hebrew) - which yielded some 16560 tweets. This is definitely not a large enough pool of tweets to claim for representation.\n\r\rWith that said, the data gathered provides an opportunity to look at some Twitter data from the election period and motivate others to use the {rtweet} package, so why not give it a go.\n\r\rTweet frequency\rFirst, let‚Äôs see how the tweets distribute across the time span we searched for. we can create a quick time plot using the ts_plot() argument from the {rtweet} package:\nelections %\u0026gt;% ts_plot(\u0026quot;2 hours\u0026quot;)+\rgeom_line(size = 1, color = \u0026quot;black\u0026quot;)+\rmini_theme()+\rscale_x_datetime(date_breaks = \u0026quot;1 day\u0026quot;,date_labels = \u0026quot;%d %b\u0026quot;)+\rlabs(x= NULL, y = NULL,\rtitle = \u0026quot;Tweet frequency throughout the Israeli elections week\u0026quot;,\rsubtitle = \u0026quot;Tweets aggregated by two-hour interval. Only tweets containing the word \u0026#39;elections\u0026#39;\\nin Hebrew were gathered\u0026quot;)+\rgeom_text(aes(x = as.POSIXct(\u0026quot;2020-03-02 23:00:00\u0026quot;), y = 435, label = \u0026quot;10 PM:\\nPolls close\u0026quot;),\rhjust = 0, size = 3, family = \u0026quot;Roboto Condensed\u0026quot;)+\rgeom_vline(xintercept = as.POSIXct(\u0026quot;2020-03-02 22:00\u0026quot;),linetype = \u0026quot;dashed\u0026quot;, size = 0.5, color = \u0026quot;black\u0026quot;, alpha = 5/10)+\rtheme(plot.subtitle = element_text(color = \u0026quot;gray70\u0026quot;))\rInteresting - we see the number of tweets during the closing time is equivalent to that of midday on March 4th. Most of the votes were counted by the end of March 3rd, so I can‚Äôt really put my finger on what this jump represents. After all, I collected tweets containing our word so it could have been that many people tweeted that specific term in that time slot. Anyway, I wasn‚Äôt able to find anything interesting that happened on the news that day but feel free to explore and offer suggestions.\n\rUsers with most tweets\rNext, let‚Äôs look at who tweeted the most:\nelections %\u0026gt;% count(screen_name, sort = T) %\u0026gt;% slice(1:15) %\u0026gt;% mutate(screen_name = reorder(screen_name,n)) %\u0026gt;% ggplot(aes(x= screen_name, y= n))+\rgeom_col(fill = \u0026quot;gray70\u0026quot;)+\rcoord_flip()+\rscale_y_continuous(breaks = seq(0,180, 30), labels = seq(0,180,30))+\rlabs(x = \u0026quot;Screen name\u0026quot;, y = \u0026quot;Number of tweets\u0026quot;, title = \u0026quot;Top 15 users tweeting the word \u0026#39;elections\u0026#39;\u0026quot;)+\rmini_theme()+\rtheme(text = element_text(family = \u0026quot;Calibri\u0026quot;),\raxis.text = element_text(size = 12),\raxis.title.y = element_blank())\rWe see that many news companies tweeted a lot using the word ‚Äòelections‚Äô: ‚Äònewisrael13‚Äô, ‚Äòkann_news‚Äô, ‚ÄòMaarivOnline‚Äô, ‚ÄòRotterNews‚Äô, ‚Äòbahazit_news‚Äô, ‚ÄòRotterNet‚Äô. I personnaly don‚Äôt recognize the rest, but on the other hand I use Twitter mostly to follow R and academic related tweets, not necessarily Israeli politics.\n\rCommon Hashtags\rWhen using the {rtweet} package to gather twitter data, one of the variables collected is the hashtags used in tweets. Although it doesn‚Äôt require too many lines of code to extract hashtags out of text, I think this is an amazing feature that shows the effort and details Michael W. Kearney and contributors put into the package.\nAccording to Wikipedia, a ‚ÄòHashtag‚Äô ‚Äúis a type of metadata tag used on social networks such as Twitter and other microblogging services.‚Äù, that basically tags the message with a specific theme. This helps to see trends and themes in a macro level.\nOK then, let‚Äôs see what we have:\nhashtags \u0026lt;- elections %\u0026gt;% select(hashtags) %\u0026gt;% unlist() %\u0026gt;% as.tibble() %\u0026gt;% mutate(value = tolower(value)) %\u0026gt;% count(value, name = \u0026quot;Count\u0026quot;, sort = T) %\u0026gt;%\rmutate(value = reorder(value, Count),\riscorona = ifelse(value == \u0026quot;◊ß◊ï◊®◊ï◊†◊î\u0026quot; | value == \u0026quot;coronavirus\u0026quot;, \u0026quot;y\u0026quot;, \u0026quot;n\u0026quot;)) %\u0026gt;% filter(!is.na(value)) %\u0026gt;% slice(1:20)\rggplot(data = hashtags, aes(x = Count, y = value, fill = iscorona))+\rgeom_col(show.legend = FALSE)+\rscale_fill_manual(values = c(y = \u0026quot;#1DA1F2\u0026quot;, n = \u0026quot;gray70\u0026quot;))+\rlabs(y = NULL, x = \u0026quot;Number of Tweets\u0026quot;, title = \u0026quot;Top 20 Hashtags addressing the Israeli elections\u0026quot;)+\rmini_theme()+\rtheme(text = element_text(family = \u0026quot;Calibri\u0026quot;),\raxis.text = element_text(size = 12))\rThe tweets include pretty much what we expect - hashtags about the elections - with the two leading ones being ‚Äòelections‚Äô and ‚Äòelections2020‚Äô. We also see a peculiar hashtag ‚Äòright_following_right_people‚Äô, and others such as ‚ÄòNetanyahu‚Äô (the Prime minister at the time), ‚ÄòIsrael‚Äô and others.\nI highlighted in blue an interesting hashtag at the time - Corona (in hebrew) and coronavirus. The elections were held on March 2, 2020, a little bit after the first cases reached Israel. Little did we know how it will affect us (I‚Äôm finalzing this post on April 18, 2020, and only now we‚Äôre starting to get back to routine. Slowly)\n\rMost liked and retweeted\rLet‚Äôs have a look at which tweet was most liked. Twitter doesn‚Äôt define it as ‚Äòlikes‚Äô but as ‚Äòfavorite‚Äô, or at least in the data that is collected through the {rtweet} package. Since I will want to gather the most of something - both favorite and later retweeted - I‚Äôll create a function that will minimize re-writing the code.\nThe function takes in a variable, reorders our dataset according to the variable we declared, extracts the first row and then pulls (extracts) the status id of that tweet. Lastly, the blogdown::shortcode enables to embed tweets, youtube videos and more into a blogdown post such as this, so we end the function by inserting our status id into that. For those just getting into functions notice that within the arrange argument we insert our variable in two curly brackets {{}}. This is a powerful feature of {rlang} when you want to manipulate a variable in a dataframe within a function. You can read more about that here.\nget_most \u0026lt;- function(var){\relections %\u0026gt;% arrange(desc({{var}})) %\u0026gt;% .[1,] %\u0026gt;% pull(status_id) %\u0026gt;% blogdown::shortcode(\u0026#39;tweet\u0026#39;,.)\r}\rNow Let‚Äôs see which tweet was most liked during that week:\n\r◊ô◊ï◊™◊® ◊û◊î◊õ◊ú, ◊ê◊†◊ô ◊©◊û◊ó ◊©◊ú◊ê ◊ô◊î◊ô◊ï ◊¢◊ï◊ì ◊ë◊ó◊ô◊®◊ï◊™ ◊ë◊©◊ë◊ô◊ú ◊î◊û◊©◊§◊ó◊î ◊©◊ú◊ô ◊©◊°◊ë◊ú◊î ◊ë◊í◊ë◊ï◊®◊î ◊©◊†◊î ◊ï◊®◊ë◊¢. ◊®◊¢◊ï◊™ ◊¢◊ë◊®◊ô ◊ï◊¢◊†◊® üòç\n\u0026mdash; ◊¢◊û◊ô◊™ ◊°◊í◊ú Amit Segal (@amit_segal) March 2, 2020  \rThe tweet is by ‚ÄòAmit Segal‚Äô - an Israeli news reporter - and it says (my translation):\n\r‚ÄúMore than anything, I‚Äôm glad there won‚Äôt be another elections for my family that suffered in honors a year and a quarter. Reut, Ivri and Aner üòç‚Äù\n\rHa, interestingly he wrote it before the end of the elections, hopefully he‚Äôs right!\nNow let‚Äôs look at the most re-tweeted tweet:\n\r◊ê◊ù ◊î◊î◊ß◊ú◊ò◊î ◊©◊ú ◊ô◊ï◊¢◊¶◊ï ◊©◊ú ◊í◊†◊• ◊û◊ë◊ï◊©◊ú◊™ ◊ï◊©◊ß◊®◊ô◊™ (◊õ◊ì◊ë◊®◊ô ◊í◊†◊• ◊¢◊õ◊©◊ô◊ï), ◊ê◊ñ ◊ú◊û◊î ◊í◊†◊• ◊§◊ô◊ò◊® ◊ê◊ï◊™◊ï?\n◊ô◊ï◊¢◊¶◊ï ◊©◊ú ◊í◊†◊• ◊§◊ï◊ò◊® ◊ë◊í◊ú◊ú ◊©◊ê◊û◊® ◊ê◊™ ◊î◊ê◊û◊™ ◊©◊õ◊ï◊ú◊ù ◊ô◊ï◊ì◊¢◊ô◊ù: ◊í◊†◊• ◊ú◊ê ◊ô◊õ◊ï◊ú ◊ú◊î◊ô◊ï◊™ ◊®◊ê◊© ◊û◊û◊©◊ú◊î. ◊ê◊†◊ó◊†◊ï ◊õ◊ü. ◊¢◊ï◊ì 2 ◊û◊†◊ì◊ò◊ô◊ù ◊ú◊ú◊ô◊õ◊ï◊ì ◊ï◊ê◊†◊ó◊†◊ï ◊û◊ï◊¶◊ô◊ê◊ô◊ù ◊ê◊™ ◊î◊û◊ì◊ô◊†◊î ◊û◊î◊§◊ú◊ï◊†◊ò◊®, ◊û◊ï◊†◊¢◊ô◊ù ◊¢◊ï◊ì ◊ë◊ó◊ô◊®◊ï◊™ ◊ï◊û◊ß◊ô◊û◊ô◊ù ◊û◊û◊©◊ú◊î\n\u0026mdash; Benjamin Netanyahu (@netanyahu) February 28, 2020  \rThe tweet is by Benjamin Netanyahu, at the time the prime minister of Israel, who writes:\n\r‚ÄúIf the recording of Gantz‚Äôs advisor is orcherstrated and fabricated (according to Gantz‚Äôs words just now), why did Gantz fire him? Gantz‚Äôs advisor was fired because he said the truth everyone knows: Gantz can‚Äôt be a prime minister. We can. 2 more mandates to the Likkud and we are taking the country out of the plonter, preventing another election and form a government‚Äù\n\rThis came after the exposure of a secret recording of Gantz in a closed meeting, A few days before election day.\n\rWordcloud and bigrams\rLet‚Äôs have a look at two more text-related analyses:\nA word-cloud\n\rBigrams (two-words) from our text\r\rWe could try out more algorthims but I‚Äôll save them for a different post (feel free to try on your own).\nWordcloud\rIn order to tackle the wordcloud, I‚Äôll break up all the tweets into single words, filter any Hebrew stop words (file found online) and all English words. The decision to filter English words is mainly because I‚Äôm interested in the Hebrew sentences, but also because most the common English words used in our data are those of Twitter user names cited when replying to a tweet:\nhe_stopwords \u0026lt;- read_tsv(\u0026quot;https://raw.githubusercontent.com/gidim/HebrewStopWords/master/heb_stopwords.txt\u0026quot;, col_names = \u0026quot;word\u0026quot;)\relection_token \u0026lt;- elections %\u0026gt;% unnest_tokens(word, text) %\u0026gt;% select(word) %\u0026gt;%\ranti_join(he_stopwords) %\u0026gt;% count(word, sort = T) %\u0026gt;%\rfilter(!grepl(\u0026quot;([a-z]+|◊ë◊ó◊ô◊®◊ï◊™)\u0026quot;, word), n\u0026gt;= 150)\rNow we can create a wordcloud of words appearing more than 150 times using {wordcloud2} package1:\nwordcloud2::wordcloud2(election_token, color = \u0026quot;#1DA1F2\u0026quot;, shape = \u0026quot;circle\u0026quot;)\r\rFigure 1: Wordcloud excludes Hebrew stop words and the word ‚Äòelections‚Äô\r\r\nWhat we can see is many of the words we‚Äôd expect: Political candidates, government, fourth (in the context of fourth elections), partis‚Äô names and more. I‚Äôll provide a more thorough discussion following our bigram plot below, as I believe it addresses many of the same words.\n\rCommon Bigrams\rLike we did before, we can break up our text data into two word observations, also known as bigrams. In order to account for all combinations, we break up the sentence to fit all possible options. For example, assume we have the following sentence:\n‚ÄúDanny went to vote yesterday‚Äù\nUsing the unnest_tokens we‚Äôll break the sentence into the following bigrams:\nDanny went\n\rwent to\n\rto vote\n\rvote yesterday\r\rWhich gives us all possible options. We will also include two columns consisting of the bigram broken up into single words. This will help in filtering out bigrams containing Hebrew stop words or English words. I‚Äôll not run through the following code but instead will point you to David Ronbinson \u0026amp; Julia Silge ‚ÄòText Mining with R‚Äô Book for further reading.\nelec_bigram \u0026lt;- elections %\u0026gt;%\rselect(text) %\u0026gt;% unnest_tokens(bigram, text, token = \u0026quot;ngrams\u0026quot;, n = 2) %\u0026gt;%\rseparate(bigram, into = c(\u0026quot;word1\u0026quot;, \u0026quot;word2\u0026quot;), sep = \u0026quot; \u0026quot;, remove = FALSE) %\u0026gt;% filter(!word1 %in% he_stopwords$word,\r!word2 %in% he_stopwords$word,\r!grepl(\u0026quot;([a-z]+|◊ë◊ó◊ô◊®◊ï◊™)\u0026quot;, bigram)) %\u0026gt;% count(word1, word2, sort = T) %\u0026gt;% slice(1:45) %\u0026gt;%\rgraph_from_data_frame()\rp_arrow \u0026lt;- arrow(type = \u0026quot;closed\u0026quot;, length = unit(.1, \u0026quot;inches\u0026quot;))\rggraph(elec_bigram, layout = \u0026quot;fr\u0026quot;)+\rgeom_edge_link(aes(edge_alpha = n), arrow = p_arrow, end_cap = circle(.04, \u0026quot;inches\u0026quot;), show.legend = FALSE)+\rgeom_node_point(color = \u0026quot;lightblue\u0026quot;, size = 3)+\rgeom_node_text(aes(label = name), vjust = 1, hjust = 1, family = \u0026quot;Calibri\u0026quot;)+\rtheme_void()+\rlabs(title = \u0026quot;Twitter text bigram\u0026quot;)+\rtheme(text = element_text(family = \u0026quot;Calibri\u0026quot;),\rplot.title = element_text(hjust = 0.5 , face = \u0026quot;bold\u0026quot;, size = 18))\r\rFigure 2: Word bigram excludes Hebrew stop words and the word ‚Äòelections‚Äô\r\r\nHow should we read this graph?\nFirst off, We only plotted the 45 most common bigrams (out of 100,000+). Every word is connected to another word with an arrow pointing to a given direction. The direction to which the arrow points is the way to read that bigram. In addition, bolder lines represent a higher frequency of that bigram throughout all our text.\nFor example, on the bottom of our graph we see the number ‚Äò2‚Äô connected to the words ‚Äòmandates‚Äô and ‚Äòcampagin‚Äô. The direction of the arrow signals that we should read the bigram as ‚Äò2 mandates‚Äô and ‚Äò2 campagins‚Äô.\nWhat does this all mean?\n\rWe have discussions regarding the number of chairs a govenrment will have (62/61/60/58) connected to mentions of the number of election campaigns (2/3) we had, discussions of a united and/or minimal government and the forming of one in general.\n\rWe see mentions of individuals such as ‚ÄúBenjamin Netanyahu‚Äù, ‚ÄúAmit Segal‚Äù (Both we discussed earlier), ‚ÄúNatan Eshel‚Äù, but no mention of the main candidate running against Netanyahu - ‚ÄúBenny Gantz‚Äù. That‚Äôs actually kind of odd, but more on that in a minute.\n\rWe also see mentions of political parties such as ‚ÄúMeretz‚Äù, ‚ÄúGesher‚Äù and ‚ÄúLabor‚Äù who ran together this time around, ‚ÄúOtzma Yehudit‚Äù, ‚ÄúUnited Torah Judaism‚Äù, and the ‚ÄúJoint List‚Äù. There‚Äôs no mention of the two leading parties - ‚ÄúKahol Lavan‚Äù \u0026amp; ‚ÄúThe Likkud‚Äù., despite the mentioning of the latter‚Äôs leader.\n\rMentions of Netanyahu‚Äôs indicment and the personal law associated him.\n\rMentions I‚Äôd categorize as ‚Äòother‚Äô such as ‚ÄúTerrorist supporters‚Äù, ‚ÄúWill of the people‚Äù, ‚ÄúFake news‚Äù, \"Go vote‚Äô, etc.\r\r\rActaully, this turned out more interesting than I thought. Several questions arose while looking at it: Several words are missing such as the main parties names (Likkud \u0026amp; Kahol-Lavan), The leading oponent running against Benjamin Netanyahu - Benny Gantz - and other questions such as with whom are specific terms associated. Before we close up I‚Äôll look at one question that troubles me - Why doesn‚Äôt Gantz appear in our list üò±?\nBenny Gantz‚Äôs disappearance\rIn order to see why Benny Gantz doesn‚Äôt appear in our bigram plot I‚Äôll do the following: I‚Äôll break the text into bigrams and filter to have only the bigrams containing the word Gantz. Once we have that we can see why he doesn‚Äôt appear in our bigram plot despite appearing in our wordcloud.\nBefore I run the analysis and give you the answer think for a moment - What was the process of coming up with the bigram? If I chose only the 50 most frequent bigrams, why would a word that appears many times in our text not appear in our bigram list? Alternatively, did we filter anything along the way? Maybe even give the previous chunk another glance before I answer it.\nLet‚Äôs have a look:\ngantz \u0026lt;-elections %\u0026gt;%\rselect(text) %\u0026gt;% unnest_tokens(bigram, text, token = \u0026quot;ngrams\u0026quot;, n = 2) %\u0026gt;%\rseparate(bigram, into = c(\u0026quot;word1\u0026quot;, \u0026quot;word2\u0026quot;), sep = \u0026quot; \u0026quot;, remove = FALSE) %\u0026gt;% filter(word1 %in% \u0026quot;◊í◊†◊•\u0026quot; |\rword2 %in% \u0026quot;◊í◊†◊•\u0026quot;,\r!grepl(\u0026quot;([a-z]+|◊ë◊ó◊ô◊®◊ï◊™)\u0026quot;, bigram))\rThe code is similar to what we did earlier only this time we left bigrams that match the word we want - bigrams containing the word Gantz. Now that we have our list of bigrams, let‚Äôs look at the count of bigrams containing the word ◊í◊†◊• (‚ÄòGantz‚Äô):\ngantz %\u0026gt;% count(bigram, sort = T)\r## # A tibble: 978 x 2\r## bigram n\r## \u0026lt;chr\u0026gt; \u0026lt;int\u0026gt;\r## 1 ◊©◊ú ◊í◊†◊• 160\r## 2 ◊ë◊†◊ô ◊í◊†◊• 138\r## 3 ◊í◊†◊• ◊ú◊ê 90\r## 4 ◊¢◊ú ◊í◊†◊• 70\r## 5 ◊ê◊™ ◊í◊†◊• 69\r## 6 ◊¢◊ù ◊í◊†◊• 61\r## 7 ◊ê◊ù ◊í◊†◊• 41\r## 8 ◊í◊†◊• ◊î◊ô◊î 25\r## 9 ◊í◊†◊• ◊ê◊ï 19\r## 10 ◊í◊†◊• ◊ú◊ô◊ë◊®◊û◊ü 19\r## # ... with 968 more rows\rAHA! Now I see what happened. The first bigram is a stop word and the word Gantz (‚ÄòOf Gantz‚Äô). The second bigram should have been included as it is Gantz‚Äôs full name - Benny Gantz, which appears 138 times.\nSo, why has it been filtered? This is a great question which we can answer if we look at our stop words we initially used. Let‚Äôs see if it has the word ◊ë◊†◊ô (‚Äòbenny‚Äô in Hebrew):\nhe_stopwords %\u0026gt;% filter(word == \u0026quot;◊ë◊†◊ô\u0026quot;)\r## # A tibble: 1 x 1\r## word ## \u0026lt;chr\u0026gt;\r## 1 ◊ë◊†◊ô\rYes it does. At the time of writing this blog post it leaves me in a dilemma - Should I change the stop words file I used to a different one or maybe create my own? Or should I continue as is? I think leaving it will teach me (and hopefully whoever read this far) a valuable lesson of always checking your stop words. In a different context the specific bigram wouldn‚Äôt have got me thinking, but here it didn‚Äôt make sense that our leading candidate was filtered, thus my inquire into what happened. In hebrew the word Benny also means ‚Äòmy son‚Äô, which I wouldn‚Äôt describe as a stop word but whoever made the dataset I guess did.\nIf you wish to give it a try yourself, you can find the data in the form of an .rds or smaller .csv (excludes list columns) in my github repository.\nWell then, that‚Äôs all for now folks! And remember, make sure to validate your stop words dataset!\r\r\r\r\rThe function wordcloud2 we wrote wasn‚Äôt actually run because it renders an html object which distorts the post. Instead I used the webshot of our rendered html file, read more about that here.‚Ü©\n\r\r\r","date":1587340800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1587340800,"objectID":"406620afd26c1a573f7886c332ca3564","permalink":"/post/israeli-elections-on-twitter/","publishdate":"2020-04-20T00:00:00Z","relpermalink":"/post/israeli-elections-on-twitter/","section":"post","summary":"Analyzing tweets from the the Israeli elections week","tags":["rtweet","tidytext"],"title":"Israeli elections on Twitter","type":"post"},{"authors":[],"categories":["talk"],"content":"\rR is an amazing tool for preparing and visualizing data. As political activists we want to provide our readers with information that will influence and motivate them to act. The talk outlines how I believe R can empower individuals to work with data in an efficient and aesthetic way.\nYou can find a static example of knitting a document with parameters under the folder ‚Äòrmarkdown_eg‚Äô in the code repository. Rmarkdown‚Äôs ability to render documents is something I found extremly advantageous over other programs such as SPSS, a statistical program commonly taught in the social science. The slides were given to a group of fellows of the program Israel-2050 which I was a member of in 2017. This is not a hands-on learning session but an exposure to some of R‚Äôs abilities I find fascinating and helpful for those wanting to work with data effectively.\n","date":1586869200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1560819601,"objectID":"2489080d41fe6a46f9d0995232819bf0","permalink":"/talk/israel-2050/","publishdate":"2020-04-17T00:00:00Z","relpermalink":"/talk/israel-2050/","section":"talk","summary":" ","tags":["talk"],"title":"Wrangling and Visualizing Data in R","type":"talk"},{"authors":null,"categories":["R"],"content":"\r\rTL;DR Instead of each time searching for an id in the xlsx template the university provides we make our own xlsx and merge between the two. I then run through two options of either saving the new data frame as an .xlsx using the {xlsx} package, and I show another option where I extract the new column I need using write_clip from the {clipr} package.\n\r‚ÄúProgress isn‚Äôt made by early risers. It‚Äôs made by lazy men trying to find easier ways to do something.‚Äù  ‚Äï Robert Heinlein\n\rWhat‚Äôs the story?\rThe other day I had to update students‚Äô exams into a blank excel file. Every course exam each student gets an exam id. Their id is comprised from a number / number, for example, 26/1; 1/1; 42/15 and so forth. In our course of up to 70 students the left number goes all the way to the number of students in the exam class, and the right number goes up to 15 or 20 and starts again from 1.\nThis would make it easy to insert the grade for each id into the excel file that is already organized. However, since this is a new system and I was waiting to get access to download the excel I decided to open a new spreadsheet instead. Also, writing the id instead of looking it up in the excel file each time can save, in my opinion, a little time of searching.\nSo we have our spreadsheet which is not sorted, and we have the university‚Äôs spreadsheet which is sorted - how are we going to sync between them, considering our id column we wrote is recognized as a character class? I know, let‚Äôs turn to R1.\n\rLooking at our data\rLet‚Äôs start off with loading our packages:\nlibrary(tidyverse)\r# For reading xlsx files\rlibrary(readxl)\r# To nicely display the tables in the following paragraph\rlibrary(kableExtra)\rlibrary(knitr)\rNow let‚Äôs read both files: Our spreadsheet with just the id and grade of each student we wrote in, and the other spreadsheet with the students‚Äô id and a numerical vector to sort by that the university provides.\nmessy \u0026lt;- read_excel(\u0026quot;messy_grades.xlsx\u0026quot;)\rclean \u0026lt;- read_excel(\u0026quot;clean.xlsx\u0026quot;)\rThis gives us the following tables where on the left we have our messy table we wrote and on the right our clean table we want to merge to:\n\r\rid\r\rgrade\r\r\r\r\r\r67/13\r\r94\r\r\r\r56/2\r\r90\r\r\r\r68/14\r\r84\r\r\r\r63/9\r\r100\r\r\r\r55/1\r\r89\r\r\r\r62/8\r\r97\r\r\r\r\r\r\rid\r\rparticipated\r\rnumber_for_sorting\r\r\r\r\r\r1/1\r\rV\r\r1\r\r\r\r2/2\r\rV\r\r2\r\r\r\r3/3\r\rV\r\r3\r\r\r\r4/4\r\rNA\r\r4\r\r\r\r5/5\r\rNA\r\r5\r\r\r\r6/6\r\rV\r\r6\r\r\r\r\r\nSo we now have several options:\nJoin between the two tables, save the clean table as a new xlsx and upload it to the University‚Äôs exam system.\rJoin between the two tables, clip the column with the organized grades and paste it into the university‚Äôs sorted excel file.\r\r\rOption 1 - Merge and write to a new excel file\rSo the first option will be to merge the two tables into the clean one and save that as a new excel file using the {xlsx} package:\njoined_tables \u0026lt;- messy %\u0026gt;% right_join(clean)\rxlsx::write.xlsx(joined_tables, \u0026quot;010210078-29012020C.xlsx\u0026quot;, showNA = FALSE)\rBelow is a screen shot of our new table:\nHowever, going with this approach I encountered that the new .xlsx file is saved with a new column of id numbers that we see in the screenshot. We can just delete that column and have our file all ready to go.\n\rOption 2 - Clip the sorted column into the excel file\rThis time around I‚Äôll write a function for what we‚Äôll be doing: I want to join the tables but this time around I want to clip the column I need and then manually paste it in the original template excel file:\nclip_grades \u0026lt;- function(messy, clean){\rmessy %\u0026gt;% right_join(clean) %\u0026gt;% pull(grade) %\u0026gt;% clipr::write_clip()\r}\rclip_grades(messy, clean)\rwhich gives us the following:\nThat‚Äôs it!\nWell, more or less. We need to delete the ‚ÄòNA‚Äô that are copied from the function. Unfortunately I wasn‚Äôt able to delete them from within R, so I manually delete them.\nAs to which option is better, I think the first option is more efficient as we only need to delete the id column. However, using the {xlsx} package is dependent on {rJava}and having java installed on the computer from what I encountered. Option two can be a little messy and possibly yield mistakes if we copy and paste the new grades and then manually delete the NA - your call.\nSo what did I learn here?\r\rHow to read and write an excel file.\rUsing the write_clip function which is amazingly easy.\rHow to make updating exams easier üí™\r\r\n\r\r\rFor confidentiality and other reasons I only left columns with information that can‚Äôt be linked to students (I also changed the grades altogether for this demonstration).‚Ü©\n\r\r\r","date":1581724800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1581724800,"objectID":"5b9673a3312e79a1eaa0f953d5f22fd1","permalink":"/post/update-exam-grades-easy-with-r/","publishdate":"2020-02-15T00:00:00Z","relpermalink":"/post/update-exam-grades-easy-with-r/","section":"post","summary":"In this post I discuss my use of the xlsx and clipr packages for optimizing how I updated students exams","tags":["xlsx","clipr"],"title":"Making updating exam grades easy with R","type":"post"},{"authors":null,"categories":["R"],"content":"\r\rFigure 1: Eliud Kipchoge breaks the two-hour marathon barrier. Photo from the ‚ÄòNew York Times‚Äô. Leonhard Foeger/Reuters \r\rTable of Contents\r\rBackground\rRetrieving data from wikipedia\rWarngling the Data\rPlot\r\rggimage\rPlot Aesthetics\rFinal annotation\r\rAppendix\r\r\rBackground\rOn saturday October 12, 2019, Eliud Kipchoge broke (unofficially) the two-hour marathon barrier üèÜ\nI saw Neil Saunders‚Äô Twitter post visualizing the new record and wanted to try and reproduce it with runners instead of points. In this post I‚Äôll walk through how I obtained the data from a Wikipedia page with {rvest}, wrangled and tidied it and eventually plotted it using {ggimage}.\nWhen I initially created the plot I mistakenly took the Marathon year rankings from the Wikipedia webpage. That page showcases The yearly rankings and not the world records in general. In addition, I also changed the method of obtaining the data from first creating the plot to now. When I first did it I copied and pasted the table from Wikipedia into a .csv file and worked with that. For that specific time point, where my experience with R was extremly novice, I think it was adequate. This time around I gave scraping Wikipedia‚Äôs webpage a try which also renders a reproducible example.\nLet‚Äôs start with loading the packages we‚Äôll need:\nlibrary(tidyverse)\rlibrary(rvest)\rlibrary(janitor)\rlibrary(lubridate)\rlibrary(ggimage)\rlibrary(hrbrthemes)\rWe‚Äôll use {tidyverse} for tidy manipulation and plotting, {janitor} for cleaning the column names, {lubridate} for working with dates, {ggimage} for a plot with images and {hrbrthemes} for a nice quick aesthetic theme.\n\rRetrieving data from wikipedia\rIn order to view the new record in comparison to other world records, We‚Äôll turn to Wikipedia and see what we can find there.\n\rFigure 2: Wikipedia page of marathon world records\r\rHere we can see that the webpage contains information about marathon records, where in the screenshot we see the men section. We only want the table with men‚Äôs records, so let‚Äôs get that:\n#The Wikipage we\u0026#39;ll need\rwiki_url \u0026lt;- \u0026quot;https://en.wikipedia.org/wiki/Marathon_world_record_progression\u0026quot;\rrunners_wiki \u0026lt;- wiki_url %\u0026gt;% read_html() %\u0026gt;% html_nodes(xpath=\u0026#39;//*[@id=\u0026quot;mw-content-text\u0026quot;]/div/table[1]\u0026#39;) %\u0026gt;% html_table(fill = TRUE) %\u0026gt;% as.data.frame()\rUsing the {rvest} package we are able to scrape the Wikipedia page for the table we wanted. Frankly, this is the first time I used rvest, but I found a good example from Kasia Kulma‚Äôs blog post exploring London crime with R heat maps. I used the SelctorGadget which identified the page‚Äôs content as ‚Äúmw-content-text‚Äù. Using that id we looked for the tables (/div/table), specifically the first table [1] of men world records we saw earlier in figure 2. Once we have the table we turn it into a dataframe for us to use.\nAlternatively, you can also use the following method to extract a table by extracting all tables from the Wikipedia page and choosing the first one:\nrunners_wiki_alternative \u0026lt;- wiki_url %\u0026gt;% read_html() %\u0026gt;%\rhtml_table(fill = TRUE) %\u0026gt;%\r.[[1]]\rThis option extracts all table from the html page using html_table(). Using this on the whole page parses the html tables into data frames nesting within a list object. Like before, {rvest} makes it easy for us and if the tables have inconsistent number of values it requires (or demands?) us to fill them. Once we have the tables in a list object we can extract the one we need using .[[1]]. The . acts as a place holder for the previous object passed, here a list of tables we scraped. The [[1]] following that calls for the first object within the list, but in the form of its core class - data.frame. If we‚Äôd use one square bracket [1] it would return an object with the original class from which it was drawn, in this case a list which is not good for us here since we want it as a dataframe to continue our data preparation.\n\rWarngling the Data\rLet‚Äôs look at our table to see what we have and what we‚Äôll need to do:\nhead(runners_wiki, n = 3)\r## Time Name Nationality Date\r## 1 2:55:18.4 Johnny Hayes United States July 24, 1908\r## 2 2:52:45.4 Robert Fowler United States January 1, 1909\r## 3 2:46:52.8 James Clark United States February 12, 1909\r## Event.Place Source\r## 1 London, United Kingdom IAAF[53]\r## 2 Yonkers,[nb 5]United States IAAF[53]\r## 3 New York City, United States IAAF[53]\r## Notes\r## 1 Time was officially recorded as 2:55:18 2/5.[54]Italian Dorando Pietri finished in 2:54:46.4, but was disqualified for receiving assistance from race officials near the finish.[55] Note.[56]\r## 2 Note.[56]\r## 3 Note.[56]\rA little messey but that‚Äôs OK. What we‚Äôll need to visualize Eliud Kipchoge‚Äôs record is the Name, Time and Date of all runners. We‚Äôll start with cleaning our data:\nrunners_clean \u0026lt;- runners_wiki %\u0026gt;% clean_names() %\u0026gt;% select(1,2,4)\rstr(runners_clean)\r## \u0026#39;data.frame\u0026#39;: 50 obs. of 3 variables:\r## $ time: chr \u0026quot;2:55:18.4\u0026quot; \u0026quot;2:52:45.4\u0026quot; \u0026quot;2:46:52.8\u0026quot; \u0026quot;2:46:04.6\u0026quot; ...\r## $ name: chr \u0026quot;Johnny Hayes\u0026quot; \u0026quot;Robert Fowler\u0026quot; \u0026quot;James Clark\u0026quot; \u0026quot;Albert Raines\u0026quot; ...\r## $ date: chr \u0026quot;July 24, 1908\u0026quot; \u0026quot;January 1, 1909\u0026quot; \u0026quot;February 12, 1909\u0026quot; \u0026quot;May 8, 1909\u0026quot; ...\rThe clean_names function cleans the column names making them easier to use. I then picked the columns we‚Äôll need using select. Lastly, we want to look at the variables structure to see if we they need any manipulations. Yes, it seems both the time and date are not recognized appropriately (In this case they‚Äôre characters) - let‚Äôs fix that:\nrunners_mutate \u0026lt;- runners_clean %\u0026gt;% add_row(time = \u0026quot;1:59:40\u0026quot;, name = \u0026quot;Eliud Kipchoge\u0026quot;, date = \u0026quot;November 12, 2019\u0026quot;) %\u0026gt;% mutate(run_period_raw = hms(time),\rrun_duration = as.numeric(run_period_raw, \u0026quot;minutes\u0026quot;),\rrun_year = year(mdy(date))) %\u0026gt;% select(c(-date,-time))\r## Warning: 1 failed to parse.\rLet‚Äôs see what we did here. First I add Eliud Kipchoge‚Äôs new unofficial record as an observation into our dataframe. I then turned to the {lubridate} package where I used the hms function to mutate the time variable we had into a new variable called ‚Äòr_period_raw‚Äô. Although this cleans the variable, hms transforms it into a period object which I found a little difficult to use when we want to plot. What we need is to turn it into a numeric class which we did in our new variable ‚Äòrun_duration‚Äô. This will help us in plotting but I retained the period class variable as it makes it easier to read in this case.\nI then turned the date column into a Month-Day-Year variable using the mdy function, which eventually I only extracted the year using year. Lastly I discarded the old columns we don‚Äôt need anymore. We also recieved a warning sign that one observation didn‚Äôt parse. This was because the value in the cell didn‚Äôt match the pattern of the hms fuction. The original pattern looked like this: May 26, 1909[nb 6]. All we want is the specific year which we‚Äôll probably anyway filter later so it‚Äôs no big deal, but let‚Äôs go ahead and manually add it if we decide to use it later:\nrunners_mutate[5,4] \u0026lt;- 1909\rThis brings us the following:\nggplot(runners_mutate,aes(x = run_year, y = run_duration))+\rgeom_point()\rGreat, that‚Äôs a good start. Now we want to make it a little less crowded so we can easily insert an image of runners instead of points and not have it cluttered. In order to do that we‚Äôll look at each several years and lastly at 2019, the current record. First, let‚Äôs look at the years we have:\nrunners_mutate %\u0026gt;% pull(run_year)\r## [1] 1908 1909 1909 1909 1909 1909 1913 1913 1914 1920 1925 1929 1935 1935 1935\r## [16] 1935 1947 1952 1953 1953 1954 1956 1958 1960 1963 1963 1963 1964 1964 1965\r## [31] 1967 1969 1970 1974 1978 1980 1981 1984 1985 1988 1998 1999 2002 2003 2007\r## [46] 2008 2011 2013 2014 2018 2019\rUsing the pull function we were able to extract the column we wanted, much similar to using the runners_mutate$column_name approach. ‚ÄòUnfortunately‚Äô, we can‚Äôt filter exactly by round intervals (for example every exact 10 years) so we‚Äôll create a vector with specific years to filter by. Although it might sound trivial, make sure you‚Äôre assigning years that are observed in your data set, otherwise it‚Äôll filter only by the years you do have and not those you don‚Äôt.\nyear_sub \u0026lt;- c(1908, 1920, 1929, 1947, seq(1960,1980,10), 1999, 2011, 2019)\rHere we created a vector with values for every 15+- years. Now we can filter our new dataframe according to the years we want:\nrunners_mutate \u0026lt;- runners_mutate %\u0026gt;% filter(run_year %in% year_sub)\rUsing the filter function with %in% we discard anything from the run_year column that‚Äôs not in the year_sub vector. I find %in% facsinating and extremely helpful when you want to look/filter several parameters. Basically, you can read it as ‚ÄúKeep all rows in ‚Äòrun_year‚Äô that match values in ‚Äòyear_sub‚Äô‚Äù.\n\rPlot\rggimage\rIn order for us to plot a runner icon instead of points we need to load the images into our data frame as values for each observation. To do that we use the {ggimage} package which we‚Äôll also use for the plot.\nrunners_mutate \u0026lt;- runners_mutate %\u0026gt;% mutate(run_image = \u0026quot;run.png\u0026quot;)\rAnd now let‚Äôs look at our new plot:\ng \u0026lt;- ggplot(runners_mutate, aes(x = run_year, y = run_duration))+\rgeom_image(aes(image = run_image), size = 0.05)+\rtheme_ipsum_rc()\rg\rNot bad. I like the icons although the whole graph might be a bit misleading if readers perceive that these are the only records there are. However, this is a tutorial and we‚Äôll also add that note into our plot momentarily. You can adjust the size and other parameters of the images we plot, here for example I chose to adjust the size from its default. I also added theme_ipsum_rc from the {hrbrthemes} package for a quick aesthetic theme I like.\n\rPlot Aesthetics\rSo the plot so far looks nice, but we want it to be aesthetic and also to easily understand the progress of records across years. In order to do that, let‚Äôs turn to adjust both the y and x axis, and following that add some information to understand what we‚Äôre looking at:\ng1 \u0026lt;- g +\rscale_x_continuous(name = \u0026quot;Year\u0026quot;,\rlimits = c(1900,2025),\rbreaks = seq(1900,2020,10),\rlabels = c(\u0026quot;1900\u0026quot;, paste0(\u0026quot;\u0026#39;\u0026quot;, seq(10,90,10)),\u0026quot;2000\u0026quot;,\r\u0026quot;\u0026#39;10\u0026quot;,\u0026quot;\u0026#39;19\u0026quot;))\rHere we added some nice x labels in a format that‚Äôs both concise and informative. I remember taking this from Liam Bailey‚Äôs #Tidytuesday plot a while back when i first made this visualiztion. What we did was teak the scale_x_continuous by assigning a name to the axis, expanding its limits, added specific breaks and then a label for each break using paste0. Note that you must have the same number of labels and breaks for the plot to render so it‚Äôs important to have the sequences identical in length; otherwise it‚Äôll return an error. With the paste0 we can add any value or observation and then ‚Äòstick‚Äô to it whatever else we want. Using that we are able to create years in the format of ‚Äô10 and so on. It is also possible to use the {glue} package which I heard is very intuitive, maybe next post I‚Äôll give that a try.\nNext, let‚Äôs change our y duration axis:\ng2 \u0026lt;- g1 + scale_y_time(name = \u0026quot;Time (hours)\u0026quot;,\rlimits = c(100,180),\rbreaks = seq(100,180,10),\rlabels = c(\u0026quot;1:40\u0026quot;,\u0026quot;1:50\u0026quot;, \u0026quot;2:00\u0026quot;, \u0026quot;2:10\u0026quot;,\u0026quot;2:20\u0026quot;,\u0026quot;2:30\u0026quot;,\u0026quot;2:40\u0026quot;, \u0026quot;2:50\u0026quot;, \u0026quot;3:00\u0026quot;))\rIf you recall, we previously mutated the column we read from Wikipedia into a period class and a duration of minutes. using the scale_*_time (either x or y instead of *) we can work with an hms object. What we did is add a name, expand a little the limits, add breaks and labels same as before. This time around we used our breaks as minutes, so every 60 minutes represents an hour. I initially used hours as the numeric value, but then it makes it harder to break every 10 minutes (that‚Äôll mean breaks every 0.166‚Ä¶). For the labels I was having some problems automating it so I comprimised on manually inputting it; I guess sometimes you just have to choose your battles between automating and manualy inserting.\nLet‚Äôs finish up by adding a title, subtitle and integrating last aesthetics to our plot.\ng3 \u0026lt;- g2 +\rlabs(title = \u0026quot;How does Eliud Kipchoge marathon score compare to previous yearly records?\u0026quot;,\rsubtitle = \u0026quot;Points are world records for every 10-15 years. \\nEliud Kipchoge is the first to break the two-hour barrier (unofficially), Great job!\u0026quot;)+\rtheme(\rpanel.grid.minor = element_blank(),\rpanel.grid.major = element_line(colour = \u0026quot;gray75\u0026quot;, size = 0.1, linetype = \u0026quot;dashed\u0026quot;),\rplot.title = element_text(size = 14),\rplot.subtitle = element_text(size = 10)\r)\rAfter adding some labs I tweaked a bit the gridlines using panel.grid minor or major. You can play around with them to see which minimilize your plot in the perfect way. I chose to leave the major grid lines since I find it easier to read the values with them. Although we defined a theme earlier on we can still tweak it by adding another theme argument to the previous one as we just did.\n\rFinal annotation\rLastly, we want the new record to be evident and stand out in a first glance. Here I was somewhat debating between using a regular geom_point instead of the geom_image because then we could easily use a vertical line to highlight the 2:00 hour threshold. Since a line in this case will cut right through the icon, let‚Äôs use an arrow annotation instead.\ng4 \u0026lt;- g3 +\rgeom_curve(aes(x = 2018, y = 120, xend = 2015, yend = 113),\rcolour = \u0026quot;black\u0026quot;, size = 0.9, curvature = 0.5,\rarrow = arrow(length = unit(2,\u0026quot;mm\u0026quot;), type = \u0026quot;closed\u0026quot;))+\rannotate(\u0026quot;text\u0026quot;, x=2010, y= 105, label = \u0026quot;Eliud Kipchoge\\n12.10.2019\\n1:59:40\u0026quot;,\rcolor = \u0026quot;black\u0026quot;, size = 3, hjust = 0)\rg4\rAnd voila!\nIn our final touches we added both an arrow and text to explain what we‚Äôre seeing. I decided to go with a geom_curve arrow where we can set the start and end of the arrow along with the kind of curve we want. We then set the curve to be arrow and adjust its length. You can also use a closed head arrow, for more on that read on ?arrow as part of the geom_curve or geom_segment you can use here.\nThat‚Äôs it, seems like were good to go. Great job for Elihud Kipchoge üëè\n\n\r\rAppendix\r\rWhen I initially created this visulization I was just starting with R. I first created 11 slots, added 1921, the sequence of 1930-2010 and then a 2019 (reminder: When I first created this viz I took a different dataset altogether). Little did I know how to properly use the c() function that we used in the current post.\r\ryear.sub \u0026lt;- vector (\u0026quot;double\u0026quot;, 11)\ryear.sub[1] \u0026lt;- 1921\ryear.sub[2:10] \u0026lt;- seq(1930,2010,10)\ryear.sub[11] \u0026lt;- 2019\r\rBack to top\r\r\r","date":1581033600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1581033600,"objectID":"61554887203386d7c8bd4596bbb5b349","permalink":"/post/eliud-kichoge/","publishdate":"2020-02-07T00:00:00Z","relpermalink":"/post/eliud-kichoge/","section":"post","summary":"In this post I plot Eliud Kichoge new marathon record using rvest to get data from wikipedia and ggimage for plotting images as points","tags":["ggimage","rvest"],"title":"Visualizing Eliud Kichoge's new marathon record","type":"post"},{"authors":null,"categories":["R"],"content":"\r\r\r\r\r\r\rUpdate from March 21, 2020\rI‚Äôve been wanting to return to this post and make this map more interactive. As a matter of fact it was easier than I thought, I just never got around to doing it. I won‚Äôt be going through the code for the leaflet map below but will leave it for whoever would like to review it:\nlibrary(leaflet)\rlibrary(magrittr)\rreadr::read_csv(\u0026quot;shelters.csv\u0026quot;) %\u0026gt;% leaflet() %\u0026gt;% addTiles() %\u0026gt;% setView(34.7913, 31.25181,zoom = 13) %\u0026gt;% addCircles(radius = 4, color = \u0026quot;red\u0026quot;, fill = TRUE)\r\r{\"x\":{\"options\":{\"crs\":{\"crsClass\":\"L.CRS.EPSG3857\",\"code\":null,\"proj4def\":null,\"projectedBounds\":null,\"options\":{}}},\"calls\":[{\"method\":\"addTiles\",\"args\":[\"//{s}.tile.openstreetmap.org/{z}/{x}/{y}.png\",null,null,{\"minZoom\":0,\"maxZoom\":18,\"tileSize\":256,\"subdomains\":\"abc\",\"errorTileUrl\":\"\",\"tms\":false,\"noWrap\":false,\"zoomOffset\":0,\"zoomReverse\":false,\"opacity\":1,\"zIndex\":1,\"detectRetina\":false,\"attribution\":\"\u0026copy; OpenStreetMap contributors, CC-BY-SA\"}]},{\"method\":\"addCircles\",\"args\":[[31.259018768,31.2597950910001,31.2592493760001,31.2573049280001,31.2574797520001,31.258424086,31.254246294,31.255173057,31.2556456460001,31.253666206,31.2537330280001,31.2569812850001,31.2570752350001,31.257608025,31.2575462630001,31.2700855640001,31.2702905,31.2695012190001,31.2658482960001,31.259725431,31.2646111440001,31.266921333,31.2604320470001,31.2596058720001,31.262762868,31.263789048,31.261942934,31.2638102080001,31.2632644710001,31.263504445,31.262432614,31.2708580180001,31.2711996570001,31.270725645,31.2712747300001,31.2718630300001,31.2720855330001,31.2726126790001,31.272427384,31.2704051920001,31.266736774,31.2628119170001,31.2455927190001,31.248994151,31.250188747,31.2459192310001,31.24676858,31.2479923820001,31.2478412890001,31.249363784,31.250175772,31.248725547,31.248610904,31.251846362,31.2518736100001,31.250946067,31.253505949,31.257884104,31.2540270010001,31.2559118880001,31.2710156910001,31.271205497,31.270356722,31.269982842,31.2698227120001,31.269304209,31.269990116,31.2687801060001,31.2682527530001,31.2683451450001,31.270695158,31.27323147,31.2733343200001,31.2737922810001,31.2728741080001,31.274249592,31.2746572610001,31.272394175,31.2739432400001,31.2744760080001,31.2717045570001,31.2729638220001,31.2757156430001,31.2741278410001,31.2746324350001,31.274596902,31.271494137,31.2718457610001,31.275141078,31.275589424,31.2759297440001,31.2649303200001,31.270103943,31.2659409890001,31.2669887380001,31.2697833300001,31.2704315440001,31.266575474,31.266131746,31.221500807,31.224164194,31.223587228,31.2258336940001,31.222024801,31.2197570490001,31.221235918,31.224912993,31.224000586,31.2261810250001,31.223021734,31.2238066,31.2209769780001,31.222977471,31.2321786260001,31.2324369340001,31.2431229220001,31.235916617,31.238806793,31.237065128,31.248259068,31.251491909,31.2504392380001,31.2504933070001,31.2518083160001,31.246153377,31.250630739,31.25563615,31.256696517,31.254957205,31.257290099,31.25460144,31.256567933,31.255031301,31.2575173870001,31.2579245330001,31.2579056490001,31.2226565670001,31.224564345,31.2426843960001,31.240248009,31.2399610090001,31.267945217,31.2638298580001,31.270236595,31.269875347,31.265144816,31.2690648330001,31.2681699770001,31.269377084,31.265713315,31.270953035,31.2722040720001,31.2645033750001,31.271228369,31.2715811,31.256697924,31.257101161,31.269666226,31.275281239,31.274241079,31.269109099,31.272405753,31.2598086400001,31.250707328,31.251678113,31.2525321090001,31.2486613240001,31.2554510690001,31.2490194850001,31.2670087990001,31.2682888350001,31.269309503,31.250801417,31.268097267,31.2679672850001,31.267170004,31.261017135,31.2604871560001,31.261171405,31.260286697,31.260856463,31.258957008,31.2669088850001,31.264312395,31.2644250700001,31.262675503,31.262628384,31.266354108,31.2378694280001,31.237196182,31.2522976180001,31.2551276230001,31.2555366730001,31.271009238,31.2697859260001,31.248898803,31.250194783,31.250113594,31.248703956,31.2482595930001,31.247643195,31.252913957,31.2562439010001,31.250285107,31.2561600820001,31.2510416860001,31.249713179,31.258150351,31.2590159170001,31.2581197600001,31.25558649,31.2571552970001,31.254918092,31.2545320300001,31.2465726200001,31.2520725430001,31.261599754,31.2721663470001,31.270971611,31.2616581070001,31.264973377,31.2658491130001,31.2688886780001,31.2683040970001,31.2707125310001,31.271428288,31.254769068,31.257070663,31.2465921090001,31.2692059390001,31.2443915640001,31.2314304170001,31.269208157,31.269363072,31.270308577,31.2595743270001,31.2700607460001,31.2564875580001,31.2479156200001,31.2481547410001,31.254812955,31.266566502,31.2504635080001,31.2541823770001,31.2683791260001,31.271307544,31.2482951640001,31.2737507580001,31.272590789,31.2570404000001,31.252699315,31.2517972700001,31.2663416,31.2709878490001,31.2660293330001,31.25408293,31.2701366480001,31.2402113960001,31.2646567780001,31.2463304390001,31.2347788370001,31.252428283],[34.808214546,34.8078915740001,34.809368438,34.8093634950001,34.810975436,34.810334568,34.805622382,34.8026935250001,34.804473768,34.8095251010001,34.8075520780001,34.808555186,34.7651206140001,34.76364862,34.7628454490001,34.778674504,34.7777073990001,34.7762277100001,34.770138844,34.7869285290001,34.795184944,34.7997725760001,34.7940810010001,34.7916208840001,34.7906790590001,34.79068958,34.7927366010001,34.79231459,34.7927492380001,34.792743929,34.7905235110001,34.7883433920001,34.7867639080001,34.785602972,34.7852056130001,34.787070295,34.787588817,34.7939116200001,34.7924905290001,34.7941080740001,34.8014500780001,34.792161772,34.7956425140001,34.7970971840001,34.7970376860001,34.7936621440001,34.7914949660001,34.784890746,34.787583614,34.7873553950001,34.7857147330001,34.779422326,34.7796193240001,34.7841264850001,34.7828629210001,34.7779282610001,34.782604796,34.7832538050001,34.7900850610001,34.7963698560001,34.806238138,34.8089210210001,34.809585251,34.8086036290001,34.809357278,34.8091260510001,34.8067740120001,34.8089092710001,34.8086805990001,34.8079129270001,34.804323229,34.800439962,34.8088667390001,34.8085067230001,34.809244467,34.808135475,34.8077908650001,34.8096215740001,34.8069308010001,34.806502575,34.8037255140001,34.802776814,34.802732577,34.8024465140001,34.8025668970001,34.804125362,34.802364975,34.8055181280001,34.8074389120001,34.8070149960001,34.8064910510001,34.7604294980001,34.7639117040001,34.765621094,34.7664207510001,34.7608955090001,34.7630732060001,34.7598450790001,34.7611068100001,34.775540274,34.7789006310001,34.7774103090001,34.778600274,34.7726666250001,34.773724196,34.7718155210001,34.775669044,34.780235401,34.780269586,34.7763314490001,34.775308716,34.7743583120001,34.7736159990001,34.7923748970001,34.78033953,34.779738781,34.784380764,34.7847258240001,34.7817552400001,34.792498504,34.7868126540001,34.789983894,34.789128825,34.787953332,34.7942136410001,34.7945532960001,34.7903546790001,34.7959888820001,34.7941725480001,34.795403445,34.7891443610001,34.793907173,34.7965042000001,34.7892947980001,34.781547866,34.783656121,34.7751580730001,34.780917947,34.781822312,34.781617833,34.783344335,34.7623511370001,34.762642929,34.771785949,34.7709577200001,34.7636153930001,34.7619093160001,34.7644633370001,34.764872847,34.7577812160001,34.770979358,34.765673103,34.7643799360001,34.767547478,34.769529817,34.773305672,34.773415368,34.805293723,34.805267585,34.800891017,34.808306987,34.806721075,34.8088518340001,34.8019847370001,34.8065743970001,34.810013871,34.8081264090001,34.7780347280001,34.782620235,34.7707858410001,34.7723190740001,34.774894422,34.77918176,34.7948698500001,34.7970907800001,34.795007293,34.79702511,34.7965872190001,34.795338615,34.7935443280001,34.7929193080001,34.7929353970001,34.7939598030001,34.795247798,34.797073217,34.7944644400001,34.7974265890001,34.792997642,34.785492021,34.7889980750001,34.804945893,34.8100962180001,34.808716111,34.783396279,34.7850399330001,34.795998975,34.805139436,34.8028419560001,34.8061534390001,34.804344707,34.807264053,34.81146289,34.806185188,34.806567597,34.81037658,34.80369548,34.8085987040001,34.811233825,34.809843013,34.8094435680001,34.8080172910001,34.810334235,34.8070907060001,34.8035195030001,34.8086055260001,34.802861137,34.7937478430001,34.7956186760001,34.793586141,34.7919188430001,34.79992937,34.801187988,34.8012474810001,34.792345267,34.7969623870001,34.794766757,34.8086733960001,34.80687017,34.776587685,34.79294107,34.791070099,34.7935582580001,34.7954634880001,34.7948531970001,34.7868227160001,34.7857320460001,34.795653003,34.791395838,34.7796560830001,34.781008178,34.77898084,34.794873022,34.780569927,34.8023777060001,34.7968104340001,34.7835412000001,34.7892121180001,34.807038339,34.8080192800001,34.8085495820001,34.779316936,34.8025542940001,34.7698468990001,34.8072728060001,34.8008629990001,34.8116456560001,34.7842904080001,34.7950379800001,34.7961240610001,34.8083349930001,34.7825503890001,34.808149154],4,null,null,{\"interactive\":true,\"className\":\"\",\"stroke\":true,\"color\":\"red\",\"weight\":5,\"opacity\":0.5,\"fill\":true,\"fillColor\":\"red\",\"fillOpacity\":0.2},null,null,null,{\"interactive\":false,\"permanent\":false,\"direction\":\"auto\",\"opacity\":1,\"offset\":[0,0],\"textsize\":\"10px\",\"textOnly\":false,\"className\":\"\",\"sticky\":true},null,null]}],\"setView\":[[31.25181,34.7913],13,[]],\"limits\":{\"lat\":[31.2197570490001,31.2759297440001],\"lng\":[34.7577812160001,34.8116456560001]}},\"evals\":[],\"jsHooks\":[]}\r\rOriginal post\rI‚Äôve been wanting to learn how to use maps in R for a while before creating the map in this post. Seeing dataframes with longitude and latitude coordinates on various occasions on #Tidytuesday encouraged me to do so.\nA day before this visualizaiton, I discovered our municupality‚Äôs open access data website. In this website you can find various datasets like street light coordinates, bomb shelters spread out in the city and more. A day after discovering it Israel, the country I live in, was fired missiles at. I decided to take the opportunity and map some of the shelters around my house. You know, just in case.\nLet‚Äôs begin with the packages (üì¶) we‚Äôll need:\n#for data manipulation\rlibrary(tidyverse)\r#for a nice map\rlibrary(ggmap)\r#for reading and working with .geojson file\rlibrary(geojsonio)\rlibrary(sp)\r#for integrating a nice font\rlibrary(extrafont)\rLoading and tidying the data\rI initially tried using the .csv file they have on their webiste but I was having too much trouble with the Hebrew so I decided to try and work with the geojsonio package. I had no idea how to work with a .geojson file or frankly how to work with maps in general. To my save, i found this incredible blog by John Johnson to help me transform a ‚Äògeomjson‚Äô file to a dataframe you can work with.\nLet‚Äôs begin:\n#read the .geojson file\rmy_geojson \u0026lt;- \u0026quot;shelters.geojson\u0026quot;\r#convert the .geojson file to an sp object\rdata_json \u0026lt;- geojson_read(my_geojson, what = \u0026quot;sp\u0026quot;)\r#now we can convert it to a nice data frame\rshelters \u0026lt;- as.data.frame(data_json)\r#last tidying of the column names\rnames(shelters)[6:7] \u0026lt;- c(\u0026quot;long\u0026quot;, \u0026quot;lat\u0026quot;)\rWhat we did was load the geojson file, read it as an ‚Äòsp‚Äô object and then turn it into to a dataframe. I changed the names so that it‚Äôll be easier to read the columns. we could also use dplyr::rename but I liked the base R function Johnson used in his blog so I‚Äôll stick with that.\nlet‚Äôs look at the top 3 observations of our new data frame:\n## long lat name X.U.05E7..U.05D5..U.05D3._.U.05E1.\r## 1 34.80821 31.25902 \u0026lt;U+05D2\u0026gt;/2 17\r## 2 34.80789 31.25980 \u0026lt;U+05D2\u0026gt;/1 17\r## 3 34.80937 31.25925 \u0026lt;U+05D2\u0026gt;/25 17\r## elc group_ F_.U.05E6..U.05D5..U.05D5..U.05EA.\r## 1 \u0026lt;U+05D9\u0026gt;\u0026lt;U+05E9\u0026gt; 0 ## 2 \u0026lt;U+05D9\u0026gt;\u0026lt;U+05E9\u0026gt; 0 ## 3 \u0026lt;U+05D9\u0026gt;\u0026lt;U+05E9\u0026gt; 0\rUgh, well the names weren‚Äôt read well into R. While this isn‚Äôt a big issue to resolve, I don‚Äôt find it necessary for the final piece. the names of the shelters are anyway in Hebrew and only represnt a letter and some sort of number (for e.g, A/23, only in Hebrew). Therefore we‚Äôll leave it as is since what I‚Äôm interested in is the longitude and latitude coordinations and for that we don‚Äôt need the character column.\n\rRetrieving the map\rSo we have our data frame with long and lat points, let‚Äôs get our map. I want a map that can be readable in terms of streets and roads, therefore I‚Äôll give the ggmap package a try1. Google requires you to register in order to recieve an API key to pull maps to plot. Unfortunately I won‚Äôt cover how to regiser in this blog post but I‚Äôm sure you can find plenty of tutorials addressing it online.\nLet‚Äôs get Be‚Äôer-Sheva‚Äôs map:\nb7_map \u0026lt;- get_map(location = c(34.7913 , 31.25181), zoom = 13, scale = 2, maptype = \u0026quot;roadmap\u0026quot;)\rWhat we did here was use the get_map function to pull the map according to the long and lat coordinates I gave it of Be‚Äôer-Sheva. You should first pass the longitude and then the latitude in the location argument. In addition you can change other features such as the zoom level, the maptype and more as we saw here (See ?get_map for more info).\n\rPlot\rNow that we have our data set ready and the map as an object we can go on to plot it. ggmap extends ggplot features so we can run the data frame smoothly into the ggmap function:\nggmap(b7_map)+\rgeom_point(shelters, mapping = aes(long,lat),\rcolor = \u0026quot;red\u0026quot;, size = 0.3, shape = 15)\rWhat we did was pass the b7_map as an object into the ggmap function and add a geom, in this case geom_point representing our shelter coordinates. However, this map doesn‚Äôt really help me in a time of need since it doesn‚Äôt show my address clearly.\nLet‚Äôs try zooming in so that we can see what we‚Äôre looking at:\n#retreiving a new map with a greater `zoom`\rb7_map_zoom \u0026lt;- get_map(location = c(34.7913 , 31.25181), zoom = 16, scale = 2, maptype = \u0026quot;roadmap\u0026quot;)\rp \u0026lt;- ggmap(b7_map_zoom)+\rgeom_point(shelters, mapping = aes(long,lat), color = \u0026quot;red\u0026quot;,\rsize = 3, shape = 15)\rp\rMuch nicer and clearer. Using the zoom option in get_map enables to center more on where I want. Great, this shows me some bomb shetlers I have around me in a time of need. Let‚Äôs add some fine tuning for our theme:\np+ #for the title, caption and removing the X and Y axis\rlabs (title = \u0026quot;Neighborhood B, Beer-Sheva, Israel, bomb shelters\u0026quot;,\rx = NULL, y = NULL,\rcaption = \u0026quot;data: www.beer-sheva.muni.il | @Amit_Levinson\u0026quot;)+\rtheme_minimal()+\rtheme(text = element_text(family = \u0026quot;Microsoft Tai Le\u0026quot;),\r#Changing the position of the title\rplot.title = element_text(hjust = 0.5, size = 20, face = \u0026quot;bold\u0026quot;),\raxis.text = element_blank(),\rplot.caption = element_text(size = 9, face = \u0026quot;italic\u0026quot;, hjust = 0),\rpanel.border = element_rect(color = \u0026quot;black\u0026quot;, size=2, fill = NA)\r)\rPerfect, I can now save the plot and distribute it if someone needs it.\nggsave(\u0026quot;shelters_b_eng.png\u0026quot;, width = 8, height = 8)\r\r1. I\u0026#39;ve been wanting to learn to plot maps in #rstats\n2. Yesterday I encountered open data our municipality publishes\n3. Today missiles are fired towards Israel in response to assassination of a top terrorist.\n1+2+3: Plotting bomb shelter locations near where I live#ggmap pic.twitter.com/4Irz0ZKuZr\n\u0026mdash; Amit Levinson (@Amit_Levinson) November 12, 2019  \r\r\r\rFor a first map I decided to go with a static one, but an interactive one can defenitely be a 2.0 version of this blog (as you saw with the March 21st update. Hopefully we won‚Äôt need it.‚Ü©\n\r\r\r","date":1578960000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1578960000,"objectID":"ec1b7e0fefbca8377eccf21297d09eec","permalink":"/post/bomb-shelters/","publishdate":"2020-01-14T00:00:00Z","relpermalink":"/post/bomb-shelters/","section":"post","summary":"While rockets were fired towards Israel I decided to take the opportunity and plot  bomb shelters in Beer-Sheva.","tags":["ggmap","rvest"],"title":"Mapping bomb shelters in Be'er-Sheva, IL","type":"post"},{"authors":null,"categories":null,"content":"\u0026hellip;\n","date":1530140400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1530140400,"objectID":"18d05a63a1c8d7ed973cc51838494e41","permalink":"/privacy/","publishdate":"2018-06-28T00:00:00+01:00","relpermalink":"/privacy/","section":"","summary":"\u0026hellip;","tags":null,"title":"","type":"page"},{"authors":null,"categories":null,"content":"\u0026hellip;\n","date":1530140400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1530140400,"objectID":"9b10c1f64082d3869fd4cb1f85809430","permalink":"/terms/","publishdate":"2018-06-28T00:00:00+01:00","relpermalink":"/terms/","section":"","summary":"\u0026hellip;","tags":null,"title":"","type":"page"},{"authors":null,"categories":null,"content":"","date":1461715200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461715200,"objectID":"d1311ddf745551c9e117aa4bb7e28516","permalink":"/project/external-project/","publishdate":"2016-04-27T00:00:00Z","relpermalink":"/project/external-project/","section":"project","summary":"An example of linking directly to an external project website using `external_link`.","tags":["Demo"],"title":"External Project","type":"project"},{"authors":null,"categories":null,"content":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\n","date":1461715200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461715200,"objectID":"8f66d660a9a2edc2d08e68cc30f701f7","permalink":"/project/internal-project/","publishdate":"2016-04-27T00:00:00Z","relpermalink":"/project/internal-project/","section":"project","summary":"An example of using the in-built project page.","tags":["Deep Learning"],"title":"Internal Project","type":"project"},{"authors":[],"categories":[],"content":"\r\rfunction resizeIframe(obj) {\robj.style.height = obj.contentWindow.document.body.scrollHeight + 'px';\r}\r\rWhat is TidyTuesday?\r\r‚ÄúA weekly data project aimed at the R ecosystem. As this project was borne out of the R4DS Online Learning Community and the R for Data Science textbook, an emphasis was placed on understanding how to summarize and arrange data to make meaningful charts with ggplot2, tidyr, dplyr, and other tools in the tidyverse ecosystem. However, any code-based methodology is welcome - just please remember to share the code used to generate the results.‚Äù\n\rYou can read more about it here.\nBasically, every week the R4DS community publishes a new data set where the R community (or any code-based methdogology) is welcome to analyze and visualize the data.\rWhen I started learning R I saw it as a great opportunity to practice plotting and analzying various datasets; it‚Äôs what get me motivated and fascinated with R. Every week you‚Äôre challenged with a new data set and get to see how others have analyzed it.\nWhenever I have some free time I like to join in and give it a go. If you‚Äôre looking to get some practice or enjoy analyzing the same data set as others - Join the party, there‚Äôs much to learn!\n\n\rPlots\rBelow are graphs I made, organized in a descending order by date. Some I‚Äôm more proud of than others, but I choose to display all of them in case someone finds anything useful üòÑ\r\n02.06.2020\rLink to code\n\r14.05.2020\rLink to code\r\r06.05.2020\rLink to code\n\r29.04.2020\rLink to code\n\r22.04.2020\rLink to code\n\r\r16.04.2020\rLink to code\n\r17.03.2020\rLink to code\n\r06.03.2020\rLink to code | Try the app\n\r20.01.2020\rLink to code\n\r22.12.2019\rLink to code\n\r11.12.2019\rLink to code\n\r5.12.2019\rLink to code\n\r20.11.2019\rLink to code\nNew zealand birds\n\r\r6.11.2019\rLink to code\r\r31.10.2019\rLink to code\n\r22.10.2019\rLink to code\n\n\r9.10.2019\rLink to code\r\n\r6.10.2019\rLink to code\r\n\r28.9.2019\rLink to code\n\n\r22.9.2019\rLink to code\nBack to top\n\r\r","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"22679f7a35f1c0d6138a196711b067c2","permalink":"/post/the-tidytuesday-project/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/post/the-tidytuesday-project/","section":"post","summary":"Visualizations from the Tidytuesday project - A weekly data project intended to practice summarizing, arranging and visualizing data.","tags":[],"title":"The Tidytuesday project","type":"post"}]