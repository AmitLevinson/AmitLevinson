---
title: Israeli elections on twitter
author: Amit Levinson
date: '2020-03-09'
slug: israeli-elections-on-twitter
categories: []
tags: []
subtitle: ''
summary: ''
authors: []
lastmod: '2020-03-09T19:29:42+02:00'
featured: no
draft: true
image:
  caption: ''
  focal_point: ''
  preview_only: no
projects: []
editor_options: 
  chunk_output_type: console
codefolding_show: show
---

```{r setup, include=FALSE, message=FALSE}
knitr::opts_chunk$set(echo = TRUE, message=FALSE, warning = FALSE)
```

Israel had its 3rd consecutive election on March 2, 2020.
This is because our Knesset (Hebrew term for house of representatives) wasn't able to form or hold a government after each of the previous elections. As I won't get into the politics of why they didn't succeed thus far (get it? politics :wink:), I do want to take the opportunity and analyze some tweets posted in the time before and after the elections.

### Gathering the data <i class="fab fa-twitter"></i>

Twitter's API only allows scraping for 6-9 days back for free. Therefore, I scraped the data already on March 7, 2020 and saved it for later use. To gather the tweets we can use the `{rtweet}` package which is amazingly easy to use ([Check out its website](https://rtweet.info/)).  

Let's start with the packages we'll use:

```{r}
library(rtweet)
library(tidyverse)
library(tidytext)
library(hrbrthemes)
library(igraph)
library(ggraph)
library(scales)
```

I could use a consistent theme throughout the post but I'll probably be editing each one a bit. With that said, There are some tweaks that will be consistent acorss several of the plots. Therefore, let's create a theme function as a supplement to all other theme arguments I'll use:

```{r}
adjust_axis <- function() {
  theme_classic() +
  theme(axis.ticks = element_blank(),
        axis.line = element_blank())}
```


```{r include = FALSE}
Sys.setlocale("LC_ALL", "English")
```

Next we'll gather the tweets we need:

```{r echo = TRUE, eval = FALSE}
elections_raw <- search_tweets("בחירות", n = 250000, retryonratelimit = TRUE)
```

```{r echo = FALSE}
elections <- readRDS("election_tweets.rds")
```

As I mentioned earlier I already scraped the data but wrote the command here in case you're wondering how to gather it. I used only one term, which in Hebrew is "elections" and rtweet gathered all tweets containing that word.

Before we begin, I will say this post doesn't aim to be representative of the discussions that were held during the election period. As a matter of fact, nor does it aim to be representative of the twitter discussion. this is due to two main reasons:    
1. Twitter isn't common in Israel at all. I'm not sure what's the usage rate but it's definitely not representative of the Israeli population.  
2. I searched for only one word - elections (in Hebrew) - which yielded some `r nrow(elections)` tweets. This is definitely not a large enough dataset to claim for representation.

With that said, the data gathered provides an opportunity to look at some Twitter data from the elections period, so why not give it a go.

### Tweet frequency

First, let's see how the tweets distribute across the time span we searched for. we can create a quick time plot using the `ts_plot()` from the `{rtweet}` package:  

```{r}
elections %>% 
  ts_plot("2 hours")+
  geom_line(size = 1, color = "#1DA1F2")+
  theme_ipsum_rc(plot_title_face = NULL, grid = FALSE)+
  scale_x_datetime(date_breaks = "1 day",date_labels = "%d %b")+
  labs(x= NULL, y = NULL,
       title = "Frequency of tweets throughout the Israeli elections week",
       subtitle = "Tweets aggregated by two-hour interval. only tweets with\nthe word 'elections' (in Hebrew) were gathered")+
  geom_text(aes(x = as.POSIXct("2020-03-02 23:00:00"), y = 435, label = "10 PM:\nClosing of\npolls"),
            hjust = 0, size = 2.5)+
  geom_vline(xintercept = as.POSIXct("2020-03-02 22:00"),linetype = "dashed", size = 0.5, color = "#1DA1F2")+
  theme(plot.title = element_text(size = 16),
        plot.subtitle = element_text(color = "gray65"))+
  adjust_axis()
```

Interesting, we see the number of tweets during the closing time is equivalent to that of March 4th early in the morning. Kind of an interesting anomaly which I can't put my finger on, any suggestions?

### Users with most tweets

Next, let's look at who tweeted the most:
```{r, fig.width = 8}
elections %>% 
  count(screen_name, sort = T) %>% 
  slice(1:15) %>% 
  mutate(screen_name = reorder(screen_name,n)) %>% 
  ggplot(aes(x= screen_name, y= n))+
  geom_col(fill = "#1DA1F2")+
  coord_flip()+
  scale_y_continuous(breaks = seq(0,180, 30), labels = seq(0,180,30))+
  labs(x = "Screen name", y = "Number of tweets", title = "Top 15 users tweeting the word 'elections' during the 3rd Israeli elections")+
  theme(text = element_text(family = "Calibri"),
        axis.text = element_text(size = 12),
        plot.title = element_text(family = "Roboto Condensed"))+
  adjust_axis()
```

We see that many news companies tweeted a lot using the word elections: 'newisrael13', 'kann_news', 'MaarivOnline', 'RotterNews', 'bahazit_news', 'RotterNet'. I personnaly don't recognize the rest, but on the other hand I use Twitter mostly to follow `R` and academic related tweets, not necessarily Israeli politics.

### Common Hashtags

When using the `{rtweet}` package to gather twitter data, one of the variables collected is the hashtags used in tweets. Although it requires a few lines of code to get them out of the text, I think this is an amazing feature that shows the details [Michael W. Kearney](https://mikewk.com/) put into the package.

According to [Wikipedia](https://en.wikipedia.org/wiki/Hashtag), a 'Hashtag' "is a type of metadata tag used on social networks such as Twitter and other microblogging services." that basically tags the message with a specific theme. This helps to see trends and themes in a macro level.

OK then, let's see what we have:

```{r}
hashtags <- elections %>% 
  select(hashtags) %>% 
  unlist() %>% 
  as.tibble() %>% 
  count(value, name = "Count", sort = T) %>%
  mutate(value = reorder(value, Count),
         iscorona = ifelse(value == "קורונה", "y", "n")) %>% 
  filter(!is.na(value)) %>% 
  slice(1:20)

ggplot(data = hashtags, aes(x = Count, y = value))+
  geom_col(aes(fill = iscorona), show.legend = FALSE)+
  labs(y = NULL, x = "Number of Tweets", title = "Top 20 Hashtags associated with tweets addressing the Israeli elections")+
  scale_fill_manual(values = c(y = "#1DA1F2", n = "gray75"))+
  theme(text = element_text(family = "Calibri"),
        axis.text = element_text(size = 12),
        plot.title = element_text(family = "Roboto Condensed"))+
  adjust_axis()

```

The tweets include pretty much the basics with the two leading ones being 'elections' and 'elections2020'. I highlighted in blue an interesting hashtag at the time - <font color="#1DA1F2"> Corona </font>. The elections were held on March 2, 2020, a little bit after the first cases reached Israel. Little did we know how it will affect us (I'm writing this post on April 07,2020 and we're still in quarantine heading to lock down.)


### Most liked and retweeted

Let's have a look at which tweet was **most liked**. Twitter doesn't define it as 'likes' but as 'favorite', or at least in the data that is collected through the {rtweet} package. Since I will want to do this again to get the tweet that was retweeted the most I'll create a function that will minimize re-writing the code.   
<br>
The function takes in a variable, reorders our dataset according to the variable we declared, extracts the first row and then pulls (also extracts) the status id of that tweet. Lastly, the `blogdown::shortcode` enables to embed tweets, youtube, etc., so we insert our status id in it. For those just getting into functions notice that within the `arrange` argument we insert our variable in two curly brackets {{}}. This is a powerful feature of `{rlang}` when you want to manipulate a variable in a dataframe within a function. Read more about it [here](https://www.tidyverse.org/blog/2019/06/rlang-0-4-0/)
```{r}
get_most <- function(var){
elections %>% 
  arrange(desc({{var}})) %>% 
    .[1,] %>% 
    pull(status_id) %>% 
  blogdown::shortcode('tweet',.)
}
```

<center> `r get_most(favorite_count)` </center>

So the tweet is by 'Amit Segal' - an Israeli news reporter - and it says:   
> *"More than anything, I'm glad there won't be anymore elections for my family that suffered in honors a year and a quarter, Reut, Ivri and Inbar :heart_eyes:"*

Ha, interestingly he wrote it before the end of the elections. However he's right as we see today that a government was indeed formed so we won't have any elections soon (?).

Now let's look at the **most re-tweeted** tweet:


<center> `r get_most(retweet_count)` </center>

The tweet is by Benjamin Netanyahu, at the time the prime minister of Israel, who writes:  
> "If the recording of Gantz's advisor is orcherstrated and fabricated (according to Gantz's words just now), so why did Gantz fire him?
Gantz's advisor was fired because he said the truth everyone knows: Gantz can't be a prime minister. We can. 2 more mandates to the Likkud and we are taking the country out of the plonter, preventing another election and form a government

This came after the exposure of a secret recording of Gantz in a closed meeting, A week or so before election day.

## Wordcloud and bigrams

We looked beforehand at some commonly used hashtags, let's have a look at two more things:

1. A word-cloud  
2. Distribution of words before and after the elections

We could try out more algorthims but I'll save them for a different post.

### Wordcloud

In order to tackle the wordcloud, I'll break up all the tweets into words, filter any Hebrew stop-word and all English words:

```{r}
he_stopwords <- read_tsv("https://raw.githubusercontent.com/gidim/HebrewStopWords/master/heb_stopwords.txt", col_names = "word")

election_token <- elections %>% 
  unnest_tokens(word, text) %>% 
  select(word) %>%
  anti_join(he_stopwords) %>% 
  count(word, sort = T) %>%
  filter(n>= 150, !grepl("([a-z]+)|(בחירות)", word))
```

Now we can look at our wordcloud using `{wordcloud2}`:

<br>
```{r}
wordcloud2::wordcloud2(election_token, color = "#1DA1F2", shape = "circle")
```


### Bigram of used words

Like we did before, we can break up our text data into two words observations, also known as bi-grams. In order to account for all options, we break up the sentence to fit all possible options. For example, assume we have the following sentence:  
"Danny went to vote yesterday"  
Using the `unnest_tokens` we'll break the sentence up to become:
1. Danny went  
2. went to
3. to vote
4. vote yesterday

Which gives us all possible options. We will also include two columns consisting of the bi-gram broken up into single words. This will help in filtering out bi-grams containing Hebrew stop words. I'll not run through the following code and the next and instead will point you to [David Ronbinson](http://varianceexplained.org/) & [Julia Silge](https://juliasilge.com/) fantastic ['Text Mining with R' Book](https://www.tidytextmining.com/).

```{r fig.cap="Graph excludes Hebrew stop words and the word 'elections'"}
elec_bigram <- elections %>%
  select(text) %>% 
  unnest_tokens(bigram, text, token = "ngrams", n = 2) %>%
  separate(bigram, into = c("word1", "word2"), sep = " ", remove = FALSE) %>% 
  filter(!word1 %in% he_stopwords$word,
         !word2 %in% he_stopwords$word,
         !grepl("[a-z]|בחירות", bigram)) %>% 
  count(word1, word2, sort = T) %>% 
  slice(1:45) %>%
  graph_from_data_frame()

p_arrow <- arrow(type = "closed", length = unit(.1, "inches"))

ggraph(elec_bigram, layout = "fr")+
  geom_edge_link(aes(edge_alpha = n), arrow = p_arrow, end_cap = circle(.04, "inches"), show.legend = FALSE)+
  geom_node_point(color = "lightblue", size = 3)+
  geom_node_text(aes(label = name), vjust = 1, hjust = 1, family = "Calibri")+
  theme_void()+
  labs(title = "Bigram from Twitter data")+
  theme(text = element_text(family = "Calibri"),
        plot.title = element_text(hjust = 0.5 , face = "bold", size = 18))
```
<br>

So what are we looking at?  
* We have discussions regarding the **number of chairs a govenrment will have (62/61/60/58)** connected to mentions of the number of political campaigns (2/3), discussions of united and limited government and the forming of in general.  
* We see **mentions of individuals** such as 'Yair Lapid', "Amir peretz", "Benjamin Netanyahu", "Amit Segal" (Both we discussed earlier), "Natan Eshel" **but no mention of the main candidate running against Netanyahu - "Benny Gantz"**. That's actually kind of odd so I ran the analysis again to search for Gantz and found that although he appears in 744 different bigrams, they all include different combinations of him!  
* We also see parties mentioned such as "Meretz", "Gesher" and "Labor" who ran together this time around, "Otzma Yehudit", "United Torah Judaism", and the "Joint List". **There's no mention of two leading parties - "Kahol Lavan" & "The Likkud".**, despite the mentioning of the latter's leader. This is inline with why Gantz doesn't appear - although the words appear many times, all combinations are somewhat different from one another.  
* Mentions of Netanyahu's indicment and the personal law connected to him.  
* Mentions I'd categorize as 'other' such as "Terrorist supporters", "Will of the people", "Fake news", "Last year", etc.
<br>

Actaully, this turned out more interesting than I thought. Several questions arose while looking at it: Several words are missing such as the main parties names (Likkud & Kahol-Lavan), The leading oponent running against Benjamin Netanyahu - Benny Gantz - and other questions such as with whom are specific terms associated. Before we close up I'll look at one question that troubles me - **Why doesn't Gantz appear in our list** `r emo::ji("shocked")`?

#### Benny Gantz's disappearance  

In order to see why Benny Gantz doesn't appear in our bigram plot I'll do the following: I'll break the text into bigrams and filter only to the bigrams containing the word Gantz. Once we have that we can see why doesn't he appear in our bigram plot despite appearing in our wordcloud. Before I run the analysis and give you the answer think for a moment - What was the process of coming up with the bigram? If I chose only the 50 most frequent bigrams, why would a word that appears many times in our text not appear in our bigram list? Maybe even give the previous chunk another glance before I answer it.  
<br>
Let's have a look:

```{r}
gantz <-elections %>%
  select(text) %>% 
  unnest_tokens(bigram, text, token = "ngrams", n = 2) %>%
  separate(bigram, into = c("word1", "word2"), sep = " ", remove = FALSE) %>% 
  filter(word1 %in% "גנץ" |
         word2 %in% "גנץ",
         !grepl("[a-z]|בחירות", bigram))
```

The code is similar to what we did earlier only this time we left **bigrams that match the word we want** and not those that don't match like stop-words. Now that we have our list of bigrams, let's count how many distinct bigrams include the word גנץ ('Gantz') we have:

```{r}
gantz %>% 
  count(bigram, sort = T)
```

**AHA!** Now I see what happened. The first bigram is a stop-word and the word Gantz ('Of Gantz'). The second bigram should have been included as it is Gantz's full name - Benny Gantz, which appears 138 times.   
So, why has it been filtered? This is a great question which we can answer if we look at our stop-words we initially used. Let's see if it has the word בני ('benny' in Hebrew):

```{r}
he_stopwords %>% 
  filter(word == "בני")
```

Yes it does. At the time of writing this blog post it leaves me in a dilemma - Should I change the stop-words file I used to a different one or maybe create my own? Or should I continue as is? I think leaving it will teach me (and hopefully whoever read this far) a valuable lesson of always checking your stop-words. In a different context the specific word could have been invaluable, but here it didn't make sense that our leading candidate was filtered, thus my inquire into what happened. In hebrew the word benny means my son, which I wouldn't describe as a stop-word but whoever made the dataset I guess did.

Let's look at a last attempt of bigram, only this time with a different stop word dataset:
