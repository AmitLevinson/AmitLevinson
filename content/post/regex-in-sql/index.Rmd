---
title: Translating Regex to SQL String Operations
author: Amit Levinson
date: '2022-06-02'
slug: regex-in-sql
categories: [R]
tags: [R, Python, SQL]
subtitle: 'Learning how to do several string operations with SQL'
summary: 'Unfortunately, a lot of Regex operations are not available in (MS)SQL. In this post I show how I take several regex operations I like and convert thme to string manipulations with SQL syntax'
featured: yes
image:
  caption: ''
  focal_point: ''
  preview_only: yes
projects: []
draft: true
codefolding_show: show
editor_options: 
  chunk_output_type: inline
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE, eval = TRUE)
```

Work as a Data Analyst in the fraud domain I constantly find myself manipulating text. Coming from an R background I got to play around with various regex and text manipulation projects, e.g. exploring TF-IDF, GitHub automations and more. 

As I've been using a lot of SQL at work I wanted to transfer common regex/text manipulation procedures but quickly realized that it's not so straightforward in MSSQL we're using. This post goes over three scenarios I've solved in both some coding language and SQL. As I've recently picked up Python more I'll also solved some of them using Python.


### Setup

Let's start by setting up a local connection so we can easily write SQL queries as we move forward:

```{r echo = FALSE}
library(odbc)
library(DBI)

rconn <- dbConnect(odbc(),
                      Driver = "SQL Server",
                      Server = "localhost\\SQLEXPRESS",
                      Database = "regex")

# Partial strings
partial_identifiers <- data.frame(
  id = c("a2209370-bdb0-44c5-b4c5", "1746-4ea9-9a33-f8cc3dd07b8e", "b510-442c-b0d1", "8742-4396-83ca", "6856-4df8-ac69")
)
# dbWriteTable(rconn, 'Partial_Identifiers', partial_identifiers)
```

Awesome. As we move forward we'll be working with both Python (mainly) and one time answer it in R too. So let's load the relevant libraries and data so we have them for future manipulations.


```{python}
# Python code
import pandas as pd

# content/post/regex-in-sql/
payments = pd.read_csv('content/post/regex-in-sql/data.csv')
```



```{r}
#R code
library(tidyverse)

payments <- read_csv('content/post/regex-in-sql/data.csv')
```

##### The Data

Our main dataset is a payments table. It's not normalized so that we can avoid joins and focus on what's relevant for the post. Specifically, it looks as follows:

```{r}
knitr::kable(payments)
```
We'll look at manipulations across each colum for a specific use case. Feel free to follow along; you can find the relevant dataset in the [website's GitHub repository]().

All right then, let's (^begin|start$)

### Lookarounds ðŸ‘€ â€” Extracting the domain from an email address

[lookarounds](https://www.regular-expressions.info/lookaround.html) are definitely one of my favorite and commonly go-to regex operations. As the name implies, a lookaround searches for a pattern and specified string in a specific piece of text. A *lookahead* searches for the pattern and takes what's before it, while a *lookbehind* searches for the pattern and takes what follows it. They both can be positive or negative, while the former searches for a match (positive match) and the latter searches for the string without a match to the pattern/symbol referenced.

Let's try this with a set of emails. **For example you might want to extract the email domains, which is everything that's after the @ symbol.**

##### Python

Returning to our table, we can do this using the following regex operation:

```{python}
(
  payments
  .drop_duplicates()
  .loc[:, ['userId', 'email']]
  .assign(
    emaildomain = lambda df: df.email.str.extract(r'((?<=@).+)') # <- relevant part
    )
  )
```


the pattern `(?<=@).+` essentially extracts any character(s) that follow the @ symbol, in this case our domain[^1]. Alternatively if we were interested in extracting email names instead we could use a positive lookahead, looking for the '@' symbol only this time taking what's before it (here it would be r'.+(?=@)'). 

[^1]: The extra parentheses is to solve the 'ValueError: pattern contains no capture groups' error, basically including what is it we want to be captured in our regex.

##### SQL

So how can we do it in SQL? Well, I mainly use it for the positive lookahead/behind, where we can identify the character's location and extract anything after it:

```{sql connection=rconn, echo = TRUE}
SELECT DISTINCT p.userId,
  email,
  RIGHT(EMAIL, LEN(EMAIL) - CHARINDEX('@',EMAIL)) AS email_domain
FROM PAYMENTS p
```


We're leveraging the function `CHARINDEX` in order to identifying the location of the '@' symbol, and then extract all text from that location forward.

### Partial string join â€” identifying a string from a partial match

I wouldn't say this is a common thing I do, but I had to do it once and was pretty pleased with the solution. Assuming you have another column/dataset with partial matching strings to your primary key, how can you join and identify which observartions intersect? 

For example, you received from some partner a list of ids he has for each payment. However, what he has is only a part of the full string, as we can see below:

```{r}
knitr::kable(partial_identifiers)
```
These strings are containted in our payments table in the column 'payment_identifier', but how can we easily join the relevant payments?

##### R

At the time I encountered this I was using mainly R and solved it with that. Let's solve it first and then go over the solution:

```{r echo = FALSE}

check_payment_id_exists <- function (payment_id) {
  result = partial_identifiers$id[map_lgl(partial_identifiers$id, ~ str_detect(payment_id, .))]
  result = ifelse(is_empty(result), NA, result)
  return(result)
}

payments %>% 
  mutate(
    identifier = map_chr(payment_identifier, check_payment_id_exists),
    .before = 'payment_identifier', .keep='used'
  )


```


```{r}
library(fuzzyjoin)

regex_left_join(x = payments, y = partial_identifiers, by = c('payment_identifier' = 'id')) %>% 
  select(payment_identifier, id)
```

So what do we have here? Well the idea is to iterate across all payment identifiers we have and see which of the partial identifiers matches it. To do this we break the process up, the first section is a function to do exactly that, identify if a payment matches any of the partial identifiers, and the second section is the analysis in which I iterate across all payment identifiers and check if they match. 

At the time I faced this challenge I was mainly using R so I used that. I actually had a different answer that basically extracted which values matched as new column, and then joined on that. I really like this solution though, and shows the awesoem power of the [{fuzzyjoin}](https://cran.r-project.org/web/packages/fuzzyjoin/index.html) R package. 

I realized this post solves the other two challenges with Python, so I might as well try it with that too. You know, just for the kicks:

```{python}
payments
r.partial_identifiers

payments.apply(lambda x: x.payment_identifier.find())
dataFrameFull.apply(
    lambda x: x.work_name.find(x.sub_work_name), axis=1).ge(0)

```


##### SQL

```{sql connection=rconn, echo = TRUE}
SELECT payment_identifier,
  pi.id
FROM PAYMENTS p
LEFT JOIN partial_identifiers pi on p.payment_identifier like concat('%', pi.id, '%')
```



The idea is pretty straight forward. We can leverage the `LIKE` operator in a join to do the partial matching for us, matching any payment identifiers to the partial identifiers id.

### Extracting / Separating text & digits.

Occasionally you might encounter values that contain both a string and digits combined, for example payment descriptions, email users, security answers and more. Being able to separate the text from numbers might be a necessary step for cleaning our data and further analysis. Let's see how can we do this on the column payment_description that contains both what seems as a name and digits.

```{python echo=TRUE}
payments[['name', 'number']] = payments.payment_description.str.split(r'(\d+)', expand = True).iloc[:, 0:2]
payments
```


it's pretty straightforward using the python `split` argument. We pass it a pattern to separate by and wrap it in a parenetheses (so it won't drop). From there we just remove an empty column and assign it as new columns in our dataframe.

#### SQL

This requires a little more work, as we want to split it while we have varying lengths of numbers across strings. For this we can use Translate and convert any characters or digits - according to what we're extracting - to a blank value.

```{sql connection=rconn}
SELECT 
  Payment_Description,
  REPLACE(TRANSLATE(PAYMENT_DESCRIPTION, '0123456789',
                                         '##########'),
          '#','') AS Name,
  REPLACE(TRANSLATE(PAYMENT_DESCRIPTION, 'abcdefghijklmnopqrstuvwxyz.',
                                         '###########################'),
          '#','') AS Numbers
FROM PAYMENTS
```
We combine the `TRANSLATE` and `REPALCE` functions to do a string-extract kind of operation. The Translate basically converts any of the chracters noted in the second argument to a character in the third argument. We then replace all hashtags to empty values.

This is done both for the name and numbers, converting all letters and a period to empty values, and all numbers to empty values correspondently.

You usually would have data that's a little messier, e.g. numbers appearing in between letters, but it should give the main idea and help you start from there (or at least did so for me).

### Conclusion

- three examples

- translating regex to sql

- Usually it can be done, it's a matter of finding the right flow of operation.

### Appendix




```{r}
# library(readr)
# dat <- read_csv('content/post/regex-in-sql/data.csv')

# dbWriteTable(conn = rconn,
#             'Payments',
#             dat,
#             overwrite = TRUE)
```


```{r echo = FALSE}
dbDisconnect(rconn)
```

