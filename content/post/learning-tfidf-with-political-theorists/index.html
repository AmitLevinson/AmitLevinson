---
title: Learning Tfidf with Political Theorists
author: Amit Levinson
date: '2020-05-31'
slug: learning-tfidf-with-political-theorists
categories: [R]
tags: [tidytext]
subtitle: ''
summary: 'Learning tf-idf through political theorists.'
featured: yes
draft: false
image:
  caption: ''
  focal_point: ''
  preview_only: yes
projects: []
codefolding_show: hide
---

<script src="/rmarkdown-libs/kePrint/kePrint.js"></script>


<style>
p.caption {
  font-size: 0.9em;
}
</style>
<p>Thanks to <a href="https://almogsi.com/">Almog Simchon</a> for insightful comments on a first draft of this post.</p>
<div id="introduction" class="section level3">
<h3>Introduction</h3>
<p>Learning <code>R</code> for the past nine months or so has enabled me to explore new topics that are of interest to me, one of them being text analysis. In this post I‚Äôll explain what is Term-Frequency Inverse Document Frequency (tf-idf) and how it can help us explore important words for a document within a corpus of documents<a href="#fn1" class="footnote-ref" id="fnref1"><sup>1</sup></a>. The analysis helps in finding words that are common in a given document but are rare across all other documents.</p>
<p>Following the explanation we‚Äôll implement the method on four great philosophers‚Äô books: ‚ÄòRepublic‚Äô (Plato), ‚ÄòThe Prince‚Äô (Machiavelli), ‚ÄòLeviathan‚Äô (Hobbes) and lastly, one of my favorite books - ‚ÄòOn Liberty‚Äô (Mill) üòç. Lastly, we‚Äôll see how tf-idf compares to a Bag of Words analysis (word count) and how using both can benefit your exploring of text.</p>
<p><strong>The post is aimed for anyone exploring text-analysis</strong> and wants to learn about tf-idf. <strong>I will be using <code>R</code> to analyze our data but won‚Äôt be explaining the different functions</strong>, as this post focuses on the tf-idf analysis. If you wish to see the code, feel free to download or explore the .Rmd source code on my <a href="https://github.com/AmitLevinson/amitlevinson.com/blob/master/content/post/learning-tfidf-with-political-theorists/index.Rmd">github repository</a>.</p>
</div>
<div id="term-frequency" class="section level3">
<h3>Term frequency</h3>
<p>tf-idf gauges a word‚Äôs value according to two parameters: The first parameter is the <strong>term-frequency of a word: How common is a word in a given document</strong> (Bag of Words analysis); one method to calculate term frequency of a word is just to count the total number of times each words appears. Another method - which we‚Äôll use in the tf-idf - is, after summing the total number of times a word appears, we‚Äôll divide it by the total number of words in that document, <strong>describing term frequency as such:</strong></p>
<p><span class="math display">\[tf = \frac{\textrm{Number of times a word appears in a document}}{\textrm{Total number of words in that document}}\]</span></p>
<p>Also written as <span class="math inline">\(tf(t,d)\)</span> where <span class="math inline">\(t\)</span> is the number of times a term appears out of all words in document <span class="math inline">\(d\)</span>. Using the above method we‚Äôll have the <strong>proportion</strong> of each word in our document, a value ranging from 0 to 1, where common words will have higher values.</p>
<p>While this gives us a value gauging how common a word is in a document, what happens when we have many words across many documents? How do we find <ins>unique</ins> words for each document? This brings us to <em>idf</em>.</p>
</div>
<div id="inverse-document-frequency" class="section level3">
<h3>Inverse document frequency</h3>
<p><strong>Inverse document frequency accounts for the occurrence of a word across all documents, thereby giving a higher value to words appearing in less documents.</strong> In this case, for each term we will calculate the log ratio<a href="#fn2" class="footnote-ref" id="fnref2"><sup>2</sup></a> of all documents divided by the number of documents that word appears in. This gives us the following:</p>
<p><span class="math display">\[ idf = \log {\frac{\textrm{N documents in corpus}}{\textrm{n documents containing the term}}}\]</span></p>
<p>Also written as <span class="math inline">\(idf = \log{\frac{N}{n(t)}}\)</span> Where <span class="math inline">\(N\)</span> is the total number of documents in our corpus and <span class="math inline">\(n(t)\)</span> is the number of documents the word appears within our corpus of documents.</p>
<p>To those unfamiliar, a logarithmic transformation helps in reducing wide-ranged numbers to smaller scopes. In this case, if we have 7 documents, and our term appears in all 7 documents, we‚Äôll have following idf value: <span class="math inline">\(log_e(\frac{7}{7}) = 0\)</span>. What if we have a term that appears in only 1 document out of all 7 documents? We‚Äôll have the following: <span class="math inline">\(log_e(\frac{7}{1}) = 1.945\)</span>. Even if a word appears in only 1 document out of 100, a logarithmic transformation will reduce its high value to mitigate bias when we multiply it with its <span class="math inline">\(tf\)</span> value.</p>
<p><strong>So what do we understand from the idf?</strong> Since our numerate always remains the same (N documents in corpus), the <em>idf</em> of a word is contingent upon how common it is <em>across</em> documents. Words that appear in a small number of documents will have a higher <em>idf</em>, while words that are common across documents will have a lower <em>idf</em>.</p>
</div>
<div id="term-frequency-inverse-document-frequency-tfidf" class="section level3">
<h3>Term-Frequency Inverse Document Frequency (tfidf)</h3>
<p>Once we have the term frequency and inverse document frequency for each word we can calculate the tf-idf by multiplying the two: <span class="math inline">\(tf(t,d) \cdot idf(t,D)\)</span> where <span class="math inline">\(D\)</span> is our corpus of documents.</p>
<p><strong>To summarize our explanation:</strong> The two paramteres used to calculate the tf-idf provide each word with a value for its importance to that document in that corpus of text. Ideally We take <strong>words that are <u>common within a document</u> and that are <u>rare across documents</u></strong>. I write ideally because as we‚Äôll see soon, we might have words that are extremely common in one document but are filtered out because they‚Äôre evident in all documents (can happen in a small corpus of documents). This also highlights the question as to what is <em>important</em>; I define important as contributing to understanding a document in comparison to all other documents.</p>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-2"></span>
<img src="/post/learning-tfidf-with-political-theorists/index_files/figure-html/unnamed-chunk-2-1.png" alt="Using tf-idf we can calculate how common a word is within a document and how rare is it across documents" width="85%" />
<p class="caption">
Figure 1: Using tf-idf we can calculate how common a word is within a document and how rare is it across documents
</p>
</div>
<p>Now that we have some background as to how tf-idf works, let‚Äôs dive in to our case study.</p>
</div>
<div id="tf-idf-on-political-theorists." class="section level3">
<h3>TF-IDF on political theorists.</h3>
<p>I‚Äôm a big fan of political theory. I have a small collection at home and always like to read and learn more about it. Except for Mill, we read Plato, Machiavelli and Hobbes in our BA first semester course in political theory. While some of the theorists overlap to some degree, over-all they discuss different topics. tf-idf will help us distinguish important words specific to each book, in a comparison across all books.</p>
<p>Before we conduct our tf-idf we‚Äôd like to explore our text a bit. The following exploratory analysis is inspired from Julia Silge‚Äôs blog post <a href="https://juliasilge.com/blog/term-frequency-tf-idf/">‚ÄòTerm Frequency and tf-idf Using Tidy Data Principles‚Äô</a>, a fantastic read.</p>
</div>
<div id="data-collection-analysis" class="section level3">
<h3>Data collection &amp; Analysis</h3>
<p>The package we‚Äôll use to gather the data is the <code>{gutenbergr}</code> package. It enables us to access the <a href="https://www.gutenberg.org/">Project Gutenberg</a> free books, a library of over 60,000 free books. As many other amazing things in <code>R</code> someone, in this case David Robinson, created a package for it. All we need to do is download them to our computer.</p>
<pre class="r"><code>Mill &lt;- gutenberg_download(34901)
Hobbes &lt;- gutenberg_download(3207)
Machiavelli &lt;- gutenberg_download(1232)
Plato &lt;- gutenberg_download(150)</code></pre>
<p>Several of the books contain sections at the beginning or at the end that aren‚Äôt relevant for our analysis. For example long introductions from contemporary scholars; another whole different book at the end, etc. These can confound our analysis and therefore we‚Äôll exclude them. In order to conduct our analysis we also need all the books we collected in one object.</p>
<p><strong>Once we are able to clean the books, this is what our text looks like:</strong></p>
<pre class="r"><code>remove_text &lt;- function(book, low_id, top_id = max(rowid), author = deparse(substitute(book))){
  book %&gt;%
  mutate(author = as.factor(author)) %&gt;% 
  rowid_to_column() %&gt;% 
  filter(rowid &gt;= {{low_id}}, rowid &lt;= {{top_id}}) %&gt;% 
  select(author, text, -c(rowid, gutenberg_id))}

books &lt;- rbind(
  remove_text(Mill, 454),
  remove_text(Hobbes, 360, 22317),
  remove_text(Machiavelli, 464, 3790),
  remove_text(Plato, 606))</code></pre>
<pre><code>## # A tibble: 45,490 x 2
##    author text                                                                  
##    &lt;fct&gt;  &lt;chr&gt;                                                                 
##  1 Mill   &quot;&quot;                                                                    
##  2 Mill   &quot;&quot;                                                                    
##  3 Mill   &quot;CHAPTER I.&quot;                                                          
##  4 Mill   &quot;&quot;                                                                    
##  5 Mill   &quot;INTRODUCTORY.&quot;                                                       
##  6 Mill   &quot;&quot;                                                                    
##  7 Mill   &quot;&quot;                                                                    
##  8 Mill   &quot;The subject of this Essay is not the so-called Liberty of the Will, ~
##  9 Mill   &quot;unfortunately opposed to the misnamed doctrine of Philosophical&quot;     
## 10 Mill   &quot;Necessity; but Civil, or Social Liberty: the nature and limits of th~
## # ... with 45,480 more rows</code></pre>
<p>Each row is some text with chapters separated by headings and a column referencing who is the author. Our data frame consists of ~45,000 rows with the filtered text from our four books. Tf-idf can also be done on any n-grams we choose (number of consequent words). We could calculate the tf-idf for each bigram of words (two-words), trigram, etc. I find a unigram an appropriate approach both for tf-idf and especially now when we want to learn more about it. <strong>We just saw that our text is in the form of sentences, so let‚Äôs break it into single words.</strong></p>
<pre><code>## # A tibble: 12 x 4
## # Groups:   author [4]
##    author      word      n sum_words
##    &lt;fct&gt;       &lt;chr&gt; &lt;int&gt;     &lt;int&gt;
##  1 Hobbes      the   14536    207849
##  2 Hobbes      of    10523    207849
##  3 Hobbes      and    7113    207849
##  4 Plato       the    7054    118639
##  5 Plato       and    5746    118639
##  6 Plato       of     4640    118639
##  7 Mill        the    3019     48006
##  8 Mill        of     2461     48006
##  9 Machiavelli the    2006     34821
## 10 Mill        to     1765     48006
## 11 Machiavelli to     1468     34821
## 12 Machiavelli and    1333     34821</code></pre>
<p>We see that stop-words dominant the frequency of occurrences. That makes sense as they are commonly used, but they‚Äôre not usually helpful for learning about a text, specifically here. <strong>We‚Äôll start by exploring how the word frequencies occur within a text:</strong></p>
<p><img src="/post/learning-tfidf-with-political-theorists/index_files/figure-html/unnamed-chunk-8-1.png" width="672" style="display: block; margin: auto;" /></p>
<p>The plot above shows the frequency of terms across documents. We see some words that appear frequently (higher proportion = right side of the x-axis) and many words that are rarer (low proportion). Actually, I had to limit the x-axis or otherwise it would distort the plot with words that are extremely common.</p>
<p>To help find useful words with the highest tf-idf from each book, we‚Äôll remove stop words before we extract the words with a high tf-idf value:</p>
<table class="table" style="width: auto !important; margin-left: auto; margin-right: auto;">
<thead>
<tr>
<th style="text-align:center;">
Author
</th>
<th style="text-align:center;">
Word
</th>
<th style="text-align:center;">
n
</th>
<th style="text-align:center;">
Sum words
</th>
<th style="text-align:center;">
Term Frequency
</th>
<th style="text-align:center;">
IDF
</th>
<th style="text-align:center;">
TF-IDF
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center;">
Mill
</td>
<td style="text-align:center;">
opinion
</td>
<td style="text-align:center;">
150
</td>
<td style="text-align:center;">
48006
</td>
<td style="text-align:center;">
0.0094132
</td>
<td style="text-align:center;">
0.0000000
</td>
<td style="text-align:center;">
0.0000000
</td>
</tr>
<tr>
<td style="text-align:center;">
Hobbes
</td>
<td style="text-align:center;">
god
</td>
<td style="text-align:center;">
1047
</td>
<td style="text-align:center;">
207849
</td>
<td style="text-align:center;">
0.0149024
</td>
<td style="text-align:center;">
0.0000000
</td>
<td style="text-align:center;">
0.0000000
</td>
</tr>
<tr>
<td style="text-align:center;">
Machiavelli
</td>
<td style="text-align:center;">
prince
</td>
<td style="text-align:center;">
185
</td>
<td style="text-align:center;">
34821
</td>
<td style="text-align:center;">
0.0172704
</td>
<td style="text-align:center;">
0.2876821
</td>
<td style="text-align:center;">
0.0049684
</td>
</tr>
<tr>
<td style="text-align:center;">
Plato
</td>
<td style="text-align:center;">
true
</td>
<td style="text-align:center;">
485
</td>
<td style="text-align:center;">
118639
</td>
<td style="text-align:center;">
0.0152953
</td>
<td style="text-align:center;">
0.0000000
</td>
<td style="text-align:center;">
0.0000000
</td>
</tr>
</tbody>
<tfoot>
<tr>
<td style="padding: 0; border: 0;" colspan="100%">
<sup></sup> Random sample of words and their corresponding tf-idf values
</td>
</tr>
</tfoot>
</table>
<p>Above we have our tf-idf for a given word from each document. I removed stop-words and calculated the tf-idf for each word in each book. For Hobbes the word ‚ÄòGod‚Äô appears 1047 times, thus has a <span class="math inline">\(tf\)</span> of <span class="math inline">\(\frac {1047} {207849}\)</span> and an idf of 0 (since it appears in all documents), so it‚Äôll have a tf-idf of 0.</p>
<p>With Machiavelli the word prince appears 185 times, with a <span class="math inline">\(tf\)</span> of <span class="math inline">\(\frac {185} {34821}\)</span>, resulting in a proportion of 0.0173. The word prince has an idf of 0.288 <span class="math inline">\((log_e(\frac 4 {3}))\)</span>, as there are 4 documents and it appears in 3 of them, so a total tf-idf value of <span class="math inline">\(0.0173 \cdot 0.288\)</span> = <span class="math inline">\(0.00497\)</span>.</p>
</div>
<div id="tf-idf-plot" class="section level3">
<h3>Tf-idf plot</h3>
<p>As we wrap up our tf-idf analysis, <strong>We don‚Äôt want to see all words and their tf-idf, but only words with the highest tf-idf value</strong> for each author, indicating the importance of a word to a given document. We can look at these words by plotting the top 10 highest valued tf-idf words for each author:</p>
<pre class="r"><code> ggplot(data = books_for_plot, aes(x = word, y = tf_idf, fill = author))+
  geom_col(show.legend = FALSE)+
  labs(x = NULL, y = &quot;tf-idf&quot;)+
  coord_flip()+
  scale_x_reordered()+
  facet_wrap(~ author, scales = &quot;free_y&quot;, ncol = 2)+
  labs(title = &quot;&lt;b&gt;Term Frequency Inverse Document Frequency&lt;/b&gt; - Political theorists&quot;,
       subtitle = &quot;tf-idf for The Leviathan (Hobbes), On Liberty (Mill), The Prince (Machiavelli)\nand Republic (Plato)&quot;)+
  scale_fill_manual(values = plot_colors)+
  theme_post+
  theme(plot.title = element_markdown())</code></pre>
<p><img src="/post/learning-tfidf-with-political-theorists/index_files/figure-html/unnamed-chunk-11-1.png" width="672" style="display: block; margin: auto;" /></p>
<p><strong>Lovely!</strong></p>
<p>Let‚Äôs review each book and see what we can learn from our tf-idf analysis. My memory of these books is kind of rusty but I‚Äôll try my best:</p>
<ul>
<li><p><strong>Hobbes:</strong> Hobbes in his book describes the <em>natural</em> state of human beings and how they can leave it by revoking many of their right to the <em>sovereign</em> who will facilitate order. In his book he describes the soveragin (note the ‚Äòa‚Äô) as needed to be strict, rigorous and <em>hath</em>.</p></li>
<li><p><strong>Machiavelli:</strong> Machiavelli provides a leader with a guide on how to rule his country. He prefaces his book with an introduction letter to the <em>Duke</em>, the recipient of his work. Machiavelli throughout the book conveys his message with examples of many <em>princes</em>, <em>Alexander</em> the great, the <em>Orsini</em> brothers and more. Several of his examples include mentioning of Italy (where he resides), specifically <em>Venetians</em> and <em>Milan</em>.<br />
</p></li>
<li><p><strong>Mill:</strong> Mill in his book ‚ÄòOn Liberty‚Äô describes the importance of freedom and liberty for individuals. He does so by describing the relation between people and their <em>society</em> and other relations with the <em>social</em>. He highlights in his discussion on liberty a <em>person‚Äôs</em> belonging; these can be <em>Feelings</em> or basically anything <em>personal</em>. Protecting the personal is important for the <em>development</em> of both society and that of the individual.</p></li>
<li><p><strong>Plato:</strong> Plato‚Äôs book consists of 10 chapters and it is by far the longest compared to the others. The book is written in the form of a dialogue with <em>replies</em> between Socrate and his discussants. Along Socrate‚Äôs journey to finding out what is the meaning of justice he talks to many people, among them <em>Glaucon</em>, <em>Thrasymachus</em> and <em>Adeimantus</em>. In one section Socrates describes a just society with distinct <em>classes</em> such as the <em>guardians</em>. The classes should receive appropriate education, for e.g.¬†<em>gymnastics</em> for the guardians.</p></li>
</ul>
<p>With the above analysis we were able to explore uniqueness of words for each book across all books. <strong>Some words provided us with great insights while others didn‚Äôt necessarily help us despite their uniqeness</strong>, for example, the names of discussants with Socrate. Tf-idf gauges them as important (as to how I defined importance here) to distinguish between Plato‚Äôs book and the others, but I‚Äôm sure they‚Äôre not the first words that come to mind when someone talks about the Republic.</p>
<p>The analysis also shows this methodology‚Äôs value addition is not in just applying tf-idf - or any other statistical analysis ‚Äì rather its power lies in its explanatory abilities. In other words, <strong>tf-idf provides us with a value indicating the importance of a word to a given document within a corpus, it is our job to take that extra step interpreting and contextualizing the output.</strong></p>
</div>
<div id="comparing-to-bag-of-words-bog" class="section level3">
<h3>Comparing to Bag Of Words (BOG)</h3>
<p>A common text analysis is a word count I discussed earlier, also known as Bag of Words (BoW). This is an easy to understand method that can be done easily when exploring text. However, relying only on a bag of words method to draw insights can limit its usefulness if other analytic methods are not also included. The BoW relies only on the frequency of a word, so if a word is common across all documents, it might show up in all of them and not contribute to finding <em>unique words</em> for each document.</p>
<p>Now that we have our books we can also explore the raw occurrence of each word to compare it to our above tf-idf analysis:</p>
<pre class="r"><code>ggplot(data = bow_books, aes(x = reorder(word_with_color,n), y = n, fill = author))+
  geom_col(show.legend = FALSE)+
  labs(x = NULL, y = &quot;Word Frequency&quot;)+
  coord_flip()+
  scale_x_reordered()+
  facet_wrap(~ author, scales = &quot;free&quot;, ncol = 2)+
  labs(title = &quot;&lt;b&gt;Term Frequency&lt;/b&gt; - Political theorists&quot;)+
  scale_fill_manual(values = plot_colors)+
  theme_post+
  theme(axis.text.y = element_markdown(),
        plot.title = element_markdown(),
        strip.text = element_text(color = &quot;grey50&quot;))</code></pre>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-13"></span>
<img src="/post/learning-tfidf-with-political-theorists/index_files/figure-html/unnamed-chunk-13-1.png" alt="Term frequency plot with words that are common across documents in bold" width="672" />
<p class="caption">
Figure 2: Term frequency plot with words that are common across documents in bold
</p>
</div>
<p><strong>The above plot amplifies, in my opinion, tf-idf‚Äôs contribution in finding unique words for each document.</strong> While many of the words are similar to those we found in the previous tf-idf analysis, we also draw words that are common across documents. For example, we see the frequency of ‚ÄòTime‚Äô, ‚ÄòPeople‚Äô and ‚ÄòNature‚Äô twice in different books and words such as ‚ÄòTrue‚Äô and ‚ÄòTruth‚Äô with similar meanings do so too (however this could have happened in tf-idf too).</p>
<p><strong>However, the Bag of Words also provided new words we didn‚Äôt see earlier.</strong> Here we can learn on new words like Power in Hobbes, Opinions in Mill and more. With the bag of words we get words that are common without controlling for other texts, while the tf-idf searches for words that are common within but are rare across.</p>
</div>
<div id="closing-remarks" class="section level3">
<h3>Closing remarks</h3>
<p>In this post we learned the term frequency inverse document frequency (tf-idf) analysis and implemented it on four great political theorists. We finished by exploring tfidf in comparison to a bag of words analysis and showed the benefits of each. This also emphasizes how we define <em>important</em>: Important to a document by itself or important to a document compared to other documents.
The definition of ‚Äòimportant‚Äô here also highlights tf-idf heuristic quantifying approach (<a href="https://en.wikipedia.org/wiki/Tf%E2%80%93idf">specifically the idf</a>) and thus should be used with caution. If you are aware of theoretical development of it I‚Äôd be glad to read more about it.</p>
<p>By now you should be equipped to give tf-idf a try yourself on a corpus of documents you find appropriate.</p>
</div>
<div id="where-to-next" class="section level3">
<h3>Where to next</h3>
<ul>
<li><p>Further reading about text analysis - If you want to read more on text mining with R, I highly recommend the Julia Silge &amp; David Robinson‚Äôs <a href="https://www.tidytextmining.com/">text mining with R book</a>and/or exploring the <a href="https://quanteda.io/"><code>{quanteda}</code></a> package.</p></li>
<li><p>Text datasets - As to finding text data, you can try the <code>{gutenbergr}</code> package that gives access to thousands of books, a <a href="https://github.com/rfordatascience/tidytuesday">#TidyTuesday</a> data set or collect tweets from Twitter using the <code>{rtweet}</code> package.</p></li>
<li><p>Other posts of mine - If you‚Äôre interested in other posts of mine where I explore some text you can read my <a href="https://amitlevinson.com/2020/04/20/israeli-elections-on-twitter/">Israeli elections Twitter tweets analysis</a>.</p></li>
</ul>
<p>That‚Äôs it for now. Feel free to contact me for any and all comments!</p>
<div id="notes" class="section level4">
<h4>Notes</h4>
</div>
</div>
<div class="footnotes">
<hr />
<ol>
<li id="fn1"><p>A single document can be a book, chapter, paragraph or sentence, it all depends on your research and what you define as an ‚Äòentity‚Äô within a corpus of text.<a href="#fnref1" class="footnote-back">‚Ü©</a></p></li>
<li id="fn2"><p><strong>What‚Äôs log ratio?</strong> In general, and for the purpose of the tf-idf, a logarithm transformation (in short <span class="math inline">\(log\)</span>) helps in reducing wide ranged numbers to smaller scopes. Assuming we have the following <span class="math inline">\(\log _{2}(16) = x\)</span>, we ask ourselves (and calculate) 2 in the power of what (x) will give us 16. so in this case 2^3 will give us 16, which is basically written as <span class="math inline">\(\log _{2}(16) = 3\)</span>. In order to generalize it, <span class="math inline">\(\log _{b}(x) = y\)</span>, means b is the base we will raise to the power of y to reach x. Therefore written oppositely as <span class="math inline">\(b^y = x\)</span>. The common uses of log are <span class="math inline">\(\log_2\)</span>, <span class="math inline">\(\log_{10}\)</span> and <span class="math inline">\(log_e\)</span>, also written as plain log.<a href="#fnref2" class="footnote-back">‚Ü©</a></p></li>
</ol>
</div>
