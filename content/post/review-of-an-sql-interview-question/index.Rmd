---
title: Reviewing an SQL interview question
author: Amit Levinson
date: '2021-08-31'
slug: review-of-an-sql-interview-question
categories: []
tags: []
subtitle: ''
summary: 'Solving an SQL interview question using three different approaches.'
authors: []
lastmod: ''
featured: no
image:
  caption: ''
  focal_point: ''
  preview_only: yes
draft: false
projects: []
disable_codefolding: true
---

```{r setup, include=FALSE, warning=FALSE, message=FALSE}
knitr::opts_chunk$set(echo = FALSE, warning = FALSE, message = FALSE)
```

## Introduction

During the first half of 2021, as I was finishing up my M.A. thesis, I started searching for a job in Data Analytics. [My journey into analytics was through learning R](https://amitlevinson.com/blog/my-year-in-r/) and I realized I had to learn some SQL, or at least familiarize myself with it.

Fast forward to interviewing, and most of the SQL interview questions were relevant and interesting, with one question particularly motivating further thoughts; this blog post details that question and several answers to it.


**The question and data have no connection my current employer.** The data used here is made up and the question came from a different company altogether. 

I'll be using R to setup a local SQL connection but power through the blog post with actual SQL code. While not necessary, some SQL knowledge is useful to understanding the various answers', and more so the syntax.

## The Interview question

So here it goes:

> Let's say you have a table of users' payments. The table has the user's name, date of payment and the amount they received. Users have multiple records with different amounts and dates. For each user, return: the user name, the maximum amount they received and the date of that payment.

> Once you solve that, solve it again using a different approach.

For a more practical example, consider the following raw data:

```{r echo = FALSE}
library(dplyr)
library(gt)
library(tibble)
library(DBI)
library(keyring)
library(stringr)
library(ggplot2)
library(gt)
library(forcats)
library(extrafont)
library(sqldf)
```


```{r}
# create the data
payments <- tribble(
  ~username,    ~payment_date,    ~amount,
  "Danny",  "2021-07-05",     42,
  "Danny",  "2021-05-12",     62,
  "Danny",  "2021-08-01",     80,
  "Danny",  "2021-06-12",     87,
  "Alice",  "2021-07-03",     60,
  "Alice",  "2021-05-29",     72,
  "Alice",  "2021-05-12",     85,
  "Alice",  "2021-06-24",     45,
  "Tom",    "2021-06-28",     80,
  "Tom",    "2021-07-12",     56,
  "Tom",    "2021-07-19",     95,
  "Tom",    "2021-05-11",     75
) %>% 
  mutate(payment_date = as.character(payment_date))

# Setup a localhost connection
localhost_con <- dbConnect(RMariaDB::MariaDB(),
                  host = 'localhost',
                  dbname = "sql_interview",
                  username = key_get("mysql-local-user"),
                  password = key_get("mysql-local-password"),
                  port = 3306)

# dbWriteTable(localhost_con, "payments", payments, overwrite = TRUE)
```


```{r, fig.align = 'left'}

# payments %>% clipr::write_clip()

raw_payments <- payments %>% 
  arrange( payment_date)

gt(raw_payments) %>% 
    tab_style(
      style = cell_fill(color = '#E5F6DF'),
      locations = cells_body (
        rows = c(3,5,11)
      )
    ) %>% 
  tab_options(column_labels.text_transform = "Capitalize")
```

Return the following table (rows highlighted in light green):

```{r, fig.align='center'}
payments %>% 
  group_by(username) %>% 
  filter(amount == max(amount)) %>% 
  ungroup() %>% 
  arrange(username) %>% 
  gt()
```


{{% alert note %}}
Want to first try solving it yourself? Solve it [here](http://sqlfiddle.com/#!18/c9308e/46) and compare with the answers below.
{{% /alert %}}

So we know what we have to do. Before we do it, let's see how not to do it.


### &#10006; Why not just GROUP BY? &#10006;

If you're new to SQL, an immediate question that might come to mind is why not use a `GROUP BY` for the UserName, date and select the `MAX` value. In other words, just filter each observation by the max value according to one of the variables.  

The issue is that when we use `GROUP BY` we retrieve the information that is already aggregated. That is, if we group by the seller name and the payment date when we select the max, then we'll get the value for each distinct user and date:


```{sql connection=localhost_con, echo = TRUE}
SELECT UserName,
  payment_date,
  MAX(amount) AS amount
FROM Payments 
GROUP BY UserName, payment_date
```

Alternatively, if we `GROUP BY` the UserName and `SELECT` the `MAX` value and the date, the result will depend on the Relational Database Management System (RDBMS) you use. For example, if you're using Microsoft SQL server you'll get an error since you have a column which is selected but not contained in an aggregate function, nor in the GROUP BY clause.   

In other RDMBS, e.g. MySQL (which I use here), we'll get the information for each User, their max value and some date (here the top date value), **but not the correct date!**

```{sql connection=localhost_con, echo = TRUE}
SELECT UserName,
  payment_date,
  MAX(amount)
FROM payments 
GROUP BY UserName
```


So How do we solve this? Let's dive in.  

<br>

## Solutions

### 1. Window functions

The first solution that might come to mind is using a Window function. If you don't know window functions I suggest you familiarize yourself with their abilities. To borrow from [PostgreSQL's description](https://www.postgresql.org/docs/9.1/tutorial-window.html), a window function "performs a calculation across a set of table rows that are somehow related to the current row". In contrast to aggregate operations (sum, avg, etc), using window functions doesn't cause rows to become grouped into single row outputs.  

We can use the window function [DENSE_RANK()](https://docs.microsoft.com/en-us/sql/t-sql/functions/dense-rank-transact-sql?view=sql-server-ver15)/RANK()[^1] to retrieve the rank of each amount for each user, and extract the relevant row with an outer query:

[^1]: One reason I'm not going for ROW_NUMBER here is that were interested in the top value that could have multiple appearances for a user. ROW NUMBER will only give us one value, here I'm interested in the max value that could appear several times.

```{sql connection=localhost_con, echo = TRUE, eval = FALSE}
SELECT UserName, Payment_Date as 'Payment Date', amount
FROM (
  SELECT *,
    DENSE_RANK() OVER(Partition BY UserName Order by amount DESC) as rnk
  FROM payments) AS ranked_table
WHERE rnk = 1
```

OK, that was pretty straight forward. But the interview question doesn't end there but asks for another approach. Let's move on.

### 2. Self Join

`JOIN` are key functions when querying data. Considering the large amount of data a company has, and the normalization procedures it does you'll be expected to join a lot. In this specific case we can leverage the arithmetic features of a `JOIN` to retrieve the relevant value:


```{sql connection=localhost_con, echo = TRUE, eval = FALSE}
SELECT DISTINCT p.UserName, p.payment_date, p.amount
FROM payments p
LEFT JOIN payments pp ON p.UserName = pp.UserName
  AND p.amount < pp.amount
WHERE pp.amount IS NULL;
```

While we're all familiar with 'regular' `* JOIN` using an equality sign `=`, we can check for other operations such as smaller than `<`. Essentially we do a cartesian join of the table on itself by UserName, and match rows where values (p.amount) are smaller than other values in the table we join on (pp.amount). Our max value won't find any relevant rows to join, considering it's not smaller than anything, which will result in a `NULL` value we can use to filter.  

**We can also explore the intermediate step** of the above-code by looking at one of the users' observations:

```{sql connection=localhost_con, echo = TRUE, echo = FALSE}
SELECT p.UserName, p.payment_date, p.amount, pp.amount as amount2
FROM payments p
LEFT JOIN payments pp ON p.UserName = pp.UserName
  AND p.amount < pp.amount
  WHERE p.UserName = 'Danny'
  ORDER BY p.amount DESC
  LIMIT 3;
```
As we can see from the top 3 observations (though 7 are returned per user), values that are not smaller than other values, i.e. our max value, return a null value we can use to filter. If you want to explore it more just copy the above code to the snippet example, remove the `WHERE` clause and also select `pp.amount`.

### 3. Correlated subquery

We've come to my final approach for this blog post. I've come to appreciate correlated subqueries since learning them, as I find them somewhat similar to vectorized operations in `R` such as the `apply` family and the `purrr` library.

A correlated subquery is a row-by-row process, in which each subquery is executed once for the outer query (adapted from [GeeksforGeeks](https://www.geeksforgeeks.org/sql-correlated-subqueries/)). Let's look at the code and explain it more clearly:


```{sql corr-subqeury, connection=localhost_con, echo = TRUE, eval = FALSE}
SELECT UserName,
  Payment_Date,
  amount
  FROM Payments p
  WHERE amount = (SELECT MAX(amount)
                  FROM Payments pp
                  WHERE pp.UserName = p.UserName) -- Notice the relation to the parent table
```

To easily read the query and understand correlated subqueries, let's start from the inside. From the payments tables where the UserName is equal to the UserName in the outer query, grab the maximum amount. Now the outer query goes *row by row for each user* and compares whether that row's amount is equal to that user's max amount, which is retrieved from the inner query. 

And there we have it, three different approaches to the same problem.

![](https://media.giphy.com/media/YRuFixSNWFVcXaxpmX/giphy.gif?cid=ecf05e47x2u6c65w26gljefosulh4xiu3xds5gz55z0cq8li&rid=giphy.gif&ct=g)


## Benchmarking

A question that arose for me is, for this specific case, which method is faster? Let's try and answer it using a bit larger dataset:

```{r echo = FALSE}
payments_big <- data.frame(
  username = sample(c("Danny", "Alice", "Tom"), size = 2e3, replace = TRUE),
  payment_date = sample(seq(as.Date("2018/01/01"), as.Date("2021/08/01"), by = "day"), 2e3, replace = T),
  amount = sample.int(c(1e4), 2e3, replace = T)
)
```


```{r echo = TRUE}
glimpse(payments_big)
```


```{r eval= FALSE, echo = FALSE}
# Write table
# dbWriteTable(localhost_con, "payments_big", payments_big, overwrite = TRUE)
```


A total of 2,000 rows for all the three users. Now, let's benchmark using The R package `{sqldf}` which passes the SQL statements to a temporally created database:

```{r eval = FALSE}
Window_script <- "SELECT UserName, Payment_Date as 'Payment Date', amount
FROM (
  SELECT *,
    DENSE_RANK() OVER(Partition BY UserName Order by amount DESC) as rnk
  FROM payments_big) AS ranked_table
WHERE rnk = 1"

Join_script <- "SELECT DISTINCT p.UserName, p.payment_date, p.amount
FROM payments_big p
LEFT JOIN payments_big pp ON p.UserName = pp.UserName
  AND p.amount < pp.amount
WHERE pp.amount IS NULL"

Correlated_subquery_script <- "SELECT UserName,
  Payment_Date,
  amount
  FROM payments_big p
  WHERE amount = (SELECT MAX(amount)
                          FROM payments_big pp
                          WHERE pp.UserName = p.UserName -- Notice the relation to the parent table
                          )
  GROUP BY UserName;"
```


```{r eval = FALSE, echo = TRUE}
benchmarking <- microbenchmark::microbenchmark("Window" = sqldf(Window_script),
                                               "Join" = sqldf(Join_script),
                                               "Correlated subquery" = sqldf(Correlated_subquery_script),
                                               unit = "ms")
```

Finally, let's explore the benchmarking scores:

```{r echo=FALSE}
# Reading it manually as it is too big to run on every knit.
# saveRDS(benchmarking, "benchmark.Rds")
benchmarking <- readRDS("benchmark.Rds")

library(microbenchmark)


summary(benchmarking) %>% 
  rename(approach = expr) %>% 
  mutate(across(where(is.numeric), function(x) round(x, 1))) %>% 
  rename(`25%` = lq,
         `75%` = uq,
         N = neval) %>% 
  gt() %>% 
  tab_options(table.align = 'center',
              column_labels.text_transform = "Capitalize",
              column_labels.font.weight = "bold") %>% 
  tab_style(cell_text(font = "Lora"),
            locations = cells_body()) %>% 
  tab_style(cell_text(font = "Lora"),
            locations = cells_column_labels()) 


```

```{r}
theme_set(theme_minimal()+
            theme(
              text = element_text(family = "Segoe UI"),
              plot.title = element_text(size = 14, face = "bold"),
              plot.subtitle = element_text(size = 12, color = "gray15"),
              plot.title.position = "plot",
              panel.grid.minor = element_blank(),
              axis.text.x = element_text(size = 12),
              axis.text.y = element_text(size = 9, color = "gray25"),
              axis.title = element_text(size = 10, color = "gray15"),
              plot.margin = margin(8,2,8,5)
          ))


ggplot(data = data.frame(expr = benchmarking$expr, time = benchmarking$time / 1e6))+
  geom_boxplot(aes(x = fct_reorder(expr, time), y = time))+
  scale_y_continuous(name = "Time (ms)\n", labels = scales::label_comma())+
  labs(title = "Benchmarking different solutions",
       subtitle = "Values from 100 iterations of each solution on a 2,000 row table",
       x = NULL)


```

One caveat is that some noise might have occurred when I queried the data: Since we used the `{sqldf}` R package to benchmark, the table is loaded to a temporarily created database and the SQL statement is run on it. With that said, I imagine that if would have caused some issues, it would have done so across all statements.

As to our results, we can see that for the current question, the window function is most efficient. I think it's also the most friendly for beginners and commonly used.

However, I believe that knowing all approaches can help you write better SQL. That is, sometimes one approach is a better fit to a specific use-case. I definitely wrote a correlated subquery at work as it was the best fit at the time (in terms of readability and as an immediate answer), so though it's least efficient here I'm sure it's worth knowing.

### Closing remarks

This was a pretty short post on some SQL approaches to solving a question. You can probably think of different approaches, or variants of the current ones. When contemplating this question I felt that it required me to utilize different SQL functions to solve the same question, so overall I'm glad I came across it. I didn't get the job but that's OK. Eventually that's how I ended up where I am today :)

Hope you find this useful and learned something new. Feel free to reach out and let me know of other solutions you thought of!  

```{r echo = FALSE}
# Close our SQL connection
dbDisconnect(localhost_con)
```
